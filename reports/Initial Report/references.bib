@article{Cen2024,
   abstract = {Energy consumption in buildings contributes to over a third of global energy consumption and 28% of greenhouse gas emissions. With urbanization and population growth, rising building energy demand can lead to environmental degradation. While significant renewable resources are used to generate electricity to mitigate environmental problems, demand-side management remains crucial for achieving net-zero emissions and enhancing energy efficiency. Accurate building load forecasting is pivotal in devising optimal demand response schemes to shift or reduce the demand on power grids. Recent studies have achieved progressive breakthroughs in building energy forecasting through machine learning algorithms. However, most studies focused on building-level energy forecasting rather than individual load forecasting, which cannot support controlled demand response programs. In this study, we propose a multi-task learning model incorporating Patch, Temporal Convolutional Network and Time-Series Transformer (PatchTCN-TST) based on the channel-independent strategy for floor-level multiple electricity loads and indoor environmental forecasting. The PatchTCN-TST model is implemented to predict future data ranging from one-step ahead to three-step ahead on a real-world office building in Bangkok, Thailand. The experiment results indicate that the prediction performance of our model outperforms the prevalent methods, including LSTM, GRU, TCN, Transformer, Informer and Autoformer. The PatchTCN-TST model demonstrates superior accuracy in three forecasting scenarios, significantly reducing MAE, MSE, RMSE, and aSMAPE by 34%, 23%, 12%, and 36.4%, respectively, compared to the best baseline model.},
   author = {Senfeng Cen and Chang Gyoon Lim},
   doi = {10.1109/ACCESS.2024.3355448},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Deep learning,demand response,smart buildings,time-series forecasting},
   pages = {19553-19568},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multi-Task Learning of the PatchTCN-TST Model for Short-Term Multi-Load Energy Forecasting Considering Indoor Environments in a Smart Building},
   volume = {12},
   year = {2024},
}
@article{Zhang2022,
   abstract = {This paper develops an intelligent grid-interactive building controller, which optimizes building operation during both normal hours and demand response (DR) events. To avoid costly on-demand computation and to adapt to non-linear building models, the controller utilizes reinforcement learning (RL) and makes real-time decisions based on a near-optimal control policy. Learning such a policy typically amounts to solving a hard non-convex optimization problem. We propose to address this problem with a novel global-local policy search method. In the first stage, an RL algorithm based on zero-order gradient estimation is leveraged to search for the optimal policy globally, due to its scalability and the potential to escape some poor performing local optima. The obtained policy is then fine-tuned locally to bring the first-stage solution closer to that of the original unsmoothed problem. Experiments on a simulated five-zone commercial building demonstrate the advantages of the proposed method over existing learning approaches. They also show that the learned control policy outperforms a pragmatic linear model predictive controller (MPC) and approaches the performance of an oracle MPC in testing scenarios. Using a state-of-the-art advanced computing system, we demonstrate that the controller can be learned and deployed within hours of training.},
   author = {Xiangyu Zhang and Yue Chen and Andrey Bernstein and Rohit Chintala and Peter Graf and Xin Jin and David Biagioni},
   doi = {10.1109/TSG.2022.3141625},
   issn = {19493061},
   issue = {3},
   journal = {IEEE Transactions on Smart Grid},
   keywords = {Reinforcement learning,demand response,smart building,zero-order gradient estimation},
   month = {5},
   pages = {1976-1987},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Two-Stage Reinforcement Learning Policy Search for Grid-Interactive Building Control},
   volume = {13},
   year = {2022},
}
@article{Song2024,
   abstract = {Despite the maturity of ensemble numerical weather prediction (NWP), the resulting forecasts are still, more often than not, under-dispersed. As such, forecast calibration tools have become popular. Among those tools, quantile regression (QR) is highly competitive in terms of both flexibility and predictive performance. Nevertheless, a long-standing problem of QR is quantile crossing, which greatly limits the interpretability of QR-calibrated forecasts. On this point, this study proposes a non-crossing quantile regression neural network (NCQRNN), for calibrating ensemble NWP forecasts into a set of reliable quantile forecasts without crossing. The overarching design principle of NCQRNN is to add on top of the conventional QRNN structure another hidden layer, which imposes a non-decreasing mapping between the combined output from nodes of the last hidden layer to the nodes of the output layer, through a triangular weight matrix with positive entries. The empirical part of the work considers a solar irradiance case study, in which four years of ensemble irradiance forecasts at seven locations, issued by the European Centre for Medium-Range Weather Forecasts, are calibrated via NCQRNN, as well as via an eclectic mix of benchmarking models, ranging from the naïve climatology to the state-of-the-art deep-learning and other non-crossing models. Formal and stringent forecast verification suggests that the forecasts post-processed via NCQRNN attain the maximum sharpness subject to calibration, amongst all competitors. Furthermore, the proposed conception to resolve quantile crossing is remarkably simple yet general, and thus has broad applicability as it can be integrated with many shallow- and deep-learning-based neural networks.},
   author = {Mengmeng Song and Dazhi Yang and Sebastian Lerch and Xiang’ao Xia and Gokhan Mert Yagli and Jamie M. Bright and Yanbo Shen and Bai Liu and Xingli Liu and Martin János Mayer},
   doi = {10.1007/s00376-023-3184-5},
   issn = {18619533},
   journal = {Advances in Atmospheric Sciences},
   keywords = {CORP reliability diagram,ensemble weather forecasting,forecast calibration,non-crossing quantile regression neural network,post-processing},
   publisher = {Science Press},
   title = {Non-crossing Quantile Regression Neural Network as a Calibration Tool for Ensemble Weather Forecasts},
   year = {2024},
}
@article{Li2023,
   abstract = {This paper introduces a Generative Adversarial Nets (GAN) based, Load Profile
Inpainting Network (Load-PIN) for restoring missing load data segments and
estimating the baseline for a demand response event. The inputs are time series
load data before and after the inpainting period together with explanatory
variables (e.g., weather data). We propose a Generator structure consisting of
a coarse network and a fine-tuning network. The coarse network provides an
initial estimation of the data segment in the inpainting period. The
fine-tuning network consists of self-attention blocks and gated convolution
layers for adjusting the initial estimations. Loss functions are specially
designed for the fine-tuning and the discriminator networks to enhance both the
point-to-point accuracy and realisticness of the results. We test the Load-PIN
on three real-world data sets for two applications: patching missing data and
deriving baselines of conservation voltage reduction (CVR) events. We benchmark
the performance of Load-PIN with five existing deep-learning methods. Our
simulation results show that, compared with the state-of-the-art methods,
Load-PIN can handle varying-length missing data events and achieve 15-30%
accuracy improvement.},
   author = {Yiyan Li and Lidong Song and Yi Hu and Hanpyo Lee and Di Wu and PJ Rehm and Ning Lu},
   doi = {10.1109/tsg.2023.3293188},
   issn = {1949-3053},
   journal = {IEEE Transactions on Smart Grid},
   month = {7},
   pages = {1-1},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Load Profile Inpainting for Missing Load Data Restoration and Baseline Estimation},
   year = {2023},
}
@misc{Ma2023,
   author = {X Ma and D Mccaughey and D Quintana and D Wu and R Major and K Carbonnier and R Rebich and T Barham and A Miller},
   title = {Energy Northwest-Advanced Grid Interactive Load Efficiency (AGILE) A Techno-economic Assessment Prepared for Energy Northwest},
   year = {2023},
}
@article{Farakhor2023,
   abstract = {This article presents a novel modular, reconfigurable battery energy storage system. The proposed design is characterized by a tight integration of reconfigurable power switches and DC/DC converters. This characteristic enables the isolation of faulty cells from the system and allows fine power control for individual cells toward optimal system-level performance. An optimal power management approach is developed to extensively exploit the merits of the proposed design. Based on receding-horizon convex optimization, this approach aims to minimize the total power losses in charging/discharging while allocating the power in line with each cell's condition to achieve state-of-charge (SoC) and temperature balancing. By appropriate design, the approach manages to regulate the power of a cell across its full SoC range and guarantees the feasibility of the optimization problem. We perform extensive simulations and further develop a lab-scale prototype to validate the proposed system design and power management approach.},
   author = {Amir Farakhor and Di Wu and Yebin Wang and Huazhen Fang},
   doi = {10.1109/TTE.2022.3223993},
   issn = {23327782},
   issue = {2},
   journal = {IEEE Transactions on Transportation Electrification},
   keywords = {Battery management systems (BMSs),cell balancing,convex optimization,reconfigurable battery energy storage systems (RBESSs)},
   month = {6},
   pages = {2878-2890},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Novel Modular, Reconfigurable Battery Energy Storage System: Design, Control, and Experimentation},
   volume = {9},
   year = {2023},
}
@article{Wang2024,
   abstract = {Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.},
   author = {Chien-Yao Wang and I-Hau Yeh and Hong-Yuan Mark Liao},
   month = {2},
   title = {YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information},
   url = {http://arxiv.org/abs/2402.13616},
   year = {2024},
}
@article{Hu2023,
   abstract = {Data-driven forecasting techniques have been widely used for building load forecasting due to their accuracy and wide availability of operational data. Recent advances have been underpinned by the increased capability of machine learning (ML) algorithms; however, most studies only tested ML techniques on a single or a small number of buildings over short periods, lacking reliable tests. Moreover, few studies focused on the effects of characteristics of building load profiles on forecast accuracy, lacking the interpretation of ML-based prediction results. In this study, we investigate the impacts of building load dispersion level on its best load forecasting accuracy, which is obtained by comparing the forecasting performances of 11 prediction models over 9 weeks for 56 British non-domestic buildings. We find that conventional shallow ML models still outperform the increasingly popular deep learning models for time-series load forecasting, and ensemble learning can help improve forecast accuracy by integrating diverse individual models. We demonstrate that each building's best forecasting performance is largely influenced by the load dispersion level. In practice, the proposed dispersion metrics are recommended to quantify load dispersion levels before model development. For a building with a low dispersion level, the simple persistence model has satisfactory performance and could be directly used for design, control, and fault diagnosis of building energy systems for energy efficiency and energy flexibility.},
   author = {Maomao Hu and Bruce Stephen and Jethro Browell and Stephen Haben and David C.H. Wallom},
   doi = {10.1016/j.enbuild.2023.112896},
   issn = {03787788},
   journal = {Energy and Buildings},
   keywords = {Building load forecasting,Deep learning,Long short term memory,Machine learning,Reliability and interpretability},
   month = {4},
   publisher = {Elsevier Ltd},
   title = {Impacts of building load dispersion level on its load forecasting accuracy: Data or algorithms? Importance of reliability and interpretability in machine learning},
   volume = {285},
   year = {2023},
}
@article{Wang2024,
   abstract = {Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit\{generate high-performing neural network parameters\}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models.},
   author = {Kai Wang and Zhaopan Xu and Yukun Zhou and Zelin Zang and Trevor Darrell and Zhuang Liu and Yang You},
   month = {2},
   title = {Neural Network Diffusion},
   url = {http://arxiv.org/abs/2402.13144},
   year = {2024},
}
@misc{Gagnon2023,
   author = {Pieter Gagnon and Pedro Andres Sanchez Perez and Kodi Obika and Marty Schwarz and James Morris and Jianli Gu and Jordan Eisenman},
   title = {Cambium 2023 Scenario Descriptions and Documentation},
   url = {www.nrel.gov/publications.},
   year = {2023},
}
@article{,
   abstract = {The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.},
   author = {Johan Obando-Ceron and Ghada Sokar and Timon Willi and Clare Lyle and Jesse Farebrother and Jakob Foerster and Gintare Karolina Dziugaite and Doina Precup and Pablo Samuel Castro},
   month = {2},
   title = {Mixtures of Experts Unlock Parameter Scaling for Deep RL},
   url = {http://arxiv.org/abs/2402.08609},
   year = {2024},
}
@article{Biagioni2022,
   abstract = {Grid-interactive building control is a challenging and important problem for reducing carbon emissions, increasing energy efficiency, and supporting the electric power grid. Currently researchers and practitioners are confronted with a choice of control strategies ranging from model-free (purely data-driven) to model-based (directly incorporating physical knowledge) to hybrid methods that combine data and models. In this work, we identify state-of-the-art methods that span this methodological spectrum and evaluate their performance for multi-zone building HVAC control in the context of three demand response programs. We demonstrate, in this context, that hybrid methods offer many benefits over both purely model-free and model-based methods as long as certain requirements are met. In particular, hybrid controllers are relatively sample efficient, fast online, and high accuracy so long as the test case falls within the distribution of training data. Like all data-driven methods, hybrid controllers are still subject to generalization errors when applied to out-of-sample scenarios. Key takeaways for control strategies are summarized and the developed software framework is open-sourced.},
   author = {David Biagioni and Xiangyu Zhang and Christiane Adcock and Michael Sinner and Peter Graf and Jennifer King},
   month = {10},
   title = {From Model-Based to Model-Free: Learning Building Control for Demand Response},
   url = {http://arxiv.org/abs/2210.10203},
   year = {2022},
}
@article{Shaqour2022,
   abstract = {Modern power grids face the challenge of increasing renewable energy penetration that is stochastic in nature and calls for accurate demand predictions to provide the optimized power supply. Hence, increasing the self-consumption of renewable energy through demand response in households, local communities, and micro-grids is essential and calls for high demand prediction performance at lower levels of demand aggregations to achieve optimal performance. Although many of the recent studies have investigated both macro and micro scale short-term load forecasting (STLF), a comprehensive investigation on the effects of electrical demand aggregation size on STLF is minimal, especially with large sample sizes, where it is essential for optimal sizing of residential micro-grids, demand response markets, and virtual power plants. Hence, this study comprehensively investigates STLF of five aggregation levels (3, 10, 30, 100, and 479) based on a dataset of 479 residential dwellings in Osaka, Japan, with a sample size of (159, 47, 15, 4, and 1) per level, respectively, and investigates the underlying challenges in lower aggregation forecasting. Five deep learning (DL) methods are utilized for STLF and fine-tuned with extensive methodological sensitivity analysis and a variation of early stopping, where a detailed comparative analysis is developed. The test results reveal that a MAPE of (2.47–3.31%) close to country levels can be achieved on the highest aggregation, and below 10% can be sustained at 30 aggregated dwellings. Furthermore, the deep neural network (DNN) achieved the highest performance, followed by the Bi-directional Gated recurrent unit with fully connected layers (Bi-GRU-FCL), which had close to 15% faster training time and 40% fewer learnable parameters.},
   author = {Ayas Shaqour and Tetsushi Ono and Aya Hagishima and Hooman Farzaneh},
   doi = {10.1016/j.egyai.2022.100141},
   issn = {26665468},
   journal = {Energy and AI},
   keywords = {Bidirectional gated recurrent units,Convolutional neural network,Deep Neural Networks,Recurrent neural network,Residential load aggregation,Short-term load forecasting},
   month = {5},
   publisher = {Elsevier B.V.},
   title = {Electrical demand aggregation effects on the performance of deep learning-based short-term load forecasting of a residential building},
   volume = {8},
   year = {2022},
}
@article{,
   abstract = {Developing an appropriate model for accurate prediction of energy consumption is very essential for developing an effective energy management system for residential buildings. In view of this, the Short-term Load Forecasting (STLF) of household appliances has been performing an important role in supervising and managing energy in the residential community. In the domain of big data analytics, data-driven load forecasting approaches have realized an amazing performance in the recognition of patterns of residential electric loads and forecasting energy consumption. Nevertheless, current research emphasizes the use of powerful feature-engineering methods, which are ineffective and result in low generalization performance. Further, considering the differences in the consumption behavior of various home appliances, it is unfeasible to discover energy consumption characteristics physically in the power system. Thus, this study addresses the problems of STLF using a novel two-stream deep learning (DL) model called STLF-Net. The first stream is designed with Gated Recurrent Units (GRUs) to learn and capture the long-term temporal representations of the energy utilization data. Simultaneously, in the second stream, the short-term information and positional representations are modeled using a stack of temporal convolutional (TC) modules. The TC module is designated using dilated causal convolutions and residual connection to enable efficient feature extraction while alleviating the gradient vanishing issues. The learned representations from the two streams are fused and subsequently passed to several dense layers to generate the final hour-ahead load forecasts. Experimental assessments on two public energy consumption predictions datasets (IHEPC and AEP) demonstrated the superior performance of the STLF-Net over the recent cutting-edge data-driven approaches.},
   author = {Mohamed Abdel-Basset and Hossam Hawash and Karam Sallam and S. S. Askar and Mohamed Abouhawwash},
   doi = {10.1016/j.jksuci.2022.04.016},
   issn = {22131248},
   issue = {7},
   journal = {Journal of King Saud University - Computer and Information Sciences},
   keywords = {Deep learning,Gated recurrent units,Load forecasting,Residential energy consumption,Temporal convolutions},
   month = {7},
   pages = {4296-4311},
   publisher = {King Saud bin Abdulaziz University},
   title = {STLF-Net: Two-stream deep network for short-term load forecasting in residential buildings},
   volume = {34},
   year = {2022},
}
@article{,
   abstract = {The use of deep learning for electrical demand forecasting has shown great potential in generating accurate results, but requires a large amount of data to train the models. However, the limited availability of electricity consumption information for new users or recently monitoring buildings in local energy communities make it difficult to achieve these good results. To address this challenge, this research study proposes a novel methodology based on the combination of the transfer learning (TL) concept and the temporal fusion transformer (TFT) architecture. This is an innovative approach used to transfer knowledge from a general pre-trained model based on the historical data of the community's buildings to new users, reducing the needs for large amounts of data for each new building. The results show that TFT provides more accurate load estimates than other state-of-the-art methods by reducing RMSE by more than 11%. Moreover, the TL approach improves the prediction of load demand of buildings with limited availability of historical data, reducing CV_RMSE, SMAPE, and WQLoss by more than 40% compared to models that do not use knowledge transfer. This is a significant improvement that facilitates the incorporation of new users to the community achieving accurate load estimations even with lack of data. Moreover, this approach leads to an optimized and cost-efficient planning and management of energy distribution in local energy communities.},
   author = {Miguel López Santos and Saúl Díaz García and Xela García-Santiago and Ana Ogando-Martínez and Fernando Echevarría Camarero and Gonzalo Blázquez Gil and Pablo Carrasco Ortega},
   doi = {10.1016/j.enbuild.2023.113164},
   issn = {03787788},
   journal = {Energy and Buildings},
   keywords = {Limited data availability,Local energy communities,Short-term load forecasting,TFT,Transfer learning},
   month = {8},
   publisher = {Elsevier Ltd},
   title = {Deep learning and transfer learning techniques applied to short-term load forecasting of data-poor buildings in local energy communities},
   volume = {292},
   year = {2023},
}
@article{Jiang2022,
   abstract = {Very short-term load forecasting (VSLTF) plays an essential role in guaranteeing effective electricity dispatching and generating in residential microgrid systems. However, the extreme fluctuations and irregular data patterns of VSTLF have brought severe challenges to accurate forecasting. Deep learning methods have been mostly utilized in time series predicting tasks like load forecasting. Recently, an Autoformer neural network has been proposed in many time series forecasting scenarios. Based on Autoformer, this paper proposes a new Deep-Autoformer framework, where the extra MLP layers are added to the basic Autoformer framework for a more efficient deep information extraction. Taking a microgrid system in Austin, Texas from the Pecan Street dataset as a case study, Deep-Autoformer and other five baseline models are utilized to forecast the load data of 15 min and one-hour time resolution. The main contributions of the proposed Deep-Autoformer are: (i)the experiment results indicate that the proposed Deep-Autoformer has achieved State-Of-The-Art (SOTA) results in both VSTLF and STLF, and(ii) the ‘deep’ method, where the MLP layers are added in the appropriate positions of the neural network, can contribute to more efficient feature extraction. Moreover, due to the unintuitive phenomenons in the experiment, two hypotheses are also proposed: (i) the long-ago historical data may affect the performance of the auto-correlation mechanism of the Autoformer, and (ii) models are probably overfitting the historical patterns if the time series data are too long. Overall, the proposed Deep-Autoformer can provide a feasible approach and a new baseline for the real application of VSTLF.},
   author = {Yuqi Jiang and Tianlu Gao and Yuxin Dai and Ruiqi Si and Jun Hao and Jun Zhang and David Wenzhong Gao},
   doi = {10.1016/j.apenergy.2022.120120},
   issn = {03062619},
   journal = {Applied Energy},
   keywords = {Auto-correlation,Deep-autoformer,Fluctuation forecasting accuracy index,Time series decomposition,Very short-term load forecasting},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {Very short-term residential load forecasting based on deep-autoformer},
   volume = {328},
   year = {2022},
}
@inproceedings{Emami2023,
   abstract = {In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define and learn periodic multi-agent policies. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effectively learn multi-timescale policies is validated on a gridworld and building energy management environment.},
   author = {Patrick Emami and Xiangyu Zhang and David Biagioni and Ahmed S. Zamzam},
   doi = {10.1109/CDC49753.2023.10384223},
   isbn = {9798350301243},
   issn = {25762370},
   journal = {Proceedings of the IEEE Conference on Decision and Control},
   pages = {2372-2378},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning},
   year = {2023},
}
@inproceedings{Wu2022,
   abstract = {With the increasing popularity of electric vehicles and the growing trend of working from home, electricity consumption in the residential sector is expected to continue to grow rapidly over the next few years. As a consequence, short-term residential load forecasting is becoming even more vital for the reliability and sustainability of the smart grid. Although deep learning models have shown impressive success in different areas including short-term electric load forecasting, such models require a large amount of training data. For many real-world load forecasting cases, we may not have enough training data to learn a reliable forecasting model. In this paper, we address this challenge through the use of boosting-based transfer learning with multiple sources. We first train a set of deep regression models on source houses that can provide relatively abundant data. We then transfer these learned models via the boosting framework to support data-scarce target houses. The transfer process is selective and customized for each target house to minimize the potential for negative transfer. Experimental results, based on real-world residential data sets, show that the proposed method can significantly improve forecasting accuracy.},
   author = {Di Wu and Yi Tian Xu and Michael Jenkin and Ju Wang and Hang Li and Xue Liu and Gregory Dudek},
   doi = {10.1109/ICC45855.2022.9838983},
   isbn = {9781538683477},
   issn = {15503607},
   journal = {IEEE International Conference on Communications},
   keywords = {Electric load forecasting,boosting,regression,transfer learning},
   pages = {5530-5536},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Short-term Load Forecasting with Deep Boosting Transfer Regression},
   volume = {2022-May},
   year = {2022},
}
@inproceedings{Bohara2022,
   abstract = {Higher penetration of renewable and smart home technologies at the residential level challenges grid stability as utility-customer interactions add complexity to power system operations. In response, short-term residential load forecasting has become an increasing area of focus. However, forecasting at the residential level is challenging due to the higher uncertainties involved. Recently deep neural networks have been leveraged to address this issue. This paper investigates the capabilities of a bidirectional long short-term memory (BiLSTM) and a convolutional neural network-based BiLSTM (CNN-BiLSTM) to provide a day ahead (24 hr.) forecasting at an hourly resolution while minimizing the root mean squared error (RMSE) between the actual and predicted load demand. Using a publicly available dataset consisting of 34 homes, the BiLSTM and CNN-BiLSTM models are trained to forecast the aggregated active power demand for each hour within a 24 hr. span, given the previous 24 hr. load data. The BiLSTM model achieved the lowest RMSE of 1.4842 for the overall daily forecast. In addition, standard LSTM and CNN-LSTM models are trained and compared with the BiLSTM architecture. The RMSE of BiLSTM is 5.60%, 2.85% and 2.60% lower than LSTM, CNN-LSTM and CNN-BiLSTM models respectively.},
   author = {Bharat Bohara and Raymond I. Fernandez and Vysali Gollapudi and Xingpeng Li},
   doi = {10.1109/3ICT56508.2022.9990696},
   isbn = {9781665451932},
   journal = {2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies, 3ICT 2022},
   keywords = {Bidirectional long short-term memory (BiLSTM),Deep Neural Networks (DNN),Energy Management,Load forecasting,Long short-term memory (LSTM),Optimal load dispatch},
   pages = {37-43},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Short-Term Aggregated Residential Load Forecasting using BiLSTM and CNN-BiLSTM},
   year = {2022},
}
@article{Wu2022,
   abstract = {This paper presents a novel dispatch and evaluation framework for battery energy storage systems (BESSs) to minimize a load servicing entity's coincident demand during system peak hours. The framework consists of i) a two-step BESS dispatch process that accounts for uncertainties in forecasting system peak and using limited battery cycle life, and ii) procedures to design control parameters, determine BESS duration, and estimate the corresponding net benefits. In the proposed dispatch, a rule-based triggering mechanism is executed to determine whether to dispatch a BESS on an operating day by comparing the peak-day probability with a predetermined threshold. Once the dispatch is triggered, a model predictive control is carried out to maximize the expected reduction in peak demand. By exercising this two-step dispatch method with different thresholds, one can explore the trade-off between peak demand reduction effectiveness and loss of battery life, and thereby identify the optimal thresholds to maximize cumulative economic benefits. Case studies are conducted using the data provided by utilities in North Carolina. Simulation results are presented to demonstrate the effectiveness of the proposed method.},
   author = {Di Wu and Xu Ma and Tao Fu and Zhangshuan Hou and P. J. Rehm and Ning Lu},
   doi = {10.1109/OAJPE.2022.3196690},
   issn = {26877910},
   journal = {IEEE Open Access Journal of Power and Energy},
   keywords = {Battery energy management system,battery degradation,capacity charge,optimization,uncertainty},
   pages = {351-360},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Design of a Battery Energy Management System for Capacity Charge Reduction},
   volume = {9},
   year = {2022},
}
@inproceedings{Ma2023,
   abstract = {The economic assessment of a hybrid energy system (HES) pairing battery energy storage systems (BESSs) and pho-tovoltaics is highly important to advancing their deployment for a decarbonized grid. This paper presents an innovative control and assessment framework for using the HES for bundling services, including energy charge reduction, demand charge reduction, and transmission charge reduction. To tackle the uncertainty associated with the load, on each operating day, the probability of the monthly peak load occurring on the day is generated and compared with a preset threshold to determine whether to activate the dispatch of BESS. If the dispatch of BESS is triggered, a model predictive control is then carried out to minimize the total operational cost. By exercising this policy with different thresholds, one can explore the trade-offs between short- term benefits and battery lifetime, and identify an optimal threshold that maximizes the present value benefits over the battery lifetime. To illustrate the proposed framework, we also present an evaluation study of a real-world HES project deployed at Horn Rapids in the City of Richland, Washington.},
   author = {Xu Ma and Di Wu and Alasdair Crawford},
   doi = {10.1109/ISGT51731.2023.10066460},
   isbn = {9781665453554},
   journal = {2023 IEEE Power and Energy Society Innovative Smart Grid Technologies Conference, ISGT 2023},
   keywords = {Battery energy storage systems,load forecast uncertainties,model predictive control,optimal dispatch},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Incorporating Operational Uncertainties into the Dispatch of an Integrated Solar and Storage System},
   year = {2023},
}
@article{Ma2024,
   abstract = {Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary \{-1, 0, 1\}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
   author = {Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
   month = {2},
   title = {The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits},
   url = {http://arxiv.org/abs/2402.17764},
   year = {2024},
}
@inproceedings{Kim2023,
   abstract = {This paper presents an independent component analysis (ICA) based unsupervised-learning method for heat, ventilation, and air-conditioning (HVAC) load disaggregation using low-resolution (e.g., 15 minutes) smart meter data. We first demonstrate that electricity consumption profiles on mild-temperature days can be used to estimate the non-HVAC base load on hot days. A residual load profile can then be calculated by subtracting the mild-day load profile from the hot-day load profile. The residual load profiles are processed using ICA for HVAC load extraction. An optimization-based algorithm is proposed for post-adjustment of the ICA results, considering two bounding factors for enhancing the robustness of the ICA algorithm. First, we use the hourly HVAC energy bounds computed based on the relationship between HVAC load and temperature to remove unrealistic HVAC load spikes. Second, we exploit the dependency between the daily nocturnal and diurnal loads extracted from historical meter data to smooth the base load profile. Pecan Street data with sub-metered HVAC data were used to test and validate the proposed methods. Simulation results demonstrated that the proposed method is computationally efficient and robust across multiple customers.},
   author = {Hyeonjin Kim and Kai Ye and Han Pyo Lee and Rongxing Hu and Ning Lu and Di Wu and P. J. Rehm},
   doi = {10.1109/ISGT51731.2023.10066402},
   isbn = {9781665453554},
   journal = {2023 IEEE Power and Energy Society Innovative Smart Grid Technologies Conference, ISGT 2023},
   keywords = {HVAC system,Independent component analysis,Non-intrusive load monitoring,Smart meter data},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An ICA-Based HVAC Load Disaggregation Method Using Smart Meter Data},
   year = {2023},
}
@inproceedings{Wu2022,
   abstract = {The modern power system is transitioning towards increasing penetration of renewable energy generation and demand from different types of electrical appliances. With this transition, residential load forecasting, especially short-term load forecasting (STLF), is becoming more and more challenging and important. Accurate short-term load forecasting can help improve energy dispatching efficiency and, as a consequence, reduce overall power system operation cost. Most current load forecasting algorithms assume that there is a large amount of training data available upon which to learn a reliable load forecasting model. However, this assumption can be challenging for real-world applications. In this work, we first propose the use of transfer learning and an attention mechanism to improve short-term load forecasting for a target domain with only a limited amount of available data. Furthermore, we extend the proposed method to utilize heterogeneous features which enables the approach to deal with more complex scenarios in the real world. Experimental results using real-world data sets show that the proposed methods can improve forecasting accuracy by a large margin over several existing baselines.},
   author = {Di Wu and Michael Jenkin and Yi Tian Xu and Xue Liu and Gregory Dudek},
   doi = {10.1109/GLOBECOM48099.2022.10001524},
   issn = {25766813},
   journal = {Proceedings - IEEE Global Communications Conference, GLOBECOM},
   keywords = {attention,residential load forecasting,transfer learning},
   pages = {5285-5291},
   title = {Attentive Knowledge Transfer for Short-term Load Forecasting},
   year = {2022},
}
@inproceedings{Zhu2023,
   abstract = {This paper presents an innovative multi-objective assessment and sizing framework for battery energy storage systems paired with photovoltaics for a cost-effective and resilient microgrid. In the proposed framework, we simultaneously consider i) economic benefits from bundling grid services in grid-connected mode and ii) enhanced resilience by operating the microgrid in island mode upon outages of the main grid. We first develop a survivability evaluation method to quantify the resilience of a microgrid. Next, with resilience and economic benefits being formulated as two separate objectives, we develop a multiobjective optimization to explore different sizing and dispatch options and generate Pareto efficient solutions to assist decisionmaking in system design. Case studies on a real-world microgrid project in the Puget Sound Energy's system are presented to illustrate the proposed modeling and sizing methods.},
   author = {Yanyan Zhu and Xu Ma and Di Wu and Joseph Do},
   doi = {10.1109/PESGM52003.2023.10253339},
   isbn = {9781665464413},
   issn = {19449933},
   journal = {IEEE Power and Energy Society General Meeting},
   keywords = {Battery energy storage systems,microgrids,optimal sizing,photovoltaics,resilience enhancement},
   publisher = {IEEE Computer Society},
   title = {A Multi-objective Microgrid Assessment and Sizing Framework for Economic and Resilience Benefits},
   volume = {2023-July},
   year = {2023},
}
@article{Huang2019,
   abstract = {Accurate day-ahead individual residential load forecasting is of great importance to various applications of smart grid on day-ahead market. Deep learning, as a powerful machine learning technology, has shown great advantages and promising application in load forecasting tasks. However, deep learning is a computationally-hungry method, and requires high costs (e.g., time, energy and CO2 emission) to train a deep learning model, which aggravates the energy crisis and incurs a substantial burden to the environment. As a consequence, the deep learning methods are difficult to be popularized and applied in the real smart grid environment. In this paper, we propose a low training cost model based on convolutional neural network, namely LoadCNN, for next-day load forecasting of individual resident with reduced training cost. The experiments show that the training time of LoadCNN is only approximately 1/54 of the one of other state-of-the-art models, and energy consumption and CO2 emissions are only approximate 1/45 of those of other state-of-the-art models based on the same indicators. Meanwhile, the prediction accuracy of our model is equal to that of current state-of-the-art models, making LoadCNN the first load forecasting model simultaneously achieving high prediction accuracy and low training costs. LoadCNN is an efficient green model that is able to be quickly, cost-effectively and environmentally-friendly deployed in a realistic smart grid environment.},
   author = {Yunyou Huang and Nana Wang and Wanling Gao and Xiaoxu Guo and Cheng Huang and Tianshu Hao and Jianfeng Zhan},
   month = {8},
   title = {LoadCNN: A Low Training Cost Deep Learning Model for Day-Ahead Individual Residential Load Forecasting},
   url = {http://arxiv.org/abs/1908.00298},
   year = {2019},
}
@article{Liu2024,
   abstract = {Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding.},
   author = {Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
   month = {2},
   title = {DoRA: Weight-Decomposed Low-Rank Adaptation},
   url = {http://arxiv.org/abs/2402.09353},
   year = {2024},
}
@article{Graf2024,
   abstract = {Neurosymbolic AI combines the interpretability, parsimony, and explicit reasoning of classical symbolic approaches with the statistical learning of data-driven neural approaches. Models and policies that are simultaneously differentiable and interpretable may be key enablers of this marriage. This paper demonstrates three pathways to implementing such models and policies in a real-world reinforcement learning setting. Specifically, we study a broad class of neural networks that build interpretable semantics directly into their architecture. We reveal and highlight both the potential and the essential difficulties of combining logic, simulation, and learning. One lesson is that learning benefits from continuity and differentiability, but classical logic is discrete and non-differentiable. The relaxation to real-valued, differentiable representations presents a trade-off; the more learnable, the less interpretable. Another lesson is that using logic in the context of a numerical simulation involves a non-trivial mapping from raw (e.g., real-valued time series) simulation data to logical predicates. Some open questions this note exposes include: What are the limits of rule-based controllers, and how learnable are they? Do the differentiable interpretable approaches discussed here scale to large, complex, uncertain systems? Can we truly achieve interpretability? We highlight these and other themes across the three approaches.},
   author = {Peter Graf and Patrick Emami},
   month = {2},
   title = {Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks},
   url = {http://arxiv.org/abs/2402.05307},
   year = {2024},
}
@article{Wu2022,
   abstract = {Hydrogen energy storage (HES) transforms and stores electric energy from the grid into hydrogen, and supplements other energy storage and demand response resources in addressing challenges in renewable-intensive power systems. Understanding how to optimally utilize an HES system to maximize its economic benefits from stacked value streams is highly important to its development and deployment. This paper presents a techno-economic assessment framework for an HES system considering three common energy delivery pathways and multiple grid and end-user services. Models are developed to capture the operational capability, flexibility, and constraints associated with hydrogen production, compression, storage, and utilization as well as different grid services in an economic assessment. To define the technically achievable benefits, an optimal dispatch formulation is proposed to maximize the economic benefits over a representative year with an hourly time step considering the trade-offs among different value streams. Representative case studies are designed and carried out to show how system configuration, energy delivery pathways, and grid services may affect economic benefits. It was found that value streams from bundling grid services account for up to 76% of the total benefits and are critical for an HES project to be financially viable.},
   author = {Di Wu and Dexin Wang and Thiagarajan Ramachandran and Jamie Holladay},
   doi = {10.1016/j.energy.2022.123638},
   issn = {03605442},
   journal = {Energy},
   keywords = {Economic analysis,Energy storage,Grid services,Hydrogen electrolysis,Optimization,Stacked value streams},
   month = {6},
   publisher = {Elsevier Ltd},
   title = {A techno-economic assessment framework for hydrogen energy storage toward multiple energy delivery pathways and grid services},
   volume = {249},
   year = {2022},
}
@article{So2023,
   abstract = {The growth of urban areas and the management of energy resources highlight the need for precise short-term load forecasting (STLF) in energy management systems to improve economic gains and reduce peak energy usage. Traditional deep learning models for STLF present challenges in addressing these demands efficiently due to their limitations in modeling complex temporal dependencies and processing large amounts of data. This study presents a groundbreaking hybrid deep learning model, BiGTA-net, which integrates a bi-directional gated recurrent unit (Bi-GRU), a temporal convolutional network (TCN), and an attention mechanism. Designed explicitly for day-ahead 24-point multistep-ahead building electricity consumption forecasting, BiGTA-net undergoes rigorous testing against diverse neural networks and activation functions. Its performance is marked by the lowest mean absolute percentage error (MAPE) of 5.37 and a root mean squared error (RMSE) of 171.3 on an educational building dataset. Furthermore, it exhibits flexibility and competitive accuracy on the Appliances Energy Prediction (AEP) dataset. Compared to traditional deep learning models, BiGTA-net reports a remarkable average improvement of approximately 36.9% in MAPE. This advancement emphasizes the model’s significant contribution to energy management and load forecasting, accentuating the efficacy of the proposed hybrid approach in power system optimizations and smart city energy enhancements.},
   author = {Dayeong So and Jinyeong Oh and Insu Jeon and Jihoon Moon and Miyoung Lee and Seungmin Rho},
   doi = {10.3390/systems11090456},
   issn = {20798954},
   issue = {9},
   journal = {Systems},
   keywords = {bi-directional gated recurrent unit,building energy forecasting,energy management system,hybrid deep learning model,short-term load forecasting,temporal convolutional network},
   month = {9},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {BiGTA-Net: A Hybrid Deep Learning-Based Electrical Energy Forecasting Model for Building Energy Management Systems},
   volume = {11},
   year = {2023},
}
@article{Gonzalez2023,
   abstract = {The need for and interest in very-short-term load forecasting (VSTLF) is increasing and important for goals such as energy pricing markets. There is greater challenge in predicting load consumption for residential-load-type data, which is highly variable in nature and does not form visible patterns present in aggregated nodal-type load data. Previous works have used methods such as LSTM and CNN for VSTLF; however, the use of DNN has yet to be investigated. Furthermore, DNNs have been effectively used in STLF but have not been applied to very-short-term time frames. In this work, a deep network architecture is proposed and applied to very-short-term forecasting of residential load patterns that exhibit high variability and abrupt changes. The method extends previous work by including delayed load demand as an input, as well as working for 1 min data resolution. The deep model is trained on the load demand data of selected days—one, two, and a week—prior to the targeted day. Test results on real-world residential load patterns encompassing a set of 32 days (a sample from different seasons and special days) exhibit the efficiency of the deep network in providing high-accuracy residential forecasts, as measured with three different error metrics, namely MSE, RMSE, and MAPE. On average, MSE and RMSE are lower than 0.51 kW and 0.69 kW, and MAPE lower than 0.51%.},
   author = {Reynaldo Gonzalez and Sara Ahmed and Miltiadis Alamaniotis},
   doi = {10.3390/en16093636},
   issn = {19961073},
   issue = {9},
   journal = {Energies},
   keywords = {1 min data,deep neural network,individual household,parameter selection analysis,residential load,small data,very-short-term forecasting},
   month = {5},
   publisher = {MDPI},
   title = {Implementing Very-Short-Term Forecasting of Residential Load Demand Using a Deep Neural Network Architecture},
   volume = {16},
   year = {2023},
}
@misc{Yang2024,
   abstract = {Owing to the persisting hype in pushing toward global carbon neutrality, the study scope of atmospheric science is rapidly expanding. Among numerous trending topics, energy meteorology has been attracting the most attention hitherto. One essential skill of solar energy meteorologists is solar power curve modeling, which seeks to map irradiance and auxiliary weather variables to solar power, by statistical and/or physical means. In this regard, this tutorial review aims to deliver a complete overview of those fundamental scientific and engineering principles pertaining to the solar power curve. Solar power curves can be modeled in two primary ways, one of regression and the other of model chain. Both classes of modeling approaches, alongside their hybridization and probabilistic extensions, which allow accuracy improvement and uncertainty quantification, are scrutinized and contrasted thoroughly in this review.},
   author = {Dazhi Yang and Xiang’ao Xia and Martin János Mayer},
   doi = {10.1007/s00376-024-3229-4},
   issn = {18619533},
   journal = {Advances in Atmospheric Sciences},
   keywords = {energy meteorology,model chain,review,solar power curve,solar power prediction},
   publisher = {Science Press},
   title = {A Tutorial Review of the Solar Power Curve: Regressions, Model Chains, and Their Hybridization and Probabilistic Extensions},
   year = {2024},
}
@article{Salmi2020,
   abstract = {This paper presents a novel deep learning architecture for short-term load forecasting of building energy loads. The architecture is based on a simple base learner and multiple boosting systems that are modelled as a single deep neural network. The architecture transforms the original multivariate time series into multiple cascading univariate time series. Together with sparse interactions, parameter sharing and equivariant representations, this approach makes it possible to combat against overfitting while still achieving good presentation power with a deep network architecture. The architecture is evaluated in several short-term load forecasting tasks with energy data from an office building in Finland. The proposed architecture outperforms state-of-the-art load forecasting model in all the tasks.},
   author = {Tuukka Salmi and Jussi Kiljander and Daniel Pakkala},
   doi = {10.3390/en13092370},
   issn = {19961073},
   issue = {9},
   journal = {Energies},
   keywords = {Deep neural networks; short-term load forecasting},
   month = {5},
   publisher = {MDPI AG},
   title = {Stacked boosters network architecture for short-term load forecasting in buildings},
   volume = {13},
   year = {2020},
}
@article{Kirillov2023,
   abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
   author = {Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
   month = {4},
   title = {Segment Anything},
   url = {http://arxiv.org/abs/2304.02643},
   year = {2023},
}
@misc{Lopez2023,
   author = {Anthony Lopez and Pavlo Pinchuk and Michael Gleason and Wesley Cole Contributing and Trieu Mai and Travis Williams and Owen Roberts and Marie Rivers and Mike Bannister and Sophie-Min Thomson and Gabe Zuckerman and Brian Sergi},
   title = {Solar Photovoltaics and Land-Based Wind Technical Potential and Supply Curves for the Contiguous United States: 2023 Edition},
   url = {www.nrel.gov/publications.},
   year = {2023},
}
@article{Hewamalage2023,
   abstract = {Recent trends in the Machine Learning (ML) and in particular Deep Learning (DL) domains have demonstrated that with the availability of massive amounts of time series, ML and DL techniques are competitive in time series forecasting. Nevertheless, the different forms of non-stationarities associated with time series challenge the capabilities of data-driven ML models. Furthermore, due to the domain of forecasting being fostered mainly by statisticians and econometricians over the years, the concepts related to forecast evaluation are not the mainstream knowledge among ML researchers. We demonstrate in our work that as a consequence, ML researchers oftentimes adopt flawed evaluation practices which results in spurious conclusions suggesting methods that are not competitive in reality to be seemingly competitive. Therefore, in this work we provide a tutorial-like compilation of the details associated with forecast evaluation. This way, we intend to impart the information associated with forecast evaluation to fit the context of ML, as means of bridging the knowledge gap between traditional methods of forecasting and adopting current state-of-the-art ML techniques.We elaborate the details of the different problematic characteristics of time series such as non-normality and non-stationarities and how they are associated with common pitfalls in forecast evaluation. Best practices in forecast evaluation are outlined with respect to the different steps such as data partitioning, error calculation, statistical testing, and others. Further guidelines are also provided along selecting valid and suitable error measures depending on the specific characteristics of the dataset at hand.},
   author = {Hansika Hewamalage and Klaus Ackermann and Christoph Bergmeir},
   doi = {10.1007/s10618-022-00894-5},
   issn = {1573756X},
   issue = {2},
   journal = {Data Mining and Knowledge Discovery},
   keywords = {Forecast evaluation,Time series forecasting},
   month = {3},
   pages = {788-832},
   publisher = {Springer},
   title = {Forecast evaluation for data scientists: common pitfalls and best practices},
   volume = {37},
   year = {2023},
}
@article{Angelopoulos2024,
   abstract = {Background: The deployment of black-box AI models in medical imaging presents significant challenges, especially in maintaining reliability across different clinical settings. These challenges are compounded by distribution shifts that can lead to failures in reproducing the accuracy attained during the AI model's original validations. Method: We introduce the conformal triage algorithm, designed to categorize patients into low-risk, high-risk, and uncertain groups within a clinical deployment setting. This method leverages a combination of a black-box AI model and conformal prediction techniques to offer statistical guarantees of predictive power for each group. The high-risk group is guaranteed to have a high positive predictive value, while the low-risk group is assured a high negative predictive value. Prediction sets are never constructed; instead, conformal techniques directly assure high accuracy in both groups, even in clinical environments different from those in which the AI model was originally trained, thereby ameliorating the challenges posed by distribution shifts. Importantly, a representative data set of exams from the testing environment is required to ensure statistical validity. Results: The algorithm was tested using a head CT model previously developed by Do and colleagues [9] and a data set from Massachusetts General Hospital. The results demonstrate that the conformal triage algorithm provides reliable predictive value guarantees to a clinically significant extent, reducing the number of false positives from 233 (45%) to 8 (5%) while only abstaining from prediction on 14% of data points, even in a setting different from the training environment of the original AI model. Conclusions: The conformal triage algorithm offers a promising solution to the challenge of deploying black-box AI models in medical imaging across varying clinical settings. By providing statistical guarantees of predictive value for categorized patient groups, this approach significantly enhances the reliability and utility of AI in optimizing medical imaging workflows, particularly in neuroradiology.},
   author = {Anastasios N Angelopoulos and Stuart Pomerantz and Synho Do and Stephen Bates and Christopher P Bridge and Daniel C Elton and Michael H Lev and R Gilberto González and Michael I Jordan and Jitendra Malik},
   doi = {10.1101/2024.02.09.24302543},
   isbn = {10.1101/2024.02.0},
   title = {Conformal Triage for Medical Imaging AI Deployment},
   url = {https://doi.org/10.1101/2024.02.09.24302543},
   year = {2024},
}
@misc{Arlot2009,
   author = {Sylvain Arlot and Alain Celisse},
   title = {A survey of cross-validation procedures for model selection},
   year = {2009},
}
@article{Hewamalage2023,
   abstract = {Recent trends in the Machine Learning (ML) and in particular Deep Learning (DL) domains have demonstrated that with the availability of massive amounts of time series, ML and DL techniques are competitive in time series forecasting. Nevertheless, the different forms of non-stationarities associated with time series challenge the capabilities of data-driven ML models. Furthermore, due to the domain of forecasting being fostered mainly by statisticians and econometricians over the years, the concepts related to forecast evaluation are not the mainstream knowledge among ML researchers. We demonstrate in our work that as a consequence, ML researchers oftentimes adopt flawed evaluation practices which results in spurious conclusions suggesting methods that are not competitive in reality to be seemingly competitive. Therefore, in this work we provide a tutorial-like compilation of the details associated with forecast evaluation. This way, we intend to impart the information associated with forecast evaluation to fit the context of ML, as means of bridging the knowledge gap between traditional methods of forecasting and adopting current state-of-the-art ML techniques.We elaborate the details of the different problematic characteristics of time series such as non-normality and non-stationarities and how they are associated with common pitfalls in forecast evaluation. Best practices in forecast evaluation are outlined with respect to the different steps such as data partitioning, error calculation, statistical testing, and others. Further guidelines are also provided along selecting valid and suitable error measures depending on the specific characteristics of the dataset at hand.},
   author = {Hansika Hewamalage and Klaus Ackermann and Christoph Bergmeir},
   doi = {10.1007/s10618-022-00894-5},
   issn = {1573756X},
   issue = {2},
   journal = {Data Mining and Knowledge Discovery},
   keywords = {Forecast evaluation,Time series forecasting},
   month = {3},
   pages = {788-832},
   publisher = {Springer},
   title = {Forecast evaluation for data scientists: common pitfalls and best practices},
   volume = {37},
   year = {2023},
}
@misc{Myers2024,
   abstract = {Many institutions are grappling with how to support their employees and other constituents who drive electric vehicles (EVs) by providing local charging services. We formulate a novel "parking stall electrification" model that estimates con-stituents' charging needs and can guide institutional strategy for deploying and expanding workplace charging. Unlike most prior literature, our model explicitly incorporates commuter behavior-pivotal for any network built to operate under real-world commuter conditions-derived from recurring surveys of EV drivers (N =626) at the University of California San Diego (UCSD). We demonstrate the model at UCSD using these behavioral data and find that institutional goals and choice of chargers have a profound effect on commuters' network usage and stall electrification requirements. To support practitioners, we have made the full model and a sample dataset available to the public at https://tinyurl.com/2v262znh.},
   author = {Jeff Myers and Ryan Hanna and Emily Carlton and Teevrat Garg and Jan Kleissl and Sebastian Tebbe and David G Victor and Byron Washom and Josh Kavanagh},
   keywords = {Index Terms-Commuting,decarbonization,electric vehicle,electrification,transportation},
   title = {Integrating human behavior into planning models for workplace EV charging networks},
   url = {https://sebastiantebbe.github.io/uploads/Planning_model.pdf},
   year = {2024},
}
@article{Theodosiou2011,
   abstract = {This paper is a re-examination of the benefits and limitations of decomposition and combination techniques in the area of forecasting, and also a contribution to the field, offering a new forecasting method. The new method is based on the disaggregation of time series components through the STL decomposition procedure, the extrapolation of linear combinations of the disaggregated sub-series, and the reaggregation of the extrapolations to obtain estimates for the global series. Applying the forecasting method to data from the NN3 and M1 Competition series, the results suggest that it can perform well relative to four other standard statistical techniques from the literature, namely the ARIMA, Theta, Holt-Winters' and Holt's Damped Trend methods. The relative advantages of the new method are then investigated further relative to a simple combination of the four statistical methods and a Classical Decomposition forecasting method. The strength of the method lies in its ability to predict long lead times with relatively high levels of accuracy, and to perform consistently well for a wide range of time series, irrespective of the characteristics, underlying structure and level of noise of the data. © 2011 International Institute of Forecasters.},
   author = {Marina Theodosiou},
   doi = {10.1016/j.ijforecast.2010.11.002},
   issn = {01692070},
   issue = {4},
   journal = {International Journal of Forecasting},
   keywords = {ARIMA models,Combining forecasts,Decomposition,Evaluating forecasts,Forecasting competitions,Time series},
   month = {10},
   pages = {1178-1195},
   title = {Forecasting monthly and quarterly time series using STL decomposition},
   volume = {27},
   year = {2011},
}
@article{Gneiting2014,
   abstract = {A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibration, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilistic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharpness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts.We emphasizemethodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed ensemble forecasts in numerical weather prediction as an example. Throughout, we illustrate concepts and methodologies in data examples. © 2014 by Annual Reviews.},
   author = {Tilmann Gneiting and Matthias Katzfuss},
   doi = {10.1146/annurev-statistics-062713-085831},
   issn = {2326831X},
   journal = {Annual Review of Statistics and Its Application},
   keywords = {Calibration,Consistent scoring function,Distributional regression,Ensemble forecast,Proper scoring rule},
   pages = {125-151},
   publisher = {Annual Reviews Inc.},
   title = {Probabilistic forecasting},
   volume = {1},
   year = {2014},
}
@article{Hyndman2006,
   abstract = {We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition as well as the M3-competition, and many of the measures recommended by previous authors on this topic, are found to be degenerate in commonly occurring situations. Instead, we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series. © 2006 International Institute of Forecasters.},
   author = {Rob J. Hyndman and Anne B. Koehler},
   doi = {10.1016/j.ijforecast.2006.03.001},
   issn = {01692070},
   issue = {4},
   journal = {International Journal of Forecasting},
   keywords = {Forecast accuracy,Forecast error measures,Forecast evaluation,M-competition,Mean absolute scaled error},
   month = {10},
   pages = {679-688},
   title = {Another look at measures of forecast accuracy},
   volume = {22},
   year = {2006},
}
@article{Bergmeir2012,
   abstract = {In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand. In traditional forecasting, it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training. Thus it is not made full use of the data, but theoretical problems with respect to temporal evolutionary effects and dependencies within the data as well as practical problems regarding missing values are eliminated. On the other hand, when evaluating machine learning and other regression methods used for time series forecasting, often cross-validation is used for evaluation, paying little attention to the fact that those theoretical problems invalidate the fundamental assumptions of cross-validation. To close this gap and examine the consequences of different model selection procedures in practice, we have developed a rigorous and extensive empirical study. Six different model selection procedures, based on (i) cross-validation and (ii) evaluation using the series' last part, are used to assess the performance of four machine learning and other regression techniques on synthetic and real-world time series. No practical consequences of the theoretical flaws were found during our study, but the use of cross-validation techniques led to a more robust model selection. To make use of the "best of both worlds", we suggest that the use of a blocked form of cross-validation for time series evaluation became the standard procedure, thus using all available information and circumventing the theoretical problems. © 2012 Elsevier Inc. All rights reserved.},
   author = {Christoph Bergmeir and José M. Benítez},
   doi = {10.1016/j.ins.2011.12.028},
   issn = {00200255},
   journal = {Information Sciences},
   keywords = {Cross-validation,Error measures,Machine learning,Predictor evaluation,Regression,Time series},
   month = {5},
   pages = {192-213},
   title = {On the use of cross-validation for time series predictor evaluation},
   volume = {191},
   year = {2012},
}
@article{Looney2021,
   abstract = {Spectral differences affect solar cell performance, an effect that is especially visible when comparing different solar cell technologies. To reproduce the impact of varying spectra on solar cell performance in the lab, a unique classification of spectra is needed, which is currently missing in literature. The most commonly used classification, average photon energy (APE), is not unique, and a single APE value may represent various spectra depending on location. In this work, we propose a classification method based on an iterative use of the k-means clustering algorithm. We call this method RISE (Representative Identification of Spectra and the Environment). We define a set of 18 spectra using RISE and reproduce the spectral impact on energy yield for various solar cell technologies and locations. We explore effects on yield for commercially available solar cell technologies (Si and CdTe) in four locations: Singapore (fully humid equatorial climate), Colorado (cold arid), Brazil (warm, humid, and subtropical), and Denmark (fully humid warm temperature). We then reduce our findings to practice by implementing the spectrum set into an LED current–voltage (IV) tester. We verify our performance predictions using our set of representative spectra to reproduce energy yield differences between Si solar cells and CdTe solar cells with an average error of less than 1.5 ± 0.5% as compared to over 5% when using standard testing conditions.},
   author = {Erin E. Looney and Zhe Liu and Andrej Classen and Haohui Liu and Nicholas Riedel and Marília Braga and Pradeep Balaji and André Augusto and Tonio Buonassisi and Ian Marius Peters},
   doi = {10.1002/pip.3358},
   issn = {1099159X},
   issue = {2},
   journal = {Progress in Photovoltaics: Research and Applications},
   keywords = {energy yield,machine learning,photovoltaics,solar spectra classification},
   month = {2},
   pages = {200-211},
   publisher = {John Wiley and Sons Ltd},
   title = {Representative identification of spectra and environments (RISE) using k-means},
   volume = {29},
   year = {2021},
}
@article{Cerqueira2023,
   abstract = {Evaluating predictive models is a crucial task in predictive analytics. This process is especially challenging with time series data because observations are not independent. Several studies have analyzed how different performance estimation methods compare with each other for approximating the true loss incurred by a given forecasting model. However, these studies do not address how the estimators behave for model selection: the ability to select the best solution among a set of alternatives. This paper addresses this issue. The goal of this work is to compare a set of estimation methods for model selection in time series forecasting tasks. This objective is split into two main questions: (i) analyze how often a given estimation method selects the best possible model; and (ii) analyze what is the performance loss when the best model is not selected. Experiments were carried out using a case study that contains 3111 time series. The accuracy of the estimators for selecting the best solution is low, despite being significantly better than random selection. Moreover, the overall forecasting performance loss associated with the model selection process ranges from 0.28 to 0.58%. Yet, no considerable differences between different approaches were found. Besides, the sample size of the time series is an important factor in the relative performance of the estimators.},
   author = {Vitor Cerqueira and Luis Torgo and Carlos Soares},
   doi = {10.1007/s11063-023-11239-8},
   issn = {1573773X},
   issue = {7},
   journal = {Neural Processing Letters},
   keywords = {Cross-validation,Forecasting,Model selection,Performance estimation,Time series},
   month = {12},
   pages = {10073-10091},
   publisher = {Springer},
   title = {Model Selection for Time Series Forecasting An Empirical Analysis of Multiple Estimators},
   volume = {55},
   year = {2023},
}
@article{Papatheofanous2022,
   abstract = {Photovoltaic (PV) power production is characterized by high variability due to short-term meteorological effects such as cloud movements. These effects have a significant impact on the incident solar irradiance in PV parks. In order to control PV park performance, researchers have focused on Computer Vision and Deep Learning approaches to perform short-term irradiance forecasting using sky images. Motivated by the task of improving PV park control, the current work introduces the Image Regression Module, which produces irradiance values from sky images using image processing methods and Convolutional Neural Networks (CNNs). With the objective of enhancing the performance of CNN models on the task of irradiance estimation and forecasting, we propose an image processing method based on sun localization. Our findings show that the proposed method can consistently improve the accuracy of irradiance values produced by all the CNN models of our study, reducing the Root Mean Square Error by up to 10.44 W/m (Formula presented.) for the MobileNetV2 model. These findings indicate that future applications which utilize CNNs for irradiance forecasting should identify the position of the sun in the image in order to produce more accurate irradiance values. Moreover, the integration of the proposed models on an edge-oriented Field-Programmable Gate Array (FPGA) towards a smart PV park for the real-time control of PV production emphasizes their advantages.},
   author = {Elissaios Alexios Papatheofanous and Vasileios Kalekis and Georgios Venitourakis and Filippos Tziolos and Dionysios Reisis},
   doi = {10.3390/electronics11223794},
   issn = {20799292},
   issue = {22},
   journal = {Electronics (Switzerland)},
   keywords = {computer vision,convolutional neural networks,deep learning,edge computing,irradiance forecasting,photovoltaic},
   month = {11},
   publisher = {MDPI},
   title = {Deep Learning-Based Image Regression for Short-Term Solar Irradiance Forecasting on the Edge},
   volume = {11},
   year = {2022},
}
@article{Xu2023,
   abstract = {Inference for prediction errors is critical in time series forecasting pipelines. However, providing statistically meaningful uncertainty intervals for prediction errors remains relatively under-explored. Practitioners often resort to forward cross-validation (FCV) for obtaining point estimators and constructing confidence intervals based on the Central Limit Theorem (CLT). The naive version assumes independence, a condition that is usually invalid due to time correlation. These approaches lack statistical interpretations and theoretical justifications even under stationarity. This paper systematically investigates uncertainty intervals for prediction errors in time series forecasting. We first distinguish two key inferential targets: the stochastic test error over near future data points, and the expected test error as the expectation of the former. The stochastic test error is often more relevant in applications needing to quantify uncertainty over individual time series instances. To construct prediction intervals for the stochastic test error, we propose the quantile-based forward cross-validation (QFCV) method. Under an ergodicity assumption, QFCV intervals have asymptotically valid coverage and are shorter than marginal empirical quantiles. In addition, we also illustrate why naive CLT-based FCV intervals fail to provide valid uncertainty intervals, even with certain corrections. For non-stationary time series, we further provide rolling intervals by combining QFCV with adaptive conformal prediction to give time-average coverage guarantees. Overall, we advocate the use of QFCV procedures and demonstrate their coverage and efficiency through simulations and real data examples.},
   author = {Hui Xu and Song Mei and Stephen Bates and Jonathan Taylor and Robert Tibshirani},
   month = {9},
   title = {Uncertainty Intervals for Prediction Errors in Time Series Forecasting},
   url = {http://arxiv.org/abs/2309.07435},
   year = {2023},
}
@inproceedings{Abdallah2022,
   abstract = {In this work, we develop techniques for fast automatic selection of the best forecasting model for a new unseen time-series dataset, without having to first train (or evaluate) all the models on the new time-series data to select the best one. In particular, we develop a forecasting meta-learning approach called AutoForecast that allows for the quick inference of the best time-series forecasting model for an unseen dataset. Our approach learns both forecasting models performances over time horizon of same dataset and task similarity across different datasets. The experiments demonstrate the effectiveness of the approach over state-of-the-art (SOTA) single and ensemble methods and several SOTA meta-learners (adapted to our problem) in terms of selecting better forecasting models (i.e., 2X gain) for unseen tasks for univariate and multivariate testbeds.},
   author = {Mustafa Abdallah and Ryan Rossi and Kanak Mahadik and Sungchul Kim and Handong Zhao and Saurabh Bagchi},
   doi = {10.1145/3511808.3557241},
   isbn = {9781450392365},
   journal = {International Conference on Information and Knowledge Management, Proceedings},
   keywords = {automl,meta-learning,model selection,time-series forecasting},
   month = {10},
   pages = {5-14},
   publisher = {Association for Computing Machinery},
   title = {AutoForecast: Automatic Time-Series Forecasting Model Selection},
   year = {2022},
}
@article{Cerqueira2021,
   abstract = {Time series forecasting is a challenging task with applications in a wide range of domains. Auto-regression is one of the most common approaches to address these problems. Accordingly, observations are modelled by multiple regression using their past lags as predictor variables. We investigate the extension of auto-regressive processes using statistics which summarise the recent past dynamics of time series. The result of our research is a novel framework called VEST, designed to perform feature engineering using univariate and numeric time series automatically. The proposed approach works in three main steps. First, recent observations are mapped onto different representations. Second, each representation is summarised by statistical functions. Finally, a filter is applied for feature selection. We discovered that combining the features generated by VEST with auto-regression significantly improves forecasting performance in a database composed by 90 time series with high sampling frequency. However, we also found that there are no improvements when the framework is applied for multi-step forecasting or in time series with low sample size. VEST is publicly available online.},
   author = {Vitor Cerqueira and Nuno Moniz and Carlos Soares},
   doi = {10.1007/s10994-021-05959-y},
   issn = {15730565},
   journal = {Machine Learning},
   keywords = {Automatic machine learning,Feature engineering,Time series forecasting},
   publisher = {Springer},
   title = {VEST: automatic feature engineering for forecasting},
   year = {2021},
}
@article{Cerqueira2020,
   abstract = {Performance estimation aims at estimating the loss that a predictive model will incur on unseen data. This process is a fundamental stage in any machine learning project. In this paper we study the application of these methods to time series forecasting tasks. For independent and identically distributed data the most common approach is cross-validation. However, the dependency among observations in time series raises some caveats about the most appropriate way to estimate performance in this type of data. Currently, there is no consensual approach. We contribute to the literature by presenting an extensive empirical study which compares different performance estimation methods for time series forecasting tasks. These methods include variants of cross-validation, out-of-sample (holdout), and prequential approaches. Two case studies are analysed: One with 174 real-world time series and another with three synthetic time series. Results show noticeable differences in the performance estimation methods in the two scenarios. In particular, empirical experiments suggest that blocked cross-validation can be applied to stationary time series. However, when the time series are non-stationary, the most accurate estimates are produced by out-of-sample methods, particularly the holdout approach repeated in multiple testing periods.},
   author = {Vitor Cerqueira and Luis Torgo and Igor Mozetič},
   doi = {10.1007/s10994-020-05910-7},
   issn = {15730565},
   issue = {11},
   journal = {Machine Learning},
   keywords = {Cross validation,Forecasting,Model selection,Performance estimation,Time series},
   month = {11},
   pages = {1997-2028},
   publisher = {Springer},
   title = {Evaluating time series forecasting models: an empirical study on performance estimation methods},
   volume = {109},
   year = {2020},
}
@article{,
   abstract = {One of the most promising renewable energy sources used as a solution to supply the increase in electricity consumption is photovoltaic solar energy. This source has intrinsic and uncontrollable, peculiarities that cause intermittencies in its generation, due to climatic factors. Therefore, it becomes relevant to the existence of solutions for the prediction of solar photovoltaic energy generation, enabling increased security in the generation and distribution of electricity. In this context, this research proposed a new hybrid prediction method applicable to short-term solar irradiance. The proposed method employs a set of image processing metrics, to extract all-sky image features, used as input in machine learning-based prediction models. The set of all-sky image metrics, extracted with image processing, represent complementary characteristics of sky that provided an overall average accuracy ≅ 30 % higher compared to using traditional meteorological information. The experimental results of the hybrid prediction method with Artificial Neural Network and Light Gradient Boosting Machine, considering six short-term prediction horizons, showed an overall average prediction accuracy of 17.5 % better than the Persistence model. The proposed approach is more interpretable than several literature studies, demonstrated competitive results with more robust deep learning models and, represents a new path for future studies in the prediction of solar photovoltaic energy generation.},
   author = {Joylan Nunes Maciel and Jorge Javier Gimenez Ledesma and Oswaldo Hideo Ando Junior},
   doi = {10.1016/j.rser.2023.114185},
   issn = {13640321},
   journal = {Renewable and Sustainable Energy Reviews},
   month = {3},
   pages = {114185},
   publisher = {Elsevier BV},
   title = {Hybrid prediction method of solar irradiance applied to short-term photovoltaic energy generation},
   volume = {192},
   year = {2024},
}
@article{,
   abstract = {— In this study, an image processing-based deep learning approach for short-term forecast of solar radiation has been developed. For this purpose, firstly, cloud movements occurred during the day are tracked and future cloud movements are forecasted, accordingly. Subsequently, using the cloud motion estimation and extraterrestrial solar radiation data, 1-min averaged solar radiation values are estimated for 5-min time horizon. Shi-Tomasi method is employed to determine the feature points to be tracked on the sky images whereas, Lucas-Kanade optical flow method is employed to track the determined feature points on the sequential images. Average cloud velocity and directions are calculated by the help of linear regression method from tracked cloud movements. A hybrid approach including K-means and red/blue ratio is built to classify the pixels of the image whether they are clouds or sky. Finally, short-term solar radiations are estimated using the Long-Short Term Memory (LSTM) deep learning method. The performance of the proposed approach is compared with other methods in the literature. As a result it is concluded that, developed approach outperforms most methods in the literature with RMSE values of 47.576, 53.830, 68.103, and 92.386 for four different days and can be used as an alternative approach.},
   author = {Ardan Hüseyin Eşlik and Emre Akarslan and Fatih Onur Hocaoğlu},
   doi = {10.1016/j.renene.2022.10.063},
   issn = {18790682},
   journal = {Renewable Energy},
   keywords = {Cloud and sun detection,Cloud motion forecasting,Cloud motion tracking,Image processing,Long short-term memory,Short-term solar radiation forecasting},
   month = {11},
   pages = {1490-1505},
   publisher = {Elsevier Ltd},
   title = {Short-term solar radiation forecasting with a novel image processing-based deep learning approach},
   volume = {200},
   year = {2022},
}
@article{Pedro2019,
   abstract = {We describe and release a comprehensive solar irradiance, imaging, and forecasting dataset. Our goal with this release is to provide standardized solar and meteorological datasets to the research community for the accelerated development and benchmarking of forecasting methods. The data consist of three years (2014-2016) of quality-controlled, 1-min resolution global horizontal irradiance and direct normal irradiance ground measurements in California. In addition, we provide overlapping data from commonly used exogenous variables, including sky images, satellite imagery, and Numerical Weather Prediction forecasts. We also include sample codes of baseline models for benchmarking of more elaborated models.},
   author = {Hugo T.C. Pedro and David P. Larson and Carlos F.M. Coimbra},
   doi = {10.1063/1.5094494},
   issn = {19417012},
   issue = {3},
   journal = {Journal of Renewable and Sustainable Energy},
   month = {5},
   publisher = {American Institute of Physics Inc.},
   title = {A comprehensive dataset for the accelerated development and benchmarking of solar forecasting methods},
   volume = {11},
   year = {2019},
}
@article{Barancsuk2024,
   abstract = {In recent years, with the growing proliferation of photovoltaics (PV), accurate nowcasting of PV power has emerged as a challenge. Global horizontal irradiance (GHI), which is a key factor influencing PV power, is known to be highly variable as it is determined by short-term meteorological phenomena, particularly cloud movement. Deep learning and computer vision techniques applied to all-sky imagery are demonstrated to be highly accurate nowcasting methods, as they encode crucial information about the sky’s state. While these methods utilize deep neural network models, such as Convolutional Neural Networks (CNN), and attain high levels of accuracy, the training of image-based deep learning models demands significant computational resources. In this work, we present a computationally economical estimation technique, based on a deep learning model. We utilize both all-sky imagery and meteorological data, however, information on the sky’s state is encoded as a feature vector extracted using traditional image processing methods. We introduce six all-sky image features utilizing detailed knowledge of meteorological and physical phenomena, significantly decreasing the amount of input data and model complexity. We investigate the accuracy of the determined global and diffuse radiation for different combinations of meteorological parameters. The model is evaluated using two years of measurements from an on-site all-sky camera and an adjacent meteorological station. Our findings demonstrate that the model provides comparable accuracy to CNN-based methods, yet at a significantly lower computational cost.},
   author = {Lilla Barancsuk and Veronika Groma and Dalma Günter and János Osán and Bálint Hartmann},
   doi = {10.3390/en17020438},
   issn = {19961073},
   issue = {2},
   journal = {Energies},
   keywords = {deep learning,image processing,resource efficiency,solar irradiance estimation},
   month = {1},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Estimation of Solar Irradiance Using a Neural Network Based on the Combination of Sky Camera Images and Meteorological Data},
   volume = {17},
   year = {2024},
}
@article{Lou2020,
   abstract = {Sky conditions, and the corresponding luminance and radiance distribution patterns are essential to daylighting, energy and thermal environmental studies. The CIE Standard Skies define the overcast, partly cloudy, and clear sky conditions intuitively by rigorous luminance distributions, which are, however, usually determined by sophisticated and uncommon measurements (e.g. radiation in multiple vertical directions). This study proposes a simple approach to identifying the hourly sky conditions from the global horizontal irradiance (EHG) that is most widely accessible in weather stations with basic radiation measurement facilities. The sophisticated measurements are, therefore, avoided for simple applications. The partly cloudy sky, especially, is identified by the notable disparity of its EHG measurement from the theoretical cloudless condition. The proposed approach interprets the ISO/CIE Standard Sky conditions and their diffuse luminance and radiance distributions successfully. From the datasets at two sites with different climates, the approach correctly identifies 7% more sky conditions than a previous work when the direct beam and diffuse radiation measurements were not accessible. The model can thus be essential to solar energy studies for places with ground-based solar radiation measurements only.},
   author = {Siwei Lou and Yu Huang and Danny H.W. Li and Dawei Xia and Xiaoqing Zhou and Yang Zhao},
   doi = {10.1016/j.renene.2020.06.114},
   issn = {18790682},
   journal = {Renewable Energy},
   keywords = {Anisotropic sky,Luminance/radiance distribution,Sky conditions,Sky identification},
   month = {12},
   pages = {77-90},
   publisher = {Elsevier Ltd},
   title = {A novel method for fast sky conditions identification from global solar radiation measurements},
   volume = {161},
   year = {2020},
}
@article{David2023,
   abstract = {With the fast increase of solar energy plants, a high-quality short-term forecast is required to smoothly integrate their production in the electricity grids. Usually, forecasting systems predict the future solar energy as a continuous variable. But for particular applications, such as concentrated solar plants with tracking devices, the operator needs to anticipate the achievement of a solar irradiance threshold to start or stop their system. In this case, binary forecasts are more relevant. Moreover, while most forecasting systems are deterministic, the probabilistic approach provides additional information about their inherent uncertainty that is essential for decision-making. The objective of this work is to propose a methodology to generate probabilistic solar forecasts as a binary event for very short-term horizons between 1 and 30 min. Among the various techniques developed to predict the solar potential for the next few minutes, sky imagery is one of the most promising. Therefore, we propose in this work to combine a state-of-the-art model based on a sky camera and a discrete choice model to predict the probability of an irradiance threshold suitable for plant operators. Two well-known parametric discrete choice models, logit and probit models, and a machine learning technique, random forest, were tested to post-process the deterministic forecast derived from sky images. All three models significantly improve the quality of the original deterministic forecast. However, random forest gives the best results and especially provides reliable probability predictions.},
   author = {Mathieu David and Joaquín Alonso-Montesinos and Josselin Le Gal La Salle and Philippe Lauret},
   doi = {10.3390/en16207125},
   issn = {19961073},
   issue = {20},
   journal = {Energies},
   keywords = {Brier Score,all sky imager (ASI),binary probabilistic forecasts,concentrated solar plant (CSP),photovoltaic (PV),solar energy},
   month = {10},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Probabilistic Solar Forecasts as a Binary Event Using a Sky Camera},
   volume = {16},
   year = {2023},
}
@article{Ogliari2024,
   abstract = {Electrical power production by renewable energy sources is unpredictable in nature and this may cause imbalance between power generation and demand. Therefore, an accurate prediction of solar radiation is crucial for the stability and efficient management of electric grid. This study focuses on very short-term forecasts of solar radiation with a horizon in the range of 5–15 min. In this paper, a Convolutional Neural Network is proposed that uses sequences of infrared images captured by an All-Sky Imager to forecast the Global Horizontal Irradiance on different time horizon. A real case study, exploiting six months of high-resolution data, is analyzed. Additionally, an innovative technique, the Enhanced Convolutional Neural Network (ECNN), is proposed in which exogenous data, as the solar radiation measurement, is encoded in terms of colored pixels in the upper corner of the images. Considering the naïve persistence method as a baseline, a clear improvement across the key metrics has been noted with the proposed methodology. A deeper analysis of the results reveals that the proposed models are more accurate than persistence when high fluctuations of solar radiation are experienced. In that case, the ECNN achieves a forecast skill exceeding 19% for all the tested forecast horizons.},
   author = {Emanuele Ogliari and Maciej Sakwa and Paolo Cusa},
   doi = {10.1016/j.renene.2023.119735},
   issn = {18790682},
   journal = {Renewable Energy},
   keywords = {All-Sky Images,Convolutional Neural Networks,Nowcasting,Solar radiation forecast,Whole-Sky camera},
   month = {2},
   publisher = {Elsevier Ltd},
   title = {Enhanced Convolutional Neural Network for solar radiation nowcasting: All-Sky camera infrared images embedded with exogeneous parameters},
   volume = {221},
   year = {2024},
}
@article{Jensen2023,
   abstract = {Access to accurate solar resource data is critical for numerous applications, including estimating the yield of solar energy systems, developing radiation models, and validating irradiance datasets. However, lack of standardization in data formats and access interfaces across providers constitutes a major barrier to entry for new users. pvlib python's iotools subpackage aims to solve this issue by providing standardized Python functions for reading local files and retrieving data from external providers. All functions follow a uniform pattern and return convenient data outputs, allowing users to seamlessly switch between data providers and explore alternative datasets. The pvlib package is community-developed on GitHub: https://github.com/pvlib/pvlib-python. As of pvlib python version 0.9.5, the iotools subpackage supports 12 different datasets, including ground measurement, reanalysis, and satellite-derived irradiance data. The supported ground measurement networks include the Baseline Surface Radiation Network (BSRN), NREL MIDC, SRML, SOLRAD, SURFRAD, and the US Climate Reference Network (CRN). Additionally, satellite-derived and reanalysis irradiance data from the following sources are supported: PVGIS (SARAH & ERA5), NSRDB PSM3, and CAMS Radiation Service (including McClear clear-sky irradiance).},
   author = {Adam R. Jensen and Kevin S. Anderson and William F. Holmgren and Mark A. Mikofski and Clifford W. Hansen and Leland J. Boeman and Roel Loonen},
   doi = {10.1016/j.solener.2023.112092},
   issn = {0038092X},
   journal = {Solar Energy},
   keywords = {Data article,Free and open-source software (FOSS),Public data,Python,Solar energy},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {pvlib iotools—Open-source Python functions for seamless access to solar irradiance data},
   volume = {266},
   year = {2023},
}
@article{Paletta2021,
   abstract = {A number of industrial applications, such as smart grids, power plant operation, hybrid system management or energy trading, could benefit from improved short-term solar forecasting, addressing the intermittent energy production from solar panels. However, current approaches to modelling the cloud cover dynamics from sky images still lack precision regarding the spatial configuration of clouds, their temporal dynamics and physical interactions with solar radiation. Benefiting from a growing number of large datasets, data driven methods are being developed to address these limitations with promising results. In this study, we compare four commonly used deep learning architectures trained to forecast solar irradiance from sequences of hemispherical sky images and exogenous variables. To assess the relative performance of each model, we used the forecast skill metric based on the smart persistence model, as well as ramp and time distortion metrics. The results show that encoding spatiotemporal aspects of the sequence of sky images greatly improved the predictions with 10 min ahead forecast skill reaching 20.4% on the test year. However, based on the experimental data, we conclude that, with a common setup, deep learning models tend to behave just as a ‘very smart persistence model’, temporally aligned with the persistence model while mitigating its most penalising errors. Thus, despite being captured by the sky cameras, models often miss fundamental events causing large irradiance changes such as clouds obscuring the sun. We hope that our work will contribute to a shift of this approach to irradiance forecasting, from reactive to anticipatory.},
   author = {Quentin Paletta and Guillaume Arbod and Joan Lasenby},
   doi = {10.1016/j.solener.2021.05.056},
   issn = {0038092X},
   journal = {Solar Energy},
   keywords = {Computer Vision,Convolutional Neural Networks,Deep Learning,Forecasting,Sky Images,Solar irradiance},
   month = {8},
   pages = {855-867},
   publisher = {Elsevier Ltd},
   title = {Benchmarking of deep learning irradiance forecasting models from sky images – An in-depth analysis},
   volume = {224},
   year = {2021},
}
@article{Navarro2023,
   abstract = {Construction-related enterprises are acknowledged as one of the key actors responsible for shifting society toward the sustainable future claimed by the recently established Sustainable Development Goals. However, university curricula need to emphasize guaranteeing the acquisition of transversal competencies that are essential for the future management professionals required by this new challenge Consistent and critical thinking is considered a fundamental skill for education in sustainability. To date, no studies have presented an objective measure of the level of acquisition of such transverse skills in university curricula. This study provides an analytical tool to that end, based on the multi-criteria decision-making technique Analytic Hierarchy Process (AHP). Through sustainability-oriented case studies, students are faced with real managerial decision-making problems. The proposed method allows for the analytic quantification of the consistency of their responses. Such consistency is representative of their critical thinking skills. The proposed tool allows teachers not only to find the consistency of their students' responses but also to understand in which areas of sustainability students lack a clear vision of the problem. This tool is therefore useful for teachers to effectively adapt their syllabi according to their students' knowledge.},
   author = {Ignacio J. Navarro and Jose V. Marti and Victor Yepes},
   doi = {10.13039/501100011033},
   issn = {0949149X},
   issue = {3},
   journal = {International Journal of Engineering Education},
   keywords = {consistency,critical thinking,management,sustainable education: transversal competence},
   pages = {592-603},
   publisher = {Tempus Publications},
   title = {Evaluation of Higher Education Students' Critical Thinking Skills on Sustainability},
   volume = {39},
   year = {2023},
}
@article{,
   abstract = {The alternation between cloudy and clear skies alters the photovoltaic production. This makes it necessary to anticipate these disturbances hours in advance for the correct operation of the electricity distribution plants and networks. In this paper, two short-term forecasting models (3 h) are developed to forecast the photovoltaic production in an integrated plant in the CIESOL building of the University of Almería. The methodology used is based on sky camera images and Artificial Intelligence techniques. Two models have been developed and compared applying artificial neural network (ANN) and support vector machine (SVM) techniques. The global irradiance predicted using sky camera images is used as an input variable in both models. In addition, the operational status of the plants has been included as an input parameter through the performance ratio. The results have shown that the errors made by ANN and SVM are very similar. For all sky conditions, the uncertainty of the production forecast differs by less than 2% from the uncertainty of the solar resource, which is the main source of error in the production models developed.},
   author = {Mauricio Trigo-González and Marcelo Cortés-Carmona and Aitor Marzo and Joaquín Alonso-Montesinos and Mercedes Martínez-Durbán and Gabriel López and Carlos Portillo and Francisco Javier Batlles},
   doi = {10.1016/j.renene.2023.01.111},
   issn = {18790682},
   journal = {Renewable Energy},
   keywords = {Machine learning,Nowcasting,Photovoltaic plant,Sky cameras,Solar resource assessment},
   month = {4},
   pages = {251-262},
   publisher = {Elsevier Ltd},
   title = {Photovoltaic power electricity generation nowcasting combining sky camera images and learning supervised algorithms in the Southern Spain},
   volume = {206},
   year = {2023},
}
@article{Radovan2021,
   abstract = {Solar energy production based on a photovoltaic system is closely related to solar irradi-ance. Therefore, the planning of production is based on the prediction of solar irradiance. The optimal use of different energy storage systems requires an accurate prediction of solar irradiation with at least an hourly time horizon. In this work, a solar irradiance prediction method is developed based on the prediction of solar shading by clouds. The method is based on determining the current cloud position and estimating the velocity from a sequence of multiple images taken with a 180-degree wide-angle camera with a resolution of 5 s. The cloud positions for the next hour interval are calculated from the estimated current cloud position and velocity. Based on the cloud position, the percentage of solar overshadowing by clouds is determined, i.e., the solar overshadowing curve for the next hour interval is calculated. The solar irradiance is determined by normalizing the percentage of the solar unshadowing curve to the mean value of the irradiance predicted by the hydrome-teorological institute for that hourly interval. Image processing for cloud detection and localization is performed using a computer vision library and the Java programming language. The algorithm developed in this work leads to improved accuracy and resolution of irradiance prediction for the next hour interval. The predicted irradiance curve can be used as a predicted reference for solar energy production in energy storage system optimization.},
   author = {Aleksander Radovan and Viktor Šunde and Danijel Kučak and Željko Ban},
   doi = {10.3390/en14133775},
   issn = {19961073},
   issue = {13},
   journal = {Energies},
   keywords = {Cloud movement,Sky image processing,Solar irradiance prediction},
   month = {7},
   publisher = {MDPI AG},
   title = {Solar irradiance forecast based on cloud movement prediction},
   volume = {14},
   year = {2021},
}
@article{,
   abstract = {The energy available in Micro Grid (MG) that is powered by solar energy is tightly related to the weather conditions in the moment of generation. Very short-term forecast of solar irradiance provides the MG with the capability of automatically controlling the dispatch of energy. To achieve this, we propose a method for statistical quantification of cloud features extracted from long-ware infrared (IR) images to forecast the Clear Sky Index (CSI). The images are obtained using a data acquisition system (DAQ) mounted on a solar tracker. We explain how to remove cyclostationary bias in the data caused by the devices in the own DAQ. We investigate a method to obtain the CSI, after the detrending of Global Horizontal Irradiance (GHI) measurements. We propose a method to fusion multiple exposures of circumsolar visible (VI) light images. We implement a method for extracting physical features using radiometric measurements of the IR camera. We introduce a model to remove from IR images both the effect of the atmosphere scatter radiation, and the effect of the Sun direct radiation. We explain how to model of diffuse radiation of the IR camera window, which is produce by water spots and dust particles stack to the germanium lens of the DAQ enclosure. The frames, that were used to model the camera window, are selected using an atmospheric condition model. This model classifies the sky four different categories: clear, cumulus, stratus, and nimbus. We introduce a geometric transformation of the size of the pixels to their actual dimension in a plane of the atmosphere which is at a given height. This transformation is performed according to the elevation angle of the Sun and field of view (FOV) of the camera. We compare the error between the transformation and anapproximation of transformation.},
   author = {Guillermo Terrén-Serrano and Manel Martínez-Ramón},
   month = {11},
   title = {Data acquisition and image processing for solar irradiance forecasting},
   url = {http://arxiv.org/abs/2011.12401},
   year = {2020},
}
@article{Dissawa2021,
   abstract = {Power generation through solar photovoltaics has shown significant growth in recent years. However, high penetration of solar PV creates power system operational issues as a result of solar PV variability and uncertainty. Short-term PV variability mainly occurs due to the intermittency of cloud cover. Therefore, to mitigate the effects of PV variability, a sky-image-based, localized, global horizontal irradiance forecasting model was introduced considering the individual cloud motion, cloud thicknesses, and the elevations of clouds above the ground level. The proposed forecasting model works independently of any historical irradiance measurements. Two inexpensive sky camera systems were developed and placed in two different locations to obtain sky images for cloud tracking and cloud-based heights. Then, irradiance values for onsite and for a PV site located with a distance of 2 km from the main camera were forecasted for 1 minute, 5 minutes, and 15 minutes ahead of real-time. Results show that the three-level cloud categorization and the individual cloud movement tracking method introduced in this paper increase the forecasting accuracy. For partially cloudy and sunny days, the forecasting model for 15 min forecasting time interval achieved a positive skill factor concerning the persistent model. The accuracy of determining the correct irradiance state for a 1 min forecasting time interval using the proposed model is 81%. The average measures of RMSE, MAE, and SF obtained using the proposed method for 15 min forecasting time horizon are 101 Wm-2, 64 Wm-2, and 0.26, respectively. These forecasting accuracy levels are much higher than the other benchmarks considered in this paper.},
   author = {Lasanthika H. Dissawa and Roshan I. Godaliyadda and Parakrama B. Ekanayake and Ashish P. Agalgaonkar and Duane Robinson and Janaka B. Ekanayake and Sarath Perera},
   doi = {10.1155/2021/9973010},
   issn = {1687529X},
   journal = {International Journal of Photoenergy},
   publisher = {Hindawi Limited},
   title = {Sky Image-Based Localized, Short-Term Solar Irradiance Forecasting for Multiple PV Sites via Cloud Motion Tracking},
   volume = {2021},
   year = {2021},
}
@misc{Samu2021,
   abstract = {The integration of solar photovoltaic (PV) into electricity networks introduces technical challenges due to varying PV output. Rapid ramp events due to cloud movements are of particular concern for the operation of remote islanded microgrids (MGs) with high penetration of solar PV generation. PV plants and optionally controllable distributed energy resources (DERs) in MGs can be operated in an optimized way based on nowcasting, which is also called very short-term solar irradiance forecasting up to 60 min ahead. This study presents an extensive literature review on nowcasting technologies along with their current and future possible applications in the control of MGs. Ramp rates control and scheduling of spinning reserves are found to be the most recognized applications of nowcasting in MGs. An online survey has been conducted to identify the limitations, benefits and challenges of deploying nowcasting in MGs. The survey outcomes show that the incorporation of nowcasting tools in MG operations is still limited, though the possibility of increasing solar PV penetration levels in MGs if nowcasting tools are incorporated is acknowledged. Additionally, recent nowcasting tools, such as sky camera-based tools, require further validation under various conditions for more widespread adaptation by power system operators.},
   author = {Remember Samu and Martina Calais and G. M. Shafiullah and Moayed Moghbel and Md Asaduzzaman Shoeb and Bijan Nouri and Niklas Blum},
   doi = {10.1016/j.rser.2021.111187},
   issn = {18790690},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {Control strategies,Distributed energy resources,Microgrids,Sky camera,Solar PV,Solar forecasting,Solar nowcasting},
   month = {9},
   publisher = {Elsevier Ltd},
   title = {Applications for solar irradiance nowcasting in the control of microgrids: A review},
   volume = {147},
   year = {2021},
}
@misc{Ajith2023,
   abstract = {Integrating solar energy with existing grid systems is difficult due to its variability, which is impacted by factors such as the predicted horizon, meteorological conditions, and geographic position. Accurate global horizontal irradiance (GHI) estimates can help to address this issue and allow for early and effective participation in the energy planning and management market. The existing models either use time series data or sky images in various network topologies to perform solar radiance forecasts. This study compares three categories of solar irradiance forecasting models such as time series-based, image-based and hybrid models. Here several state-of-the-art methods are compared against the proposed models, namely Convolutional Long Short-Term Memory Fusion Network (CNN-L) and Multiple Image Convolutional Long Short-Term Memory Fusion Network (MICNN-L). Both models use both infrared sky images as well as past values of GHI for prediction. These methods extract spatial features using convolutional neural networks and temporal features using long short-term memory networks. The extracted features are finally concatenated and passed through a fully connected layer to obtain a prediction. Further analysis also included using a feature extraction method such as optical flow (OF) on the image data before passing it to the hybrid model MICNN-L (OF). The results observed in this comparative analysis denote that MICNN-L improves the efficacy of the forecasts in cloudy conditions compared to the rest of the state-of-the-art approaches.},
   author = {Meenu Ajith and Manel Martínez-Ramón},
   doi = {10.1016/j.rser.2023.113362},
   issn = {18790690},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {Convolutional neural networks,Hybrid models,Infra-red images,Long short term memory,Multimodal deep learning,Optical flow,Solar irradiance forecasting},
   month = {8},
   publisher = {Elsevier Ltd},
   title = {Deep learning algorithms for very short term solar irradiance forecasting: A survey},
   volume = {182},
   year = {2023},
}
@article{Wang2020,
   abstract = {Accurate minutely solar irradiance forecasting is the basis of minute-level photovoltaic (PV) power forecasting. In this paper, a minutely solar irradiance forecasting method based on real-time surface irradiance mapping model is proposed, which is beneficial to achieve higher accuracy in solar power forecasting. First, we extract the red–green–blue (RGB) values and position information of pixels in sky images after background elimination and distortion rectification, to explore the mapping relationship between sky image and solar irradiance. Then a real-time sky image-irradiance mapping model is built, trained, and updated according to real-time sky images and solar irradiance. Finally, the future solar irradiance within the time horizons varying from 1 min to 10 min ahead are capable to be forecasted by using the latest updated surface irradiance mapping model with extracted input from the current sky image. The average measures of proposed method by using MAPE, RMSE, MBE are 22.66%, 92.72, −1.26% for blocky clouds; 20.44%, 132.15, −1.06% for thin clouds and 18.82%, 120.78, −0.98% for thick clouds, thus deliver much higher forecasting accuracy than other benchmarks.},
   author = {Fei Wang and Zhiming Xuan and Zhao Zhen and Yu Li and Kangping Li and Liqiang Zhao and Miadreza Shafie-khah and João P.S. Catalão},
   doi = {10.1016/j.enconman.2020.113075},
   issn = {01968904},
   journal = {Energy Conversion and Management},
   keywords = {Minutely,Real-time model,Sky image,Solar irradiance forecasting,Surface irradiance mapping},
   month = {9},
   publisher = {Elsevier Ltd},
   title = {A minutely solar irradiance forecasting method based on real-time sky image-irradiance mapping model},
   volume = {220},
   year = {2020},
}
@article{Venitourakis2023,
   abstract = {Aiming at effectively improving photovoltaic (PV) park operation and the stability of the electricity grid, the current paper addresses the design and development of a novel system achieving the short-term irradiance forecasting for the PV park area, which is the key factor for controlling the variations in the PV power production. First, it introduces the Xception long short-term memory (XceptionLSTM) cell tailored for recurrent neural networks (RNN). Second, it presents the novel irradiance forecasting model that consists of a sequence-to-sequence image regression NNs in the form of a spatio-temporal encoder–decoder including Xception layers in the spatial encoder, the novel XceptionLSTM in the temporal encoder and decoder and a multilayer perceptron in the spatial decoder. The proposed model achieves a forecast skill of 16.57% for a horizon of 5 min when compared to the persistence model. Moreover, the proposed model is designed for execution on edge computing devices and the real-time application of the inference on the Raspberry Pi 4 Model B 8 GB and the Raspberry Pi Zero 2W validates the results.},
   author = {Georgios Venitourakis and Christoforos Vasilakis and Alexandros Tsagkaropoulos and Tzouma Amrou and Georgios Konstantoulakis and Panagiotis Golemis and Dionysios Reisis},
   doi = {10.3390/info14110617},
   issn = {20782489},
   issue = {11},
   journal = {Information (Switzerland)},
   keywords = {ConvLSTM,deep learning,edge computing,ground-based sky images,irradiance forecasting,photovoltaic parks},
   month = {11},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Neural Network-Based Solar Irradiance Forecast for Edge Computing Devices},
   volume = {14},
   year = {2023},
}
@article{Marinho2023,
   abstract = {In this paper, solar irradiance short-term forecasts were performed considering time horizons ranging from 5 min to 30 min, under a 5 min time-step. Global horizontal irradiance (GHI) and direct normal irradiance (DNI) were computed using deep neural networks with 1-dimensional convolutional neural network (CNN-1D), long short-term memory (LSTM), and CNN-LSTM layers on the benchmarking dataset FOLSOM, which is formed by predictors obtained by recursive functions on the clear sky index time series and statistical attributes extracted from images collected by a camera pointed to the zenith, characterizing endogenous and exogenous variables, respectively. To analyze the endogenous predictors influence on the accuracy of the networks, the performance was evaluated for the cases with and without them. This analysis is motivated, to our best knowledge, by the lack of works that cite the FOLSOM dataset using deep learning models, and it is necessary to verify the impact of the endogenous and exogenous predictors in the forecasts results for this specific approach. The accuracy of the networks was evaluated by the metrics mean absolute error (MAE), mean bias error (MBE), root-mean-squared error (RMSE), relative root mean squared error (rRMSE), determination coefficient (R2), and forecast skill (s). The network architectures using isolated CNN-1D and LSTM layers generally performed better. The best accuracy was obtained by the CNN-1D network for a horizon of 10 min ahead reaching an RMSE of 36.24 W/m2, improving 11.15% on this error metric compared to the persistence model.},
   author = {Felipe P. Marinho and Paulo A.C. Rocha and Ajalmar R.R. Neto and Francisco D.V. Bezerra},
   doi = {10.1115/1.4056122},
   issn = {15288986},
   issue = {4},
   journal = {Journal of Solar Energy Engineering, Transactions of the ASME},
   keywords = {deep neural networks,renewable energy,solar energy,solar irradiation forecast},
   month = {8},
   publisher = {American Society of Mechanical Engineers (ASME)},
   title = {Short-Term Solar Irradiance Forecasting Using CNN-1D, LSTM, and CNN-LSTM Deep Neural Networks: A Case Study with the Folsom (USA) Dataset},
   volume = {145},
   year = {2023},
}
@misc{Liu2024,
   author = {Li Liu and Timothy Hospedales and Yann Lecun and Mingsheng Long and Jiebo Luo and Wanli Ouyang and Matti Pietikainen and Tinne Tuytelaars},
   doi = {10.1109/TPAMI.2023.3341723},
   issn = {19393539},
   issue = {3},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   month = {3},
   pages = {1319-1326},
   publisher = {IEEE Computer Society},
   title = {Editorial: Learning With Fewer Labels in Computer Vision},
   volume = {46},
   year = {2024},
}
@article{Dittmann2021,
   abstract = {High resolution irradiance forecasts based on sky imagers are valuable for applications that require short term decisions based on ramps of solar irradiance. Here, we present our sky imager based forecasting algorithm, using images of a low cost surveillance camera. Model development and evaluation is done separately for the different steps in sky imager forecasting, starting with cloud detection, followed by estimation and extrapolation of cloud movement, and fi ally deriving irradiance forecasts from the predicted cloud images. We distinguish between clear and cloudy conditions and especially evaluate the effect of cirrus situations on the different forecasting steps. To create binary cloud masks, we adapted a pixel value based cloud algorithm using a set of manually classif ed pixels. In an independent validation dataset 90.3 % of the pixels are classif ed correctly. For the circumsolar region, where cloud decision is known to be especially challenging and crucial for the forecasting of the direct component of the solar radiation, we introduce a correction procedure using real-time irradiance measurements and object recognition methods. Applying this method we can significa tly improve the cloud detection in the circumsolar region and increase the forecast skill of the cloud decision forecast. The development of the irradiance algorithm is a special focus of this paper. Real-time irradiance measurements and cloud decision information are used as input to our irradiance model. The algorithm is developed using cloud decision information derived from measurements instead of sky imager cloud decision forecasts to exclude the inf uence of errors in cloud decision and cloud motion methods for model development. Afterwards, the irradiance algorithm is applied to sky imager based cloud decision forecasts. Even though we start with binary cloud information, the distribution of the clear sky index from our forecasts is in very good agreement with the distribution of the measurements. In a validation dataset of 46 days, we receive a positive forecast skill for all forecast horizons larger than 1 min. We also apply our forecast chain to a dataset of two month from an independent measurement station resulting in a comparable forecasting performance.},
   author = {Anna Dittmann and Nicolas Holland and Elke Lorenz},
   doi = {10.1127/metz/2020/1024},
   issn = {16101227},
   issue = {2},
   journal = {Meteorologische Zeitschrift},
   keywords = {Circumsolar region,Cloud detection,Irradiance model,Short-term solar forecasting,Sky imager},
   pages = {101-113},
   publisher = {Gebruder Borntraeger Verlagsbuchhandlung},
   title = {A new sky imager based global irradiance forecasting model with analyses of cirrus situations},
   volume = {30},
   year = {2021},
}
@article{,
   abstract = {This work proposes and evaluates methods for extending the forecasting horizon of all-sky imager (ASI)-based solar radiation nowcasts and estimating the uncertainty of these predictions. In addition, we evaluated procedures for improving the temporal resolution and latency of satellite-imagery-derived solar nowcasts. Based on these contributions, we assessed the reliability of ASIs and satellite-derived solar radiation nowcasts, with 1-min time-resolution and up-to-90-min ahead. The study was conducted in a location in Southern Spain using a set of cloudy days, specifically selected as representative of the most challenging conditions regarding solar radiation nowcasting. The results reveal that the use of ASI-based models provide low benefits compared to the use of satellite-based models for point solar radiation nowcasting. Given the frequency of occurrence of the different sky types in the study area, the results suggest that the use of a simple smart persistence algorithm, in combination with a low-resolution satellite nowcasting model could be an adequate choice, avoiding the challenges associated with the use of ASIs.},
   author = {Francisco J. Rodríguez-Benítez and Miguel López-Cuesta and Clara Arbizu-Barrena and María M. Fernández-León and Miguel Pamos-Ureña and Joaquín Tovar-Pescador and Francisco J. Santos-Alamillos and David Pozo-Vázquez},
   doi = {10.1016/j.apenergy.2021.116838},
   issn = {03062619},
   journal = {Applied Energy},
   keywords = {All-sky imagers (ASI),MSG satellite images,Short-term solar irradiance forecasting,Solar energy,Solar irradiance nowcasting},
   month = {6},
   publisher = {Elsevier Ltd},
   title = {Assessment of new solar radiation nowcasting methods based on sky-camera and satellite imagery},
   volume = {292},
   year = {2021},
}
@misc{Kumar2020,
   abstract = {With the increase in demand for energy, penetration of alternative sources of energy in the power grid has increased. Photovoltaic (PV) energy is the most common and popular form of energy sources which is widely integrated into the existing grid. As solar energy is intermittent in nature, to ensure uninterrupted and reliable power supply to the prosumers, it is essential to forecast the solar irradiance. Accurate solar forecasting is necessary to facilitate large-scale modelling and deployment of PV plants without disrupting the quality and reliability of the power grid as well as to manage the power demand and supply. There are various methods to predict the solar irradiance such as numerical weather prediction methods, satellite-based approaches, cloud-image based methodologies, data-driven methods, and sensor-network based approaches. This study gives an overall review of the different resources and methods used for forecasting solar irradiance in different time horizons and also gives an extensive review of the sensor networks that are used for determining solar irradiance. The various error metrics and accessible data sets available for the sensor networks are also discussed that can be used for validation purposes.},
   author = {Dhivya Sampath Kumar and Gokhan Mert Yagli and Monika Kashyap and Dipti Srinivasan},
   doi = {10.1049/iet-rpg.2019.1227},
   issn = {17521424},
   issue = {10},
   journal = {IET Renewable Power Generation},
   keywords = {atmospheric techniques,cloud-image based methodologies,energy sources,numerical weather prediction,photovoltaic energy,photovoltaic power systems,power demand,power grid,power grids,power supply,reviews,sensor-network,solar energy,solar forecasting,solar irradiance resource,solar power,solar power stations,sunlight,weather forecasting,wireless sensor networks},
   month = {7},
   pages = {1641-1656},
   publisher = {John Wiley and Sons Inc},
   title = {Solar irradiance resource and forecasting: a comprehensive review},
   volume = {14},
   year = {2020},
}
@article{Zhen2022,
   abstract = {Solar irradiance is the main factor affecting the output of a photovoltaic (PV) power station, which is chiefly determined by the cloud distribution over the power station. For ultra-short-term, especially the intro-hour time scale irradiance forecasting, ground-based cloud image is considered as a very necessary data as Global Horizontal Irradiance (GHI). However, the information content in the image is much higher than that of GHI record, and there is even a difference in magnitude between them. Therefore, how to effectively extract the key features in the cloud images and fuse them with GHI record data is the decisive factor affecting the performance of the forecasting model. Here, a novel convolutional auto-encoder based cloud distribution feature (CDF) extraction method is first proposed. Then for feature fusion part, an LSTM-FUSION irradiance forecasting model is established based on long short-term memory (LSTM) neural network and feature fusion by time steps considering the one-to-one correlation between CDFs and GHI. Finally, a novel determination method of input time step length based on attention distribution analysis is also proposed. Simulation results show that the proposed LSTM-FUSION model is overall superior to the benchmark models.},
   author = {Zhao Zhen and Xuemin Zhang and Shengwei Mei and Xiqiang Chang and Hua Chai and Rui Yin and Fei Wang},
   doi = {10.1049/rpg2.12280},
   issn = {17521424},
   issue = {12},
   journal = {IET Renewable Power Generation},
   month = {9},
   pages = {2604-2616},
   publisher = {John Wiley and Sons Inc},
   title = {Ultra-short-term irradiance forecasting model based on ground-based cloud image and deep learning algorithm},
   volume = {16},
   year = {2022},
}
@article{,
   abstract = {The energy available in a solar energy powered grid is uncertain due to the weather conditions at the time of generation. Forecasting global solar irradiance could address this problem by providing the power grid with the capability of scheduling the storage and dispatch of energy. The occlusion of the Sun by clouds is the main cause of instabilities in the generation of solar energy. This investigation proposes a method to visualize the wind velocity field in sequences of longwave infrared images of clouds when there are multiple wind velocity fields in an image. This method can be used to forecast the occlusion of the Sun by clouds, providing stability in the generation of solar energy. Unsupervised learning is implemented to infer the distribution of the clouds’ velocity vectors and heights in multiple wind velocity fields in an infrared image. A multi-output weighted support vector machine with flow constraints is used to extrapolate the wind velocity fields to the entire frame, visualizing the path of the clouds. The proposed method is capable of approximating the wind velocity field in a small air parcel using the velocity vectors and physical features of clouds extracted from infrared images. Assuming that the streamlines are pathlines, the visualization of the wind velocity field can be used for forecasting cloud occlusions of the Sun. This is of importance when considering ways of increasing the stability of solar energy generation.},
   author = {Guillermo Terrén-Serrano and Manel Martínez-Ramón},
   doi = {10.1016/j.apenergy.2021.116656},
   issn = {03062619},
   journal = {Applied Energy},
   keywords = {Beta mixture model,Cloud tracking,Flow visualization,Machine learning,Sky imaging,Solar forecasting},
   month = {4},
   publisher = {Elsevier Ltd},
   title = {Multi-layer wind velocity field visualization in infrared images of clouds for solar irradiance forecasting},
   volume = {288},
   year = {2021},
}
@article{Wen2021,
   abstract = {Solar forecasting is one of the most promising approaches to address the intermittent photovoltaic (PV) power generation by providing predictions before upcoming ramp events. In this article, a novel multistep forecasting (MSF) scheme is proposed for PV power ramp-rate control (PRRC). This method utilizes an ensemble of deep ConvNets without additional time series models (e.g., recurrent neural network (RNN) or long short-term memory) and exogenous variables, thus more suitable for industrial applications. The MSF strategy can make multiple predictions in comparison with a single forecasting point produced by a conventional method while maintaining the same high temporal resolution. Besides, stacked sky images that integrate temporal-spatial information of cloud motions are used to further improve the forecasting performance. The results demonstrate a favorable forecasting accuracy in comparison to the existing forecasting models with the highest skill score of 17.7%. In the PRRC application, the MSF-based PRRC can detect more ramp-rates violations with a higher control rate of 98.9% compared with the conventional forecasting-based control. Thus, the PV generation can be effectively smoothed with less energy curtailment on both clear and cloudy days using the proposed approach.},
   author = {Haoran Wen and Yang Du and Xiaoyang Chen and Enggee Lim and Huiqing Wen and Lin Jiang and Wei Xiang},
   doi = {10.1109/TII.2020.2987916},
   issn = {19410050},
   issue = {2},
   journal = {IEEE Transactions on Industrial Informatics},
   keywords = {Deep learning (DL),multistep forecasting (MSF),power ramp-rate control (PRRC),solar forecasting,stacked sky images (SIs)},
   month = {2},
   pages = {1397-1406},
   publisher = {IEEE Computer Society},
   title = {Deep Learning Based Multistep Solar Forecasting for PV Ramp-Rate Control Using Sky Images},
   volume = {17},
   year = {2021},
}
@misc{Roy2022,
   abstract = {A hybrid renewable energy source (HRES) consists of two or more renewable energy sources, suchas wind turbines and photovoltaic systems, utilized together to provide increased system efficiency and improved stability in energy supply to a certain degree. The objective of this study is to present a comprehensive review of wind-solar HRES from the perspectives of power architectures, mathematical modeling, power electronic converter topologies, and design optimization algorithms. Since the uncertainty of HRES can be reduced further by including an energy storage system, this paper presents several hybrid energy storage system coupling technologies, highlighting their major advantages and disadvantages. Various HRES power converters and control strategies from the state-of-the-art have been discussed. Different types of energy source combinations, modeling, power converter architectures, sizing, and optimization techniques used in the existing HRES are reviewed in this work, which intends to serve as a comprehensive reference for researchers, engineers, and policymakers in this field. This article also discusses the technical challenges associated with HRES as well as the scope of future advances and research on HRES.},
   author = {Pranoy Roy and Jiangbiao He and Tiefu Zhao and Yash Veer Singh},
   doi = {10.1109/OJIES.2022.3144093},
   issn = {26441284},
   journal = {IEEE Open Journal of the Industrial Electronics Society},
   keywords = {Hybrid renewable energy sources,hybrid energy storage system,optimization,photovoltaic power,power converter,wind turbine},
   pages = {81-104},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Recent Advances of Wind-Solar Hybrid Renewable Energy Systems for Power Generation: A Review},
   volume = {3},
   year = {2022},
}
@article{Duan2021,
   abstract = {Incoming solar radiation is a key factor influencing solar architecture design. It determines the thermal and optical regime of the building envelope and affects the solar heat and light transfer between the indoors and outdoors. Computational analysis is an essential tool in solar architecture design. Usually, an entire year's weather data in a conventional weather file can be imported into such computational analyses. Solar irradiance data used in a conventional solar architecture design analytics are broadband (the total of UV, VIS, and NIR); however, these three components play different roles in building energy efficiency. So, analyzing individual solar components separately can be desirable. This research is to develop estimation models of the VIS and NIR components that can be captured efficiently from readily available datasets collected from the ground weather stations; such a model can then be conveniently implemented into current solar architecture design and research. We explored and tested classification-based modeling methods for decomposing hourly broadband global horizontal solar irradiance data in conventional weather files into hourly global horizontal solar VIS and NIR components. Furthermore, a workflow of how to implement these models in solar architecture design and analysis has been developed and discussed herein.},
   author = {Qiuhua Duan and Yanxiao Feng and Julian Wang},
   doi = {10.1016/j.renene.2020.11.080},
   issn = {18790682},
   journal = {Renewable Energy},
   keywords = {Building energy,Classification trees,Prediction model,Solar architecture design,Solar energy,Solar radiation},
   month = {3},
   pages = {668-677},
   publisher = {Elsevier Ltd},
   title = {Clustering of visible and infrared solar irradiance for solar architecture design and analysis},
   volume = {165},
   year = {2021},
}
@article{Rajagukguk2021,
   abstract = {Citation: Rajagukguk, R.A.; Kamil, R.; Lee, H.-J. A Deep Learning Model to Forecast Solar Irradiance Using a Sky Camera. Appl. Sci. 2021, 11, 5049. Abstract: Solar irradiance fluctuates mainly due to clouds. A sky camera offers images with high temporal and spatial resolutions for a specific solar photovoltaic plant. The cloud cover from sky images is suitable for forecasting local fluctuations of solar irradiance and thereby solar power. Because no study applied deep learning for forecasting cloud cover using sky images, this study attempted to apply the long short-term memory algorithm in deep learning. Cloud cover data were collected by image processing of sky images and used for developing the deep learning model to forecast cloud cover 10 min ahead. The forecasted cloud cover data were plugged into solar radiation models as input in order to predict global horizontal irradiance. The forecasted results were grouped into three categories based on sky conditions: clear sky, partly cloudy, and overcast sky. By comparison with solar irradiance measurement at a ground station, the proposed model was evaluated. The proposed model outperformed the persistence model under high variability of solar irradiance such as partly cloudy days with relative root mean square differences for 10-min-ahead forecasting are 25.10% and 39.95%, respectively. Eventually, this study demonstrated that deep learning can forecast the cloud cover from sky images and thereby can be useful for forecasting solar irradiance under high variability.},
   author = {Rial A Rajagukguk and Raihan Kamil and Hyun-Jin Lee},
   doi = {10.3390/app},
   keywords = {cloud cover,deep learning,sky image,solar irradiance},
   title = {A Deep Learning Model to Forecast Solar Irradiance Using a Sky Camera},
   url = {https://doi.org/10.3390/app11115049},
   year = {2021},
}
@misc{Park2022,
   author = {Junho Park},
   title = {Hybrid Machine Learning and Physics-Based Modeling Hybrid Machine Learning and Physics-Based Modeling Approaches for Process Control and Optimization Approaches for Process Control and Optimization},
   url = {https://scholarsarchive.byu.edu/etd},
   year = {2022},
}
@article{Nevins2021,
   abstract = {Environmental photodegradation is dependent on the solar irradiance that reaches the Earth's surface, and photodegradation half-lives of contaminants are typically estimated assuming clear sky (i.e., cloudless) conditions. In this work, the effect of cloud cover on solar irradiance was investigated. Data from the National Renewable Energy Laboratory (NREL), which spanned 3 years of observations (10/2017 to 12/2020), were used to train two machine learning models to predict irradiance based on three inputs-day of year, time of day, and percentage of the sky that was cloudy. Results showed a non-linear relationship between cloud cover and irradiance. Solar irradiance was minimally impacted up to ≈50% cloud cover but decreased by ≈67% at 100% cloud cover. Both random forest and artificial neural network models performed well with relative root mean squared errors of 26-31%, which varied depending on the source of cloud cover data and the spectral region being modeled. Daily irradiance values for a whole year were predicted for varying cloud conditions using the machine learning models; this result was approximated using a quadratic fit of y = 1 - 0.00243x - (4.24 × 10-5)x2 where y is the fraction of clear sky irradiance expected and x is the percentage of cloud cover in the sky. In addition, the model results supported that there was no wavelength dependence for the effect of cloud cover. Therefore, decreases in both direct and indirect photodegradation rates should be proportional to the decrease in irradiance, which has a non-linear dependence on cloud cover.},
   author = {Michelle G. Nevins and Jennifer N. Apell},
   doi = {10.1039/d1em00314c},
   issn = {20507895},
   issue = {12},
   journal = {Environmental Science: Processes and Impacts},
   month = {12},
   pages = {1884-1892},
   pmid = {34753158},
   publisher = {Royal Society of Chemistry},
   title = {Emerging investigator series: Quantifying the impact of cloud cover on solar irradiance and environmental photodegradation},
   volume = {23},
   year = {2021},
}
@article{Blanchi2022,
   abstract = {There is a huge gap between (1) the state of workflow technology on the one hand and the practices in the many labs working with data driven methods on the other and (2) the awareness of the FAIR principles and the lack of changes in practices during the last 5 years. The CWFR concept has been defined which is meant to combine these two intentions, increasing the use of workflow technology and improving FAIR compliance. In the study described in this paper we indicate how this could be applied to machine learning which is now used by almost all research disciplines with the well-known effects of a huge lack of repeatability and reproducibility. Researchers will only change practices if they can work efficiently and are not loaded with additional tasks. A comprehensive CWFR framework would be an umbrella for all steps that need to be carried out to do machine learning on selected data collections and immediately create a comprehensive and FAIR compliant documentation. The researcher is guided by such a framework and information once entered can easily be shared and reused. The many iterations normally required in machine learning can be dealt with efficiently using CWFR methods. Libraries of components that can be easily orchestrated using FAIR Digital Objects as a common entity to document all actions and to exchange information between steps without the researcher needing to understand anything about PIDs and FDO details is probably the way to increase efficiency in repeating research workflows. As the Galaxy project indicates, the availability of supporting tools will be important to let researchers use these methods. Other as the Galaxy framework suggests, however, it would be necessary to include all steps necessary for doing a machine learning task including those that require human interaction and to document all phases with the help of structured FDOs.},
   author = {Christophe Blanchi and Binyam Gebre and Peter Wittenburg},
   doi = {10.1162/dint_a_00124},
   issn = {2641435X},
   issue = {2},
   journal = {Data Intelligence},
   keywords = {Data management,Digital objects,FAIR,Machine learning,Workflow},
   month = {4},
   pages = {173-185},
   publisher = {MIT Press Journals},
   title = {Canonical Workflow for Machine Learning Tasks},
   volume = {4},
   year = {2022},
}
@misc{Benghanem2023,
   abstract = {Several research works have investigated the direct supply of renewable electricity to electrolysis, particularly from photovoltaic (PV) and wind generator (WG) systems. Hydrogen (H2) production based on solar energy is considered to be the newest solution for sustainable energy. Different technologies based on solar energy which allow hydrogen production are presented to study their benefits and inconveniences. The technology of water decomposition based on renewable energy sources, to produce hydrogen, can be achieved by different processes (photochemical systems; photocatalysis systems, photo-electrolysis systems, bio-photolysis systems, thermolysis systems, thermochemical cycles, steam electrolysis, hybrid processes, and concentrated solar energy systems). A comparison of the different methods for hydrogen production based on PV and WG systems was given in this study. A comparative study of different types of electrolyzers was also presented and discussed. Finally, an economic assessment of green hydrogen production is given. The hydrogen production cost depends on several factors, such as renewable energy sources, electrolysis type, weather conditions, installation cost, and the productivity of hydrogen per day. PV/H2 and wind/H2 systems are both suitable in remote and arid areas. Minimum maintenance is required, and a power cycle is not needed to produce electricity. The concentrated CSP/H2 system needs a power cycle. The hydrogen production cost is higher if using wind/H2 rather than PV/H2. The green energy sources are useful for multiple applications, such as hydrogen production, cooling systems, heating, and water desalination.},
   author = {Mohamed Benghanem and Adel Mellit and Hamad Almohamadi and Sofiane Haddad and Nedjwa Chettibi and Abdulaziz M. Alanazi and Drigos Dasalla and Ahmed Alzahrani},
   doi = {10.3390/en16020757},
   issn = {19961073},
   issue = {2},
   journal = {Energies},
   keywords = {electrolysis process,hydrogen production,photovoltaic electrolysis,photovoltaic thermal system,renewable energy sources,solar water thermolysis},
   month = {1},
   publisher = {MDPI},
   title = {Hydrogen Production Methods Based on Solar and Wind Energy: A Review},
   volume = {16},
   year = {2023},
}
@article{Koleva2023,
   abstract = {As stationary hybrid energy-storage systems (HESS) for power systems applications have recently drawn interest due to their enhanced performance and decreasing cost, developing systematic approaches for HESS design while considering controls is gaining traction. Herein, a method is presented to optimally design hybrid battery storage by proposing a mathematical modeling framework, formulated as a mixed integer linear programming model. The optimization is capable of handling multiple subsystems of batteries, considering their economic and technological performance. Decisions involve sizing of the batteries, optimal temporal and strategic dispatch to end uses, and energy sources for charging each battery. The applicability of the model is tested on four case studies for three battery chemistries representing distinct objectives: high-power, high-energy, and second life. Compared to traditionally designed battery storage with a homogeneous battery, optimally designed hybrid systems can save 12%–26% of system costs, depending on the nature of the dispatch profile. Findings point to design preference toward the second life battery supplemented with some high-power or high-energy battery capacity, or both. With the utilized electricity price structure, customers can experience approximately 10%–35% reduction in their bills.},
   author = {Mariya Koleva and Ying Shi and Killian McKenna and Michael Craig and Adarsh Nagarajan},
   doi = {10.1002/ente.202300115},
   issn = {21944296},
   issue = {10},
   journal = {Energy Technology},
   keywords = {electricity markets,hybrid energy storage,multiple battery chemistries,optimization,renewable energy},
   month = {10},
   publisher = {John Wiley and Sons Inc},
   title = {Optimal Strategies for Hybrid Battery-Storage Systems Design},
   volume = {11},
   year = {2023},
}
@misc{Nasser2022,
   abstract = {Hydrogen energy, as clean and efficient energy, is considered significant support for the construction of a sustainable society in the face of global climate change and the looming energy revolution. Hydrogen is one of the most important chemical substances on earth and can be obtained through various techniques using renewable and nonrenewable energy sources. However, the necessity for a gradual transition to renewable energy sources significantly hampers efforts to identify and implement green hydrogen production paths. Therefore, this paper’s objective is to provide a technological review of the systems of hydrogen production from solar and wind energy utilizing several types of water electrolyzers. The current paper starts with a short brief about the different production techniques. A detailed comparison between water electrolyzer types and a complete illustration of hydrogen production techniques using solar and wind are presented with examples, after which an economic assessment of green hydrogen production by comparing the costs of the discussed renewable sources with other production methods. Finally, the challenges that face the mentioned production methods are illuminated in the current review.},
   author = {Mohamed Nasser and Tamer F. Megahed and Shinichi Ookawara and Hamdy Hassan},
   doi = {10.1007/s11356-022-23323-y},
   issn = {16147499},
   issue = {58},
   journal = {Environmental Science and Pollution Research},
   keywords = {Clean hydrogen,Hydrogen economy,Low/high-temperature electrolyzers,Multi-generation system,Renewable energy,Sustainable hydrogen production},
   month = {12},
   pages = {86994-87018},
   pmid = {36280638},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A review of water electrolysis–based systems for hydrogen production using hybrid/solar/wind energy systems},
   volume = {29},
   year = {2022},
}
@misc{Guerra2023,
   abstract = {As energy systems across the globe transition toward net-zero emissions, the decarbonization of hard-to-decarbonize sectors, e.g., industry and transportation, is becoming more crucial. Renewable power-driven carbon dioxide (CO2) electrolysis has the potential to facilitate this transition by (1) substituting carbon-intensive petrochemical and fuel production and (2) using CO2 otherwise emitted from industrial processes or CO2 from the atmosphere; however, because of existing technical and economic challenges, the industrial deployment of this technology is not yet imminent. Here, we present an overview of CO2 electrolysis technologies to identify key hurdles in view of the industrial deployment of this technology in net-zero emissions energy systems. From the technology standpoint, catalysts should be developed with enhanced activity, selectivity, and stability/durability as well as membranes and reactors that prevent carbonate formation or crossover, achieve higher reaction rates, e.g., >1 A/cm2, and demonstrate long-term stability, e.g., >5 years. Conversely, from the system integration standpoint, impurity-tolerant CO2 electrolysis systems need to be developed and tested under relevant conditions, e.g., CO2 streams with traces of impurities (NOx, SOx, O2, N2, H2S, etc.). Additionally, the quantification of pros and cons of different integration pathways for CO2 capture and CO2 electrolysis requires further research. Moreover, the integration with variable renewable power sources—e.g., wind and solar photovoltaic power—and electricity markets requires a better understanding. For instance, the value of CO2 electrolysis flexibility in view of variable renewable power supply or dynamic electricity prices is not well understood.},
   author = {Omar J. Guerra and Hussain M. Almajed and Wilson A. Smith and Ana Somoza-Tornos and Bri Mathias S. Hodge},
   doi = {10.1016/j.joule.2023.05.002},
   issn = {25424351},
   issue = {6},
   journal = {Joule},
   keywords = {CO2 capture,CO2 electrolysis,electricity markets,energy systems,net-zero emissions,system integration,variable renewable power},
   month = {6},
   pages = {1111-1133},
   publisher = {Cell Press},
   title = {Barriers and opportunities for the deployment of CO2 electrolysis in net-zero emissions energy systems},
   volume = {7},
   year = {2023},
}
@article{Kursa2010,
   abstract = {Machine learning methods are often used to classify objects described by hundreds of attributes; in many applications of this kind a great fraction of attributes may be totally irrelevant to the classification problem. Even more, usually one cannot decide a priori which attributes are relevant. In this paper we present an improved version of the algorithm for identification of the full set of truly important variables in an information system. It is an extension of the random forest method which utilises the importance measure generated by the original algorithm. It compares, in the iterative fashion, the importances of original attributes with importances of their randomised copies. We analyse performance of the algorithm on several examples of synthetic data, as well as on a biologically important problem, namely on identification of the sequence motifs that are important for aptameric activity of short RNA sequences.},
   author = {Miron B. Kursa and Aleksander Jankowski and Witold R. Rudnicki},
   doi = {10.3233/FI-2010-288},
   issn = {01692968},
   issue = {4},
   journal = {Fundamenta Informaticae},
   pages = {271-285},
   title = {Boruta - A system for feature selection},
   volume = {101},
   year = {2010},
}
@misc{Rawlings2022,
   author = {James B Rawlings and David Q Mayne and Moritz M Diehl and Santa Barbara},
   isbn = {2020942771},
   title = {Model Predictive Control: Theory, Computation, and Design 2nd Edition},
   url = {http://www.nobhillpublishing.com},
   year = {2022},
}
@misc{Kursa2010,
   abstract = {This article describes a R package Boruta, implementing a novel feature selection algorithm for finding all relevant variables. The algorithm is designed as a wrapper around a Random Forest classification algorithm. It iteratively removes the features which are proved by a statistical test to be less relevant than random probes. The Boruta package provides a convenient interface to the algorithm. The short description of the algorithm and examples of its application are presented.},
   author = {Miron B Kursa and Witold R Rudnicki},
   journal = {JSS Journal of Statistical Software},
   keywords = {feature ranking,feature selection,random forest},
   title = {Feature Selection with the Boruta Package},
   volume = {36},
   url = {http://www.jstatsoft.org/},
   year = {2010},
}
@article{Xu2008,
   abstract = {Lasso, or $\ell^1$ regularized least squares, has been explored extensively for its remarkable sparsity properties. It is shown in this paper that the solution to Lasso, in addition to its sparsity, has robustness properties: it is the solution to a robust optimization problem. This has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows a principled selection of the regularizer, and in particular, generalizations of Lasso that also yield convex optimization problems are obtained by considering different uncertainty sets. Secondly, robustness can itself be used as an avenue to exploring different properties of the solution. In particular, it is shown that robustness of the solution explains why the solution is sparse. The analysis as well as the specific results obtained differ from standard sparsity results, providing different geometric intuition. Furthermore, it is shown that the robust optimization formulation is related to kernel density estimation, and based on this approach, a proof that Lasso is consistent is given using robustness directly. Finally, a theorem saying that sparsity and algorithmic stability contradict each other, and hence Lasso is not stable, is presented.},
   author = {Huan Xu and Constantine Caramanis and Shie Mannor},
   month = {11},
   title = {Robust Regression and Lasso},
   url = {http://arxiv.org/abs/0811.1790},
   year = {2008},
}
@article{You2018,
   abstract = {In this paper, we introduce a novel regularization method called Adversarial Noise Layer (ANL) and its efficient version called Class Adversarial Noise Layer (CANL), which are able to significantly improve CNN's generalization ability by adding carefully crafted noise into the intermediate layer activations. ANL and CANL can be easily implemented and integrated with most of the mainstream CNN-based models. We compared the effects of the different types of noise and visually demonstrate that our proposed adversarial noise instruct CNN models to learn to extract cleaner feature maps, which further reduce the risk of over-fitting. We also conclude that models trained with ANL or CANL are more robust to the adversarial examples generated by FGSM than the traditional adversarial training approaches.},
   author = {Zhonghui You and Jinmian Ye and Kunming Li and Zenglin Xu and Ping Wang},
   month = {5},
   title = {Adversarial Noise Layer: Regularize Neural Network By Adding Noise},
   url = {http://arxiv.org/abs/1805.08000},
   year = {2018},
}
@article{Duan2019,
   abstract = {We present Natural Gradient Boosting (NGBoost), an algorithm for generic probabilistic prediction via gradient boosting. Typical regression models return a point estimate, conditional on covariates, but probabilistic regression models output a full probability distribution over the outcome space, conditional on the covariates. This allows for predictive uncertainty estimation -- crucial in applications like healthcare and weather forecasting. NGBoost generalizes gradient boosting to probabilistic regression by treating the parameters of the conditional distribution as targets for a multiparameter boosting algorithm. Furthermore, we show how the Natural Gradient is required to correct the training dynamics of our multiparameter boosting approach. NGBoost can be used with any base learner, any family of distributions with continuous parameters, and any scoring rule. NGBoost matches or exceeds the performance of existing methods for probabilistic prediction while offering additional benefits in flexibility, scalability, and usability. An open-source implementation is available at github.com/stanfordmlgroup/ngboost.},
   author = {Tony Duan and Anand Avati and Daisy Yi Ding and Khanh K. Thai and Sanjay Basu and Andrew Y. Ng and Alejandro Schuler},
   month = {10},
   title = {NGBoost: Natural Gradient Boosting for Probabilistic Prediction},
   url = {http://arxiv.org/abs/1910.03225},
   year = {2019},
}
@article{Rosenblatt1958,
   author = {F Rosenblatt},
   issue = {6},
   journal = {Psychological Review},
   pages = {19-27},
   title = {THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1},
   volume = {65},
   year = {1958},
}
@misc{Zhang2009,
   abstract = {This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches infinity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefficient is larger than a constant times the noise level. In comparison , Lasso may require the coefficients to be larger than O(√ s) times the noise level in the worst case, where s is the number of nonzero coefficients.},
   author = {Tong Zhang},
   journal = {Journal of Machine Learning Research},
   keywords = {feature selection,greedy algorithm,sparsity},
   pages = {555-568},
   title = {On the Consistency of Feature Selection using Greedy Least Squares Regression},
   volume = {10},
   year = {2009},
}
@article{Samo2021,
   abstract = {We introduce the first application of the lean methodology to machine learning projects. Similar to lean startups and lean manufacturing, we argue that lean machine learning (LeanML) can drastically slash avoidable wastes in commercial machine learning projects, reduce the business risk in investing in machine learning capabilities and, in so doing, further democratize access to machine learning. The lean design pattern we propose in this paper is based on two realizations. First, it is possible to estimate the best performance one may achieve when predicting an outcome $y \in \mathcal\{Y\}$ using a given set of explanatory variables $x \in \mathcal\{X\}$, for a wide range of performance metrics, and without training any predictive model. Second, doing so is considerably easier, faster, and cheaper than learning the best predictive model. We derive formulae expressing the best $R^2$, MSE, classification accuracy, and log-likelihood per observation achievable when using $x$ to predict $y$ as a function of the mutual information $I\left(y; x\right)$, and possibly a measure of the variability of $y$ (e.g. its Shannon entropy in the case of classification accuracy, and its variance in the case regression MSE). We illustrate the efficacy of the LeanML design pattern on a wide range of regression and classification problems, synthetic and real-life.},
   author = {Yves-Laurent Kom Samo},
   month = {7},
   title = {LeanML: A Design Pattern To Slash Avoidable Wastes in Machine Learning Projects},
   url = {http://arxiv.org/abs/2107.08066},
   year = {2021},
}
@article{Hassan2023,
   abstract = {The research study provides a techno-economic analysis for the green hydrogen generation based solar radiation data for both the single and hybrid alkaline water electrolyzer and energy storage system systems. In addition, a carbon footprint study is conducted to estimate the developed system carbon dioxide emissions. The optimal size of the alkaline water electrolyzer and energy storage system is determined by a genetic algorithm that takes into account a carbon tax on carbon emissions. Based on itemized cost estimating findings, unit hydrogen production costs for a single system and a hybrid system were $6.88/kg and $8.32/kg respectively. Furthermore, capital cost it has been found as a key element in determining the optimal scale of the alkaline water electrolyzer and energy storage system, which are essential for minimizing the unit hydrogen production cost. Lastly, an effort to minimize the capital cost of producing green hydrogen is required when the rising trend of the carbon dioxide tax is taken into account.},
   author = {Qusay Hassan and Aws Zuhair Sameen and Hayder M. Salman and Marek Jaszczur},
   doi = {10.1515/ehs-2023-0011},
   issn = {23298766},
   journal = {Energy Harvesting and Systems},
   keywords = {alkaline water electrolysis,hydrogen energy,photovoltaic energy,renewable hydrogen},
   publisher = {Walter de Gruyter GmbH},
   title = {Large-scale green hydrogen production using alkaline water electrolysis based on seasonal solar radiation},
   year = {2023},
}
@misc{,
   abstract = {Power-to-X is one of the most attention-grabbing topics in the energy sector. Researchers are exploring the potential of harnessing power from renewable technologies and converting it into fuels used in various industries and the transportation sector. With the current market and research emphasis on Power-to-X and the accompanying substantial investments, a review of Power-to-X is becoming essential. Optimization will be a crucial aspect of managing an energy portfolio that includes Power-to-X and electrolysis systems, as the electrolyzer can participate in multiple markets. Based on the current literature and published reviews, none of them adequately showcase the state-of-the-art optimization algorithms for energy portfolios focusing on Power-to-X. Therefore, this paper provides an in-depth review of the optimization algorithms applied to energy portfolios with a specific emphasis on Power-to-X, aiming to uncover the current state-of-the-art in the field.},
   author = {Nicolai Lystbæk and Mikkel Gregersen and Hamid Reza Shaker},
   doi = {10.3390/su15054422},
   issn = {20711050},
   issue = {5},
   journal = {Sustainability (Switzerland)},
   keywords = {Power-to-X,ancillary service,dynamic efficiency,electricity market,electrolysis,electrolyzer,flexibility,hydrogen,optimization},
   month = {3},
   publisher = {MDPI},
   title = {Review of Energy Portfolio Optimization in Energy Markets Considering Flexibility of Power-to-X},
   volume = {15},
   year = {2023},
}
@misc{Mcculloch1943,
   abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propo-sitional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiologi-cal assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
   author = {Warren S Mcculloch and Walter Pitts},
   journal = {BULLETIN OF MATHEMATICAL BIOPHYSICS},
   title = {A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY},
   volume = {5},
   year = {1943},
}
@article{Ginsberg2022,
   abstract = {Solar electricity enables the advancement and deployment of technologies that are strongly influenced by clean energy availability and cost. The economics of both desalination and hydrogen production from water electrolysis are dominated by the cost of energy, and the availability of inexpensive solar energy creates markets and offers incentives to the desalination and electrolyzer industries. Herein, production of high-purity water and hydrogen from seawater is focused on. Current electrolyzers require deionized water so they need to be coupled with desalination units. It is shown that such coupling is cost effective in hydrogen generation, and it also offers benefits to thermal desalination, which can utilize waste heat from electrolysis. Furthermore, such coupling can be optimized when electrolyzers operate at high current density, using low-cost solar and/or wind electricity, as such operation increases both hydrogen production and heat generation. Results of technoeconomic modeling of polymer electrolyte membrane electrolyzers define thresholds of electricity pricing, current density, and operating temperature that make clean electrolytic hydrogen cost competitive with hydrogen from steam methane reforming (SMR). By using 2020 hourly electricity pricing in California and Texas, it is estimated that hydrogen can be produced from seawater in coupled desalination−electrolyzer systems at prices near $2 kg−1 H2, reaching cost parity with hydrogen produced from SMR.},
   author = {Michael Ginsberg and Zhuoran Zhang and Adam A. Atia and Maya Venkatraman and Daniel V. Esposito and Vasilis M. Fthenakis},
   doi = {10.1002/solr.202100732},
   issn = {2367198X},
   issue = {5},
   journal = {Solar RRL},
   keywords = {PEM electrolyzers,cogeneration,electrolysis,renewable energy desalination,solar energy},
   month = {5},
   publisher = {John Wiley and Sons Inc},
   title = {Integrating Solar Energy, Desalination, and Electrolysis},
   volume = {6},
   year = {2022},
}
@article{Mazzeo2022,
   abstract = {Hydrogen is a promising energy carrier to provide sustainable energy use throughout the world. Researchers and policy-makers have focused on investigations in three areas of hydrogen-related technologies in the energy market: (1) alternative fuel production based on hydrogen and carbon dioxide; (2) hydrogen injection to the natural gas pipeline networks; (3) usage of hydrogen in transportation applications. One of the most important challenges facing hydrogen technology development is the production of green hydrogen, which can be achieved through water electrolysis coupled with renewable power generation. Although many studies have been conducted, there is still a need for further development, which requires open-source big data and models and standardization of the processes to compare different renewable-based hydrogen production systems. To fill this research gap, the performance of a grid-connected hybrid wind turbine and solar photovoltaic-based water electrolysis systems for large-scale green hydrogen production were investigated. The objective is to propose an accurate methodology to compare wind and solar systems, or hybrid ones, for green hydrogen production worldwide. A large dataset describing the hybrid wind turbine-photovoltaic hydrogen production in various locations was created by performing dynamic simulations using TRNSYS and analyzed using MATLAB and Excel. Several dimensionless indicators were employed and assessed for standardizing the performance evaluation procedure of renewable-based hydrogen production systems, including the electrolyser load portion satisfied by the wind turbine and photovoltaic, the generated wind turbine and photovoltaic energy portion supplied to the electrolyser load and the quantity of energy exported to and imported from the grid. The annual and monthly values of these indicators together with the total amount of green hydrogen production were estimated for 28 global locations. The procedure applied can be easily used to test small-scale applications like residential users or large-scale applications like industrial users, as well as for any hydrogen demand and climatic conditions.},
   author = {Domenico Mazzeo and Münür Sacit Herdem and Nicoletta Matera and John Z. Wen},
   doi = {10.1016/j.renene.2022.09.057},
   issn = {18790682},
   journal = {Renewable Energy},
   keywords = {Green hydrogen,Grid-connected system,Photovoltaic,Renewable-based system,Water electrolysis system,Wind},
   month = {11},
   pages = {360-378},
   publisher = {Elsevier Ltd},
   title = {Green hydrogen production: Analysis for different single or combined large-scale photovoltaic and wind renewable systems},
   volume = {200},
   year = {2022},
}
@article{Liu2023,
   abstract = {Identifying the primary sources of exergy destruction is a powerful method for promoting the high-efficiency operation of multi-energy supply systems. Advanced exergy analysis identifies avoidable parts of the destruction and discovers interactions between subsystems of a multi-energy complementary coupled system to explore the potential for system improvement. In this paper, a wind-solar‑hydrogen multi-energy supply (WSH-MES) system is studied, in which wind farms, photovoltaic power plants, solar thermal power plants, and hydrogen grid systems are coupled at the grid side to share the electrical load. To optimize power generation and improve system performance, a bi-level capacity-operation co-optimization model is developed specifically for the WSH-MES system. The upper level of the model employs a multi-objective optimization approach to find the best balance between energy, exergy, and advanced exergy and economy. The model is solved using the Non-dominated Sorting Genetic Algorithm-II and linear programming. To gain insights into the system's performance, advanced exergy analysis is performed under the optimal capacity configuration. By considering avoidable exergy destruction efficiency and exergy destruction proportion, the sequence of optimization potential for the WSH-MES system is determined. Surprisingly, the findings entirely different from that of conventional exergy analysis and reveal the interrelations between the analyzed subsystems. The sequence of optimization potential for the WSH-MES system is found to be as follows: concentrated solar power, photovoltaics, proton exchange membrane fuel cells, wind power, and proton exchange membrane electrolyzers. The identified optimization potential provides valuable guidance for improving energy efficiency, reducing costs, minimizing environmental impact, and serves as a reference for the design and optimization of new systems.},
   author = {Lintong Liu and Rongrong Zhai and Yangdi Hu},
   doi = {10.1016/j.apenergy.2023.121512},
   issn = {03062619},
   journal = {Applied Energy},
   keywords = {Advanced exergy analysis,Capacity-operation collaborative optimization,Optimization design,WSH-MES system},
   month = {10},
   publisher = {Elsevier Ltd},
   title = {Multi-objective optimization with advanced exergy analysis of a wind-solar‑hydrogen multi-energy supply system},
   volume = {348},
   year = {2023},
}
@article{Corengia2022,
   abstract = {Green hydrogen is a promising alternative for decarbonization of the economy. Its production is based on electrolysis powered by energy from renewable sources. Modern renewable sources like wind or solar are of an intermittent nature, a fact that poses a challenge to the operation of the electrolyzers. In this work, we present a superstructure-based optimization framework to design hydrogen production processes. The design involves selection of the power sources (wind/sun generators, grid), selection of the type and size of electrolyzer, and selection of energy storage devices (battery, vessels). The case studies include hydrogen production at different scales to allow a discussion on patterns of technology selection strategies and the synergies between them. Overall, our simulations show that solid oxide electrolyzers are a promising option, and that from the currently market available alternatives, alkaline electrolysis is preferred over proton exchange membrane electrolysis. Our results support the idea that complementary of the pattern of the energy sources should be sought, but not the idea of producing hydrogen to take advantage of energy surplus and avoid curtailment.},
   author = {Mariana Corengia and Ana I. Torres},
   doi = {10.1016/j.cherd.2022.05.007},
   issn = {02638762},
   journal = {Chemical Engineering Research and Design},
   keywords = {Decarbonization,Electrolysis,Green hydrogen,Optimal process design,P2G,P2X,Renewable energy},
   month = {7},
   pages = {235-249},
   publisher = {Institution of Chemical Engineers},
   title = {Coupling time varying power sources to production of green-hydrogen: A superstructure based approach for technology selection and optimal design},
   volume = {183},
   year = {2022},
}
@article{Liu2023,
   abstract = {This study presents an assessment of the energy, exergy, economic, and environmental aspects of a novel wind-solar-hydrogen multi-energy supply (WSH-MES) system. The design of the electric-thermal-hydrogen generation system utilizes photovoltaic, wind power, solar thermal power generation, electrolytic cell, hydrogen storage tank, and proton exchange membrane fuel cell. The fuel cell serves as a peak power source and shares the power load with the other renewable energy sources, smoothing out the fluctuations in wind and photovoltaic power generation through controlled solar thermal power generation and hydrogen production. A case study of the proposed system was conducted in Zhangbei, China, using MATLAB/Simulink software. The results show that the system has the potential to produce 931.39 kg of hydrogen per year, with an energy efficiency of 16.03% and an exergy efficiency of 17.94%. The economic analysis reveals that Zhangbei has the lowest levelized cost of energy (LCOE) of 0.2755 $/kWh and the highest net present value (NPV) of 5.06 M$. The payback time is approximately 3 years, and the system is expected to reduce 4,220,000 tons of CO2 over its lifetime. The integration of multiple renewable energy sources and hydrogen production in the WSH-MES system enhances the utilization rate of renewable energy and offers a promising solution for sustainable energy production and utilization.},
   author = {Lintong Liu and Rongrong Zhai and Yangdi Hu},
   doi = {10.1016/j.energy.2023.127386},
   issn = {03605442},
   journal = {Energy},
   keywords = {4E analysis,Hydrogen production and storage,PV/WP/CSP/PEME/PEMFC,Renewable energy generation},
   month = {8},
   publisher = {Elsevier Ltd},
   title = {Performance evaluation of wind-solar-hydrogen system for renewable energy generation and green hydrogen generation and storage: Energy, exergy, economic, and enviroeconomic},
   volume = {276},
   year = {2023},
}
@article{Rezaei2022,
   abstract = {This study examines the sensitivity of the levelised cost of hydrogen (LCOH), produced from solar photovoltaic (PV) electricity, to four factors that strongly influence the economics of green hydrogen: electrolyser efficiency, PV capacity factor, nominal interest rate and inflation rate. The authors' aim was not to calculate an absolute value for the LCOH, which varies according to location and economic circumstances, but to examine its sensitivity to these critical parameters of the economic model. This approach facilitates comparisons between potential solar hydrogen projects to select the location with the lowest LCOH. Direct coupling of a PV power plant to proton exchange membrane (PEM) electrolysis, without storage, was assumed, along with a base-case scenario with nominal interest rate 7%, inflation rate 2%, electrolyser efficiency 75% and PV capacity factor 22%. To account for the rapidly evolving electrolyser market, a learning-rate model was employed to estimate for the cost of routine end-of-life replacement of the electrolyser. Finally, the effect of grid-assisted operation on the LCOH was considered. The results demonstrated clearly the importance of careful site selection to achieve high PV capacity factor, which was more influential than foreseeable increases in electrolyser efficiency. Moreover, examination of the mutual sensitivities between the four critical parameters showed that high capacity factor is a good hedge against high inflation rates.},
   author = {Mostafa Rezaei and Alexandr Akimov and Evan Mac A. Gray},
   doi = {10.1016/j.ijhydene.2022.06.116},
   issn = {03603199},
   issue = {65},
   journal = {International Journal of Hydrogen Energy},
   keywords = {Economic analysis,Learning rate,Solar-based hydrogen generation},
   month = {7},
   pages = {27930-27943},
   publisher = {Elsevier Ltd},
   title = {Economics of solar-based hydrogen production: Sensitivity to financial and technical factors},
   volume = {47},
   year = {2022},
}
@article{Corengia2022,
   abstract = {Green hydrogen is a promising alternative for decarbonization of the economy. Its production is based on electrolysis powered by energy from renewable sources. Modern renewable sources like wind or solar are of an intermittent nature, a fact that poses a challenge to the operation of the electrolyzers. In this work, we present a superstructure-based optimization framework to design hydrogen production processes. The design involves selection of the power sources (wind/sun generators, grid), selection of the type and size of electrolyzer, and selection of energy storage devices (battery, vessels). The case studies include hydrogen production at different scales to allow a discussion on patterns of technology selection strategies and the synergies between them. Overall, our simulations show that solid oxide electrolyzers are a promising option, and that from the currently market available alternatives, alkaline electrolysis is preferred over proton exchange membrane electrolysis. Our results support the idea that complementary of the pattern of the energy sources should be sought, but not the idea of producing hydrogen to take advantage of energy surplus and avoid curtailment.},
   author = {Mariana Corengia and Ana I. Torres},
   doi = {10.1016/j.cherd.2022.05.007},
   issn = {02638762},
   journal = {Chemical Engineering Research and Design},
   keywords = {Decarbonization,Electrolysis,Green hydrogen,Optimal process design,P2G,P2X,Renewable energy},
   month = {7},
   pages = {235-249},
   publisher = {Institution of Chemical Engineers},
   title = {Coupling time varying power sources to production of green-hydrogen: A superstructure based approach for technology selection and optimal design},
   volume = {183},
   year = {2022},
}
@article{,
   abstract = {The optimisation of green hydrogen production systems is challenging. Moreover, an accurate simulation of the system is required for effective optimisation. This study presents a novel method for optimising utility-scale hybrid photovoltaic–wind systems for hydrogen production using accurate simulation models. The optimisation objective is to minimise the levelised cost of hydrogen (LCOH) using genetic algorithms. Different types of systems (such as islanded systems, grid-connected systems with or without the possibility of purchasing electricity from the grid, and grid-connected systems considering power curtailment), are evaluated and optimised. Each combination of components and control strategy is simulated during the system lifetime (20 yrs) in time steps of 5 min, considering the degradation of renewable generators during the system lifetime and different real-time pricing curves and renewable resource curves for each year of the system lifetime. Accurate models are used in the simulations, including electrolyser efficiency dependent on the input power and cold-start extra ageing. An application example located in Zaragoza (Spain) is shown, obtaining LCOH from 4.74 to 16.06 €/kg, depending on the type of project and electrolyser.},
   author = {Rodolfo Dufo-López and Juan M. Lujano-Rojas and José L. Bernal-Agustín},
   doi = {10.1016/j.ijhydene.2023.08.273},
   issn = {03603199},
   journal = {International Journal of Hydrogen Energy},
   keywords = {Electrolyser,Green hydrogen,Optimisation,PV–Wind–hydrogen system,Photovoltaic,Wind turbines},
   month = {1},
   pages = {292-309},
   publisher = {Elsevier Ltd},
   title = {Optimisation of size and control strategy in utility-scale green hydrogen production systems},
   volume = {50},
   year = {2024},
}
@article{Ash2019,
   abstract = {In many real-world deployments of machine learning systems, data arrive piecemeal. These learning scenarios may be passive, where data arrive incrementally due to structural properties of the problem (e.g., daily financial data) or active, where samples are selected according to a measure of their quality (e.g., experimental design). In both of these cases, we are building a sequence of models that incorporate an increasing amount of data. We would like each of these models in the sequence to be performant and take advantage of all the data that are available to that point. Conventional intuition suggests that when solving a sequence of related optimization problems of this form, it should be possible to initialize using the solution of the previous iterate -- to "warm start" the optimization rather than initialize from scratch -- and see reductions in wall-clock time. However, in practice this warm-starting seems to yield poorer generalization performance than models that have fresh random initializations, even though the final training losses are similar. While it appears that some hyperparameter settings allow a practitioner to close this generalization gap, they seem to only do so in regimes that damage the wall-clock gains of the warm start. Nevertheless, it is highly desirable to be able to warm-start neural network training, as it would dramatically reduce the resource usage associated with the construction of performant deep learning systems. In this work, we take a closer look at this empirical phenomenon and try to understand when and how it occurs. We also provide a surprisingly simple trick that overcomes this pathology in several important situations, and present experiments that elucidate some of its properties.},
   author = {Jordan T. Ash and Ryan P. Adams},
   month = {10},
   title = {On Warm-Starting Neural Network Training},
   url = {http://arxiv.org/abs/1910.08475},
   year = {2019},
}
@article{Dohare2021,
   abstract = {The Backprop algorithm for learning in neural networks utilizes two mechanisms: first, stochastic gradient descent and second, initialization with small random weights, where the latter is essential to the effectiveness of the former. We show that in continual learning setups, Backprop performs well initially, but over time its performance degrades. Stochastic gradient descent alone is insufficient to learn continually; the initial randomness enables only initial learning but not continual learning. To the best of our knowledge, ours is the first result showing this degradation in Backprop's ability to learn. To address this degradation in Backprop's plasticity, we propose an algorithm that continually injects random features alongside gradient descent using a new generate-and-test process. We call this the \textit\{Continual Backprop\} algorithm. We show that, unlike Backprop, Continual Backprop is able to continually adapt in both supervised and reinforcement learning (RL) problems. Continual Backprop has the same computational complexity as Backprop and can be seen as a natural extension of Backprop for continual learning.},
   author = {Shibhansh Dohare and Richard S. Sutton and A. Rupam Mahmood},
   month = {8},
   title = {Continual Backprop: Stochastic Gradient Descent with Persistent Randomness},
   url = {http://arxiv.org/abs/2108.06325},
   year = {2021},
}
@article{,
   abstract = {The acquisition of labels for supervised learning can be expensive. To improve the sample efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations, and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width neural tangent kernels and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or training code. We provide open-source code that includes efficient implementations of all kernels, kernel transformations, and selection methods, and can be used for reproducing our results.},
   author = {David Holzmüller and Viktor Zaverkin and Johannes Kästner and Ingo Steinwart},
   month = {3},
   title = {A Framework and Benchmark for Deep Batch Active Learning for Regression},
   url = {http://arxiv.org/abs/2203.09410},
   year = {2022},
}
@inproceedings{Kurle2020,
   abstract = {This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes. We represent the posterior approximation of the network weights by a diagonal Gaussian distribution and a complementary memory of raw data. This raw data corresponds to likelihood terms that cannot be well approximated by the Gaussian. We introduce a novel method for sequentially updating both components of the posterior approximation. Furthermore, we propose Bayesian forgetting and a Gaussian diffusion process for adapting to non-stationary data. The experimental results show that our update method improves on existing approaches for streaming data. Additionally, the adaptation methods lead to better predictive performance for non-stationary data.},
   author = {Richard Kurle and Botond Cseke and Alexej Klushyn and Patrick Van Der Smagt and Stephan Günnemann},
   journal = {International Conference on Learning Representations},
   title = {Continual Learning with Bayesian Neural Networks for Non-Stationary Data},
   url = {https://openreview.net/forum?id=SJlsFpVtDB},
   year = {2020},
}
@article{Frankle2018,
   abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
   author = {Jonathan Frankle and Michael Carbin},
   month = {3},
   title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
   url = {http://arxiv.org/abs/1803.03635},
   year = {2018},
}
@article{Jaderberg2016,
   abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
   author = {Max Jaderberg and Wojciech Marian Czarnecki and Simon Osindero and Oriol Vinyals and Alex Graves and David Silver and Koray Kavukcuoglu},
   month = {8},
   title = {Decoupled Neural Interfaces using Synthetic Gradients},
   url = {http://arxiv.org/abs/1608.05343},
   year = {2016},
}
@misc{Hanisch2024,
   author = {Robert Hanisch and Debra Kaiser and Alda Yuan and Andrea Medina-Smith and Bonnie Carroll and Eva Campo},
   doi = {10.6028/NIST.SP.1500-18r2},
   institution = {National Institute of Standards and Technology},
   month = {2},
   title = {NIST Research Data Framework (RDaF)},
   url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/1500-18/NIST.SP.1500-18r2.html},
   year = {2024},
}
@misc{Srivastava2014,
   abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
   author = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ruslan Salakhutdinov},
   journal = {Journal of Machine Learning Research},
   keywords = {deep learning,model combination,neural networks,regularization},
   pages = {1929-1958},
   title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
   volume = {15},
   year = {2014},
}
@article{Andrychowicz2016,
   abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
   author = {Marcin Andrychowicz and Misha Denil and Sergio Gomez and Matthew W. Hoffman and David Pfau and Tom Schaul and Brendan Shillingford and Nando de Freitas},
   month = {6},
   title = {Learning to learn by gradient descent by gradient descent},
   url = {http://arxiv.org/abs/1606.04474},
   year = {2016},
}
@article{Le2011,
   abstract = {We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.},
   author = {Quoc V. Le and Marc'Aurelio Ranzato and Rajat Monga and Matthieu Devin and Kai Chen and Greg S. Corrado and Jeff Dean and Andrew Y. Ng},
   month = {12},
   title = {Building high-level features using large scale unsupervised learning},
   url = {http://arxiv.org/abs/1112.6209},
   year = {2011},
}
@article{Radford2015,
   abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
   author = {Alec Radford and Luke Metz and Soumith Chintala},
   month = {11},
   title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
   url = {http://arxiv.org/abs/1511.06434},
   year = {2015},
}
@article{Kingma2013,
   abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
   author = {Diederik P Kingma and Max Welling},
   month = {12},
   title = {Auto-Encoding Variational Bayes},
   url = {http://arxiv.org/abs/1312.6114},
   year = {2013},
}
@article{Oord2016,
   abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
   author = {Aaron van den Oord and Nal Kalchbrenner and Koray Kavukcuoglu},
   month = {1},
   title = {Pixel Recurrent Neural Networks},
   url = {http://arxiv.org/abs/1601.06759},
   year = {2016},
}
@article{Oord2016,
   abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
   author = {Aaron van den Oord and Nal Kalchbrenner and Oriol Vinyals and Lasse Espeholt and Alex Graves and Koray Kavukcuoglu},
   month = {6},
   title = {Conditional Image Generation with PixelCNN Decoders},
   url = {http://arxiv.org/abs/1606.05328},
   year = {2016},
}
@article{Graves2013,
   abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
   author = {Alex Graves},
   month = {8},
   title = {Generating Sequences With Recurrent Neural Networks},
   url = {http://arxiv.org/abs/1308.0850},
   year = {2013},
}
@inproceedings{Wang2005,
   abstract = {This paper introduces Gaussian Process Dynamical Models (GPDM) for nonlinear time series analysis. A GPDM comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian Process (GP) priors for both the dynamics and the observation mappings. This results in a nonparametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach on human motion capture data in which each pose is 62-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces. Webpage: http://www.dgp.toronto.edu/ ∼ jmwang/gpdm/},
   author = {Jack M Wang and David J Fleet and Aaron Hertzmann},
   journal = {Advances in Neural Information Processing Systems},
   title = {Gaussian Process Dynamical Models},
   url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ccd45007df44dd0f12098f486e7e8a0f-Paper.pdf},
   year = {2005},
}
@misc{,
   institution = {Electricity Advisory Committee},
   month = {2},
   title = {Big Data Analytics: Recommendations for the U.S. Department of Energy},
   year = {2021},
}
@article{Watanabe1990,
   abstract = {Based on various approaches, several different learning algorithms have been given in the literature for neural networks. Almost all algorithms have constant learning rates or constant accelerative parameters, though they have been shown to be effective for some practical applications. The learning procedure of neural networks can be regarded as a problem of estimating (or identifying) constant parameters (i.e. connection weights of network) with a nonlinear or linear observation equation. Making use of the Kalman filtering, we derive a new back-propagation algorithm whose learning rate is computed by a time-varying Riccati difference equation. Perceptron-like and correlational learning algorithms are also obtained as special cases. Furthermore, a self-organising algorithm of feature maps is constructed within a similar framework.},
   author = {Keigo Watanabe and Spyros G. Tzafestas},
   journal = {Journal of lntelligent and Robotic Systems},
   keywords = {Kalman filters,Neural nets,learning systems,nonlinear filtering,parameter estimation,pattern recognition},
   pages = {305-319},
   publisher = {Kluwer Academic Publishers},
   title = {Learning Algorithms for Neural Networks with the Kalman Filters},
   volume = {3},
   year = {1990},
}
@article{Iiguni1992,
   author = {Youji Iiguni and Hideaki Sakai and Hidekatsu Tokumaru},
   issue = {4},
   journal = {IEEE Transactions on Signal processing},
   pages = {959-966},
   title = {A Real-Time Learning Algorithm for a Multilayered Neural Network Based on the Extended Kalman Filter},
   volume = {40},
   year = {1992},
}
@inproceedings{Puskorius1991,
   author = {G. V. Puskorius and L. A. Feldkamp},
   journal = {IJCNN-91-Seattle International Joint Conference on Neural Networks},
   pages = {771-777},
   publisher = {IEEE},
   title = {Decoupled Extended Kalman Filter Training of Feedforward Layered Networks},
   year = {1991},
}
@article{Alessandri2007,
   abstract = {The solution of nonlinear least-squares problems is investigated. The asymptotic behavior is studied and conditions for convergence are derived. To deal with such problems in a recursive and efficient way, it is proposed an algorithm that is based on a modified extended Kalman filter (MEKF). The error of the MEKF algorithm is proved to be exponentially bounded. Batch and iterated versions of the algorithm are given, too. As an application, the algorithm is used to optimize the parameters in certain nonlinear input-output mappings. Simulation results on interpolation of real data and prediction of chaotic time series are shown. © 2007 Springer Science+Business Media, LLC.},
   author = {A. Alessandri and M. Cuneo and S. Pagnan and M. Sanguineti},
   doi = {10.1007/s10589-007-9047-7},
   issn = {09266003},
   issue = {2},
   journal = {Computational Optimization and Applications},
   keywords = {Batch algorithms,Extended Kalman filter,Nonlinear least squares,Nonlinear programming,Recursive optimization},
   month = {11},
   pages = {195-216},
   title = {A recursive algorithm for nonlinear least-squares problems},
   volume = {38},
   year = {2007},
}
@inproceedings{Heimes1998,
   abstract = {It is well known that the Extended Kalman Filter (EKF) neural network training algorithm is superior to the standard backpropagation algorithm. However. there are many variations on the EKF implementation that can significantly effect its performance. For example, improper initialization of three parameters cause the algorithm to perform poorly. There are also two advanced methods, de-coupling and multi-streaming, which need to be properly applied based on the specifics of the problem. This paper presents the results of extensive experimentation in applying the EKF training method for recurrent and static neural networks. The goal is to demonstrate how different variations on its implementation effect performance and to find methods to optimize performance. The paper examines the effects of decoupling, multi-streaming, and initial values of constants used by the algorithm. Three new ideas are suggested that can lead to improved performance. These ideas are: initializing parameters to values outside the range previously suggested, a new decoupling strategy, and reducing the update rate of the error covariance matrix for faster training.},
   author = {Felix Heimes},
   journal = {1998 IEEE International Conference on Systems, Man, and Cybernetics},
   pages = {1639-1644},
   title = {Extended Kalman Filter Neural Network Training: Experimental Results and Algorithm Improvements},
   year = {1998},
}
@misc{Feldkamp1998,
   abstract = {We present in this paper a coherent neural network-based framework for solving a variety of difficult signal processing problems. The framework relies on the assertion that time-lagged recurrent networks possess the necessary representational capabilities to act as universal approximators of nonlinear dynamical systems. This property applies to modeling problems posed as system identification , time-series prediction, nonlinear filtering, adaptive filtering, and temporal pattern classification. We address the development of models of nonlinear dynamical systems, in the form of time-lagged recurrent neural networks, which can be used without further training (i.e., as fixed-weight networks). We employ a weight update procedure based on the extended Kalman filter (EKF); as a solution to the recency effect, which is the tendency for a network to forget earlier learning as it processes new examples, we have developed a technique called multistream training. We demonstrate our training framework by applying it to four problems. First, we show that a single time-lagged recurrent neural network can be trained not only to produce excellent one-time-step predictions for two different time series, but also to be robust to severe errors in the provided input sequence. The second problem involves the modeling of a complex system containing significant process noise, which was shown in [1] to lead to unstable trained models. We illustrate how multistream training may be used to enhance the stability of such models. The remaining two problems are drawn from real-world automotive applications. The first of these involves input-output modeling of the dynamic behavior of a catalyst-sensor system which is exposed to an operating engine's exhaust stream. Finally we consider real-time and continuous detection of engine misfire, which is cast as a dynamic pattern classification problem.},
   author = {Lee A Feldkamp and Gintaras V Puskorius},
   keywords = {Automotive diagnostics,Kalman filtering,backpropagation through time,multistream training,recurrent multilayer perceptrons,recurrent networks,stability,system identification,time series prediction},
   title = {A Signal Processing Framework Based on Dynamic Neural Networks with Application to Problems in Adaptation, Filtering, and Classification},
   year = {1998},
}
@misc{Saad1998,
   abstract = {Three networks are compared for low false alarm stock trend predictions. Short-term trends, particularly attractive for neural network analysis, can be used profitably in scenarios such as option trading, but only with significant risk. Therefore, we focus on limiting false alarms, which improves the risk/reward ratio by preventing losses. To predict stock trends, we exploit time delay, recurrent, and probabilistic neural networks (TDNN, RNN, and PNN, respectively), utilizing conjugate gradient and multistream extended Kalman filter training for TDNN and RNN. We also discuss different predictability analysis techniques and perform an analysis of predictability based on a history of daily closing price. Our results indicate that all the networks are feasible, the primary preference being one of convenience.},
   author = {Emad W Saad and Danil V Prokhorov and Donald C Wunsch},
   issue = {6},
   journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS},
   keywords = {Index Terms-Conjugate gradient,extended Kalman filter,fi-nancial engineering,financial forecasting,predictability analysis,probablistic neural network,recurrent neural network,stock market forecasting,time delay neural network,time series anal-ysis,time series prediction,trend prediction},
   title = {Comparative Study of Stock Trend Prediction Using Time Delay, Recurrent and Probabilistic Neural Networks},
   volume = {9},
   year = {1998},
}
@inproceedings{Feldkamp1994,
   abstract = {"IEEE catalog number 94CH3429-8"--Title page verso. v. 1. pages 1-605 --v. 2. pages 606-1294 --v. 3. pages 1295-2003 --v. 4. pages 2004-2714 --v. 5. 2415-3436 --v. 6. pages 3437-4179 --v. 7. pages 4178-4777.},
   author = {L. A. Feldkamp and G. V. Puskorius},
   isbn = {078031901X},
   journal = {Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)},
   pages = {2377-2382},
   publisher = {IEEE Neural Networks Council},
   title = {Training Controllers for Robustness: Multi-Stream DEKF},
   year = {1994},
}
@misc{Puskorius1994,
   abstract = {A6drad-Controllerr based on neural networks and h s y logic are often said to be robust , i.e., to continue to axhibit ratisfixtory performance even in the hce of changea in the rystem for which they have been trained or derigned. Our own experience, largely but not entirely baaed on model-bared simulation studier, ruggeats that thh statement carrier a considerable degree of truth. In particular, we have found that the performance of dynamic control structures in the form of recurrent neural networh tends to degrade gram f d l y (U the system ia altered from that used during training. W e have observed, however, that the degree of mburtnesr doer not always correlate poritively with the quality of control on the plant uaed in training. For example, two neural controllerr of identical architecture and trained on the name plant to comparable error may, on an independent teat, exhibit rig-nificantly different de-of mbustneer. Thin haa led u s to d d e a procedure, called multi-stream training, that mrlrru roburtness an explicit goal of training. In the prdarred form of thin procedure, we train 8 ringle eontroller ri-multaneously on l~lveral plantr relected from the expected range, thereby countering the tendency of a aerial training procear to favor the plant upon which training haa moet recently been conducted. This paper describer the multi-rtrsam method and illurtratea its use and efficacy with an example.},
   author = {G V Puskorius},
   title = {Tkaining of Robust Neural Controllers},
   year = {1994},
}
@inproceedings{Puskorius1997,
   abstract = {We discuss in this paper a powerful and enabling extension to the class of second order neural network training methods based on the extended Kalman filter (EKF). EKF training procedures are generally considered to be recursive or sequential in nature, where weight updates are performed on an instance-by-instance basis. On the other hand, other second order weight update procedures, such as conjugate gradient and quasi-Newton methods, are typically based on batch processing of training patterns. The multi-stream EKF weight update procedure combines the useful stochastic and sequential characteristics of the base EKF method with a mechanism that allows for multiple instances to be processed simultaneously in a manner that is consistent with the EKF framework , thereby resulting in an effective semi-batch, second order training method. We motivate the use of multi-stream EKF training by relating Kalman training of a single linear node to a batch least squares solution. The paper concludes with a simulation example of the application of multi-stream EKF training .},
   author = {G V Puskorius and L A Feldkamp},
   journal = {1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation},
   pages = {2006-2011},
   publisher = {IEEE},
   title = {Multi-Stream Extended Kalman Filter Training for Static and Dynamic Neural Networks},
   year = {1997},
}
@inproceedings{Hu2004,
   abstract = {This paper describes the use of a multi-stream Extended Kalman Filter (EKF) to tackle the IJCNN 2004 challenge problem-Time Series Prediction on CATS benchmark, A weighted bidirectional approach was adapted in the experiments to incorporate the forward and backward predictions of the time series.},
   author = {Xiao Hu and Donald C. II Wunsch},
   title = {Time Series Prediction with a Weighted Bidirectional Multi-stream Extended Kalman Filter},
   year = {2004},
}
@article{Agarwal2023,
   abstract = {This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.},
   author = {Naman Agarwal and Daniel Suo and Xinyi Chen and Elad Hazan},
   month = {12},
   title = {Spectral State Space Models},
   url = {http://arxiv.org/abs/2312.06837},
   year = {2023},
}
@inproceedings{,
   author = {Olov Rosén and Alexander Medvedev},
   doi = {10.1109/CDC.2011.6160363},
   journal = {2011 50th IEEE Conference on Decision and Control and European Control Conference},
   month = {12},
   title = {Efficient Parallel Implementation of a Kalman Filter for Single Output Systems on Multicore Computational Platforms},
   year = {2011},
}
@article{Davtyan2021,
   abstract = {Optimization is often cast as a deterministic problem, where the solution is found through some iterative procedure such as gradient descent. However, when training neural networks the loss function changes over (iteration) time due to the randomized selection of a subset of the samples. This randomization turns the optimization problem into a stochastic one. We propose to consider the loss as a noisy observation with respect to some reference optimum. This interpretation of the loss allows us to adopt Kalman filtering as an optimizer, as its recursive formulation is designed to estimate unknown parameters from noisy measurements. Moreover, we show that the Kalman Filter dynamical model for the evolution of the unknown parameters can be used to capture the gradient dynamics of advanced methods such as Momentum and Adam. We call this stochastic optimization method KOALA, which is short for Kalman Optimization Algorithm with Loss Adaptivity. KOALA is an easy to implement, scalable, and efficient method to train neural networks. We provide convergence analysis and show experimentally that it yields parameter estimates that are on par with or better than existing state of the art optimization algorithms across several neural network architectures and machine learning tasks, such as computer vision and language modeling.},
   author = {Aram Davtyan and Sepehr Sameni and Llukman Cerkezi and Givi Meishvilli and Adam Bielski and Paolo Favaro},
   month = {7},
   title = {KOALA: A Kalman Optimization Algorithm with Loss Adaptivity},
   url = {http://arxiv.org/abs/2107.03331},
   year = {2021},
}
@misc{Kuntz2023,
   abstract = {Multivariate linear regression is a classic statistical method that has been used in a wide array of scientific and engineering fields, in some for over two centuries. While the maximum likelihood estimation problem is well-solved in the case of nonsingular data and error covari-ance matrices, the nonsingular case is less well understood, especially the singular error covariance case. The purpose of this report is to define and derive the maximum likelihood of the singular multivari-ate regression model, under no assumptions about the rank of the underlying data or parameters. We show that a na¨ıvena¨ıve definition of the estimator has no solutions, almost surely, but it can be rigorously defined so that solutions exist and coincide with the nonsingular case. Illustrative examples of the technical results are included throughout, and applied examples in system identification are included after the technical results.},
   author = {Steven J Kuntz and James B Rawlings},
   institution = {Texas Wisconsin California Control Consortium},
   month = {12},
   title = {On multivariate linear regression with singular error covariances},
   year = {2023},
}
@misc{Kuntz2023,
   abstract = {Linear Gaussian estimation, i.e., estimation of β (or a linear function of β) in the model y = Xβ + e where e ∼ N(0, V), is a classic and ubiquitous problem in statistics. Linear Gaussian estimation under the most restrictive assumptions (X full column rank, V = σ 2 I) dates back to the late 18th century. Estimates without assumptions on the rank of X or V were stated in closed-form in the early 1970s. Recently, linear estimation has taken many new and non-Gaussian formulations with the popularity of Bayesian regression priors (e.g., Tikhonov reg-ularization, ridge and LASSO regression, sparse modeling). Given its distinguished history and prominent role in the fields of statistics, optimization, and optimal estimation and control, results on linear estimation (and least squares) are extensive and widely scattered in the literature, often with strikingly different but nevertheless equivalent closed-form solutions appearing in different fields. This review is intended to serve as a self-contained and compact resource for these many definitions and closed-form solutions of esti-mators of the linear model. We survey a wide variety of estimator definitions , including ordinary/generalized least squares estimators, maximum likelihood estimators (MLEs), maximum a posteriori (MAP) estimators, and best linear unbiased estimators (BLUEs), and derive closed-form solutions to these estimation problems. While solutions to the BLUE problem are available in the literature under our assumptions , we know of no other literature that has closed-form solutions of the MLE and MAP problems under these assumptions. Despite the breadth of estimator definitions available, we show that all of these can be formulated as an equivalent equality constrained generalized least squares (ECGLS) problem, i.e., minimization of a quadratic objective subject to linear equality constraints. Moreover, we show that the estimator of a perturbed linear model with nonsingular variance (y = Xβ + e where e ∼ N(0, V + ρI)) is a stable approximation of the estimator with singular variance. In this review, we also discuss many of the applications (semidefinite Kalman filtering and optimal control, saddle point systems, linear inverse problems) and extensions (nonlinear regression and inverse problems, Bayesian regression, sparse modeling) of the classical and generalized linear estimation problem, and show how they arise from specific solution methods in linear estimation .},
   author = {Steven J Kuntz and James B Rawlings},
   institution = {Texas-Wisconsin-California Control Consortium},
   month = {12},
   title = {On the unified theory of linear Gaussian estimation: solution methods, applications, and extensions},
   year = {2023},
}
@article{Kuntz2023,
   abstract = {For three decades, model predictive control (MPC) has been the flagship advanced control method in the chemical process industries. However, most implementations still use heuristic methods for designing MPC estimators, especially for offset-free MPC implementations. In this paper, we present a recently developed maximum likelihood-based method for the identification of linear augmented disturbance models for use in offset-free MPC. This method provides noise covariances that are used to derive Kalman filters and moving horizon estimators, forgoing the need for manual design and tuning of the estimator. The method is extended to handle closed-loop plant data. The proposed identification method and estimator design are evaluated in industrial-scale, real-world case study of a process at Eastman Chemical's Kingsport plant. Using this identified model, we reduced the mean stage cost by 38% compared to the performance of the existing, hand-tuned MPC model.},
   author = {Steven J. Kuntz and James J. Downs and Stephen M. Miller and James B. Rawlings},
   doi = {10.1016/j.compchemeng.2023.108429},
   issn = {00981354},
   journal = {Computers and Chemical Engineering},
   keywords = {Closed-loop identification,Disturbance identification,Identification for control,Model predictive control,Subspace identification,System identification},
   month = {11},
   publisher = {Elsevier Ltd},
   title = {An industrial case study on the combined identification and offset-free control of a chemical process},
   volume = {179},
   year = {2023},
}
@inproceedings{Kuntz2022,
   abstract = {Literaturangaben.},
   author = {Steven J. Kuntz and James B. Rawlings},
   city = {Atlanta, USA},
   isbn = {9781665451963},
   journal = {2022 American Control Conference (ACC)},
   month = {6},
   pages = {3961-3966},
   publisher = {IEEE},
   title = {Maximum Likelihood Estimation of Linear Disturbance Models for Offset-free Model Predictive Control},
   year = {2022},
}
@misc{Lecun2015,
   abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
   author = {Yann Lecun and Yoshua Bengio and Geoffrey Hinton},
   doi = {10.1038/nature14539},
   issn = {14764687},
   issue = {7553},
   journal = {Nature},
   month = {5},
   pages = {436-444},
   pmid = {26017442},
   publisher = {Nature Publishing Group},
   title = {Deep learning},
   volume = {521},
   year = {2015},
}
@article{Simonyan2014,
   abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
   author = {Karen Simonyan and Andrew Zisserman},
   month = {9},
   title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
   url = {http://arxiv.org/abs/1409.1556},
   year = {2014},
}
@article{Hinton2006,
   abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
   author = {Geoffrey E Hinton and Simon Osindero and Yee-Whye Teh},
   issue = {7},
   journal = {Neural computation},
   pages = {1527-1554},
   title = {A Fast Learning Algorithm for Deep Belief Nets},
   volume = {18},
   year = {2006},
}
@inproceedings{Krizhevsky2012,
   abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
   author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E Hinton},
   journal = {Advances in neural information processing systems},
   title = {ImageNet Classification with Deep Convolutional Neural Networks},
   url = {http://code.google.com/p/cuda-convnet/},
   year = {2012},
}
@article{He2015,
   abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
   author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
   month = {12},
   title = {Deep Residual Learning for Image Recognition},
   url = {http://arxiv.org/abs/1512.03385},
   year = {2015},
}
@article{Hinton2012,
   abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
   author = {Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
   month = {7},
   title = {Improving neural networks by preventing co-adaptation of feature detectors},
   url = {http://arxiv.org/abs/1207.0580},
   year = {2012},
}
@inproceedings{Szegedy2015,
   abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
   author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
   journal = {conference on computer vision and pattern recognition},
   title = {Going Deeper with Convolutions},
   year = {2015},
}
@article{Ioffe2015,
   abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
   author = {Sergey Ioffe and Christian Szegedy},
   month = {2},
   title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
   url = {http://arxiv.org/abs/1502.03167},
   year = {2015},
}
@article{Ba2016,
   abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
   author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
   month = {7},
   title = {Layer Normalization},
   url = {http://arxiv.org/abs/1607.06450},
   year = {2016},
}
@article{Chen2015,
   abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
   author = {Tianqi Chen and Ian Goodfellow and Jonathon Shlens},
   month = {11},
   title = {Net2Net: Accelerating Learning via Knowledge Transfer},
   url = {http://arxiv.org/abs/1511.05641},
   year = {2015},
}
@article{Wei2016,
   abstract = {We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as \emph\{network morphism\} in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
   author = {Tao Wei and Changhu Wang and Yong Rui and Chang Wen Chen},
   month = {3},
   title = {Network Morphism},
   url = {http://arxiv.org/abs/1603.01670},
   year = {2016},
}
@article{Hubara2016,
   abstract = {We introduce a method to train Binarized Neural Networks (BNNs)-neural networks with binary weights and activations at run-time. At train-time the binary weights and activations are used for computing the parameter gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
   author = {Itay Hubara and Matthieu Courbariaux and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
   issue = {4},
   journal = {Advances in neural information processing systems},
   title = {Binarized neural networks},
   volume = {29},
   url = {https://github.com/itayhubara/BinaryNet},
   year = {2016},
}
@misc{Sutskever2013,
   abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter , it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initial-izations have likely failed due to poor ini-tialization schemes. Furthermore, carefully tuned momentum methods suce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
   author = {Ilya Sutskever and James Martens and George Dahl and Geo↵rey Hinton},
   title = {On the importance of initialization and momentum in deep learning},
   year = {2013},
}
@article{Han2015,
   abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
   author = {Song Han and Huizi Mao and William J. Dally},
   month = {10},
   title = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
   url = {http://arxiv.org/abs/1510.00149},
   year = {2015},
}
@article{Iandola2016,
   abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
   author = {Forrest N. Iandola and Song Han and Matthew W. Moskewicz and Khalid Ashraf and William J. Dally and Kurt Keutzer},
   month = {2},
   title = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size},
   url = {http://arxiv.org/abs/1602.07360},
   year = {2016},
}
@inproceedings{Glorot2010,
   abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training , with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
   author = {Xavier Glorot and Yoshua Bengio},
   journal = {Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings},
   title = {Understanding the difficulty of training deep feedforward neural networks},
   url = {http://www.iro.umontreal.},
   year = {2010},
}
@inproceedings{Gregor2015,
   abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
   author = {Karol Gregor and Danihelka@google Com and Danilo Jimenez Rezende and Daan Wierstra},
   journal = {International conference on machine learning (PMLR)},
   pages = {1462-1471},
   title = {DRAW: A Recurrent Neural Network For Image Generation},
   url = {https://www.youtube.},
   year = {2015},
}
@inproceedings{Goodfellow2014,
   abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
   author = {Ian J Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
   journal = {Advances in neural information processing systems},
   title = {Generative Adversarial Nets},
   url = {http://www.github.com/goodfeli/adversarial},
   year = {2014},
}
@article{Mehri2016,
   abstract = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
   author = {Soroush Mehri and Kundan Kumar and Ishaan Gulrajani and Rithesh Kumar and Shubham Jain and Jose Sotelo and Aaron Courville and Yoshua Bengio},
   month = {12},
   title = {SampleRNN: An Unconditional End-to-End Neural Audio Generation Model},
   url = {http://arxiv.org/abs/1612.07837},
   year = {2016},
}
@article{Cho2014,
   abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
   author = {Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
   month = {6},
   title = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
   url = {http://arxiv.org/abs/1406.1078},
   year = {2014},
}
@article{Sutskever2014,
   abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
   author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
   month = {9},
   title = {Sequence to Sequence Learning with Neural Networks},
   url = {http://arxiv.org/abs/1409.3215},
   year = {2014},
}
@article{Bahdanau2014,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
   month = {9},
   title = {Neural Machine Translation by Jointly Learning to Align and Translate},
   url = {http://arxiv.org/abs/1409.0473},
   year = {2014},
}
@article{Graves2014,
   abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
   author = {Alex Graves and Greg Wayne and Ivo Danihelka},
   month = {10},
   title = {Neural Turing Machines},
   url = {http://arxiv.org/abs/1410.5401},
   year = {2014},
}
@article{Vinyals2015,
   abstract = {Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.},
   author = {Oriol Vinyals and Quoc Le},
   month = {6},
   title = {A Neural Conversational Model},
   url = {http://arxiv.org/abs/1506.05869},
   year = {2015},
}
@article{Weston2014,
   abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
   author = {Jason Weston and Sumit Chopra and Antoine Bordes},
   month = {10},
   title = {Memory Networks},
   url = {http://arxiv.org/abs/1410.3916},
   year = {2014},
}
@article{Hinton2015,
   abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
   author = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
   month = {3},
   title = {Distilling the Knowledge in a Neural Network},
   url = {http://arxiv.org/abs/1503.02531},
   year = {2015},
}
@misc{Bengio2012,
   abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P (x) is structurally related to some task of interest, say predicting P (y|x). This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.},
   author = {Yoshua Bengio},
   keywords = {Auto-encoders,Deep Learning,Re-stricted Boltzmann Machines,domain adaptation,multi-task learning,neural networks,representation learning,self-taught learning,transfer learn-ing,unsupervised learning},
   pages = {17-37},
   title = {Deep Learning of Representations for Unsupervised and Transfer Learning},
   volume = {27},
   url = {http://www.causality.inf.ethz.ch/unsupervised-learning.php},
   year = {2012},
}
@article{Rusu2015,
   abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
   author = {Andrei A. Rusu and Sergio Gomez Colmenarejo and Caglar Gulcehre and Guillaume Desjardins and James Kirkpatrick and Razvan Pascanu and Volodymyr Mnih and Koray Kavukcuoglu and Raia Hadsell},
   month = {11},
   title = {Policy Distillation},
   url = {http://arxiv.org/abs/1511.06295},
   year = {2015},
}
@article{Parisotto2015,
   abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed "Actor-Mimic", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
   author = {Emilio Parisotto and Jimmy Lei Ba and Ruslan Salakhutdinov},
   month = {11},
   title = {Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning},
   url = {http://arxiv.org/abs/1511.06342},
   year = {2015},
}
@article{Nouri2023,
   abstract = {The interest in shortest-term solar irradiance forecasts (nowcasts) increases steadily with the increase share of distributed solar power generation. Such solar irradiance nowcasts are beneficial for different stakeholders, from generation to transmission and demand, in order to tackle challenges caused by the variability of solar irradiance. In space and time highly resolved nowcasts can be obtained by all sky imager (ASI) systems, which analyze the sky conditions from sky images. Deterministic nowcasts from ASI systems are subject to significant uncertainties. Reliable uncertainty information are very helpful for any application, in order to derive practical benefit from nowcasts. Therefore, such nowcasts should be probabilistic in nature, which provide probability distributions. Meaningful indicators for the uncertainties at hand are provided by prediction intervals for distinct confidence levels derived from the probability distributions. Thus, a real time capable nonparametric probabilistic quantile nowcasting method based on deterministic ASI nowcast is developed. The method takes irradiance variabilities as main predictor of nowcast uncertainties into account. A benchmark against three distinct baseline models is conducted over an extensive data set, using a variety of recently recommended scores. Overall average continuous ranked probability skill scores (Clear-Sky Dependent Climatology as baseline) for nowcasts up to 20 min ahead of 0.72 ± 0.08 (direct normal irradiance) and 0.62 ± 0.09 (global horizontal irradiance) are reached. For a better evaluation of the actual performance of the probabilistic nowcasts, a discretization of the validation data set into eight irradiance variability conditions is performed. All scores are determined for each of these distinct conditions.},
   author = {Bijan Nouri and Stefan Wilbert and Niklas Blum and Yann Fabel and Elke Lorenz and Annette Hammer and Thomas Schmidt and Luis F. Zarzalejo and Robert Pitz-Paal},
   doi = {10.1016/j.solener.2023.01.060},
   issn = {0038092X},
   journal = {Solar Energy},
   keywords = {All sky imager,Probabilistic nowcasts,Quantile forecast,Solar irradiance},
   month = {3},
   pages = {285-307},
   publisher = {Elsevier Ltd},
   title = {Probabilistic solar nowcasting based on all-sky imagers},
   volume = {253},
   year = {2023},
}
@article{Rusu2016,
   abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
   author = {Andrei A. Rusu and Neil C. Rabinowitz and Guillaume Desjardins and Hubert Soyer and James Kirkpatrick and Koray Kavukcuoglu and Razvan Pascanu and Raia Hadsell},
   month = {6},
   title = {Progressive Neural Networks},
   url = {http://arxiv.org/abs/1606.04671},
   year = {2016},
}
@misc{Krishnan2023,
   abstract = {The demand for energy generation from solar energy resource has been exponentially increasing in recent years. It is integral for a grid operator to maintain the balance between the demand and supply of the grid. Solar radiation forecasting paves the way for proper planning, reserve management, and elude penalty since solar energy is sporadic in nature. Several methods can forecast solar radiation; the prior classifications are machine learning models, numerical weather prediction models, satellite imaging, sky imager and hybrid model. This article presents a comprehensive review of all those models with the working principle, challenges and future research direction. Sky imagers provide the Normalized Root Mean Square Error (nRMSE) value of 6%–9% for a time horizon of 30 min, and the satellite imagery technique provides the Root Mean Square Error (RMSE) value of 61.28 W/m2 – 346.05 W/m2 for a time horizon of 4 h ahead. Similarly, NWP mesoscale models provide the RMSE value of 411.6 W/m2 - for three days ahead of forecasting with a spatial resolution of 50 km. Machine learning models are good at delivering accurate results with the time horizon up to 1 day ahead by yielding the results of RMSE in the range of 0.1170 W/m2 – 93.04 W/m2. Deep learning and hybrid models are being developed to overcome the issues faced by the standalone techniques. In many research works, artificial intelligence techniques are integrated with NWP models, sky imagers and satellite imagers to improve the data handling algorithm, which implicitly results in forecasting accuracy.},
   author = {Naveen Krishnan and K. Ravi Kumar and Chandrapal Singh Inda},
   doi = {10.1016/j.jclepro.2023.135860},
   issn = {09596526},
   journal = {Journal of Cleaner Production},
   keywords = {Evaluation metrics,Solar radiation,Spatial resolution,Temporal resolution,Time horizon},
   month = {2},
   publisher = {Elsevier Ltd},
   title = {How solar radiation forecasting impacts the utilization of solar energy: A critical review},
   volume = {388},
   year = {2023},
}
@misc{Assaf2023,
   abstract = {The accuracy of solar energy forecasting is critical for power system planning, management, and operation in the global electric energy grid. Therefore, it is crucial to ensure a constant and sustainable power supply to consumers. However, existing statistical and machine learning algorithms are not reliable for forecasting due to the sporadic nature of solar energy data. Several factors influence the performance of solar irradiance, such as forecasting horizon, weather classification, and performance evaluation metrics. Therefore, we provide a review paper on deep learning-based solar irradiance forecasting models. These models include Long Short-Term Memory (LTSM), Gated Recurrent Unit (GRU), Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), Generative Adversarial Networks (GAN), Attention Mechanism (AM), and other existing hybrid models. Based on our analysis, deep learning models perform better than conventional models in solar forecasting applications, especially in combination with some techniques that enhance the extraction of features. Furthermore, the use of data augmentation techniques to improve deep learning performance is useful, especially for deep networks. Thus, this paper is expected to provide a baseline analysis for future researchers to select the most appropriate approaches for photovoltaic power forecasting, wind power forecasting, and electricity consumption forecasting in the medium term and long term.},
   author = {Abbas Mohammed Assaf and Habibollah Haron and Haza Nuzly Abdull Hamed and Fuad A. Ghaleb and Sultan Noman Qasem and Abdullah M. Albarrak},
   doi = {10.3390/app13148332},
   issn = {20763417},
   issue = {14},
   journal = {Applied Sciences (Switzerland)},
   keywords = {Attention Mechanism,Convolutional Neural Network,Generative Adversarial Network,Long Short-Term Memory,deep learning,hybrid model,solar irradiance forecasting},
   month = {7},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {A Review on Neural Network Based Models for Short Term Solar Irradiance Forecasting},
   volume = {13},
   year = {2023},
}
@misc{,
   abstract = {The intermittency of renewable energy sources, such as wind and solar, means that they require reliable and accurate forecasts to integrate properly into energy systems. This review introduces and examines a selection of state-of-the-art methods that are applied for multivariate forecasting of wind and solar power production. Methods such as conditional parametric and combined forecasting already see wide use in practice, both commercially and scientifically. In the context of multivariate forecasting, it is vital to model the dependence between forecasts correctly. In recent years, reconciliation of forecasts to ensure coherency across spatial and temporal aggregation levels has shown great promise in increasing the accuracy of renewable energy forecasts. We introduce the methodology used for forecast reconciliation and review some recent applications for wind and solar power forecasting. Many forecasters are beginning to see the benefit of the greater information provided by probabilistic forecasts. We highlight stochastic differential equations as a method for probabilistic forecasting, which can also model the dependence structure. Lastly, we discuss forecast evaluation and how choosing a proper approach to evaluation is vital to avoid misrepresenting forecasts. This article is categorized under: Climate and Environment > Net Zero Planning and Decarbonization Sustainable Energy > Solar Energy Sustainable Energy > Wind Energy.},
   author = {Mikkel L. Sørensen and Peter Nystrup and Mathias B. Bjerregård and Jan K. Møller and Peder Bacher and Henrik Madsen},
   doi = {10.1002/wene.465},
   issn = {2041840X},
   issue = {2},
   journal = {Wiley Interdisciplinary Reviews: Energy and Environment},
   keywords = {forecast evaluation,forecast reconciliation,solar power forecasting,stochastic differential equations,wind power forecasting},
   month = {3},
   publisher = {John Wiley and Sons Ltd},
   title = {Recent developments in multivariate wind and solar power forecasting},
   volume = {12},
   year = {2023},
}
@article{,
   abstract = {The increasing penetration of solar energy leaves power grids vulnerable to fluctuations in the solar radiation that reaches the surface of the Earth due to the projection of cloud shadows. Therefore, an intra-hour solar forecasting algorithm is necessary to reduce power instabilities caused by the impact of moving clouds on energy generation. The most accurate intra-hour solar forecasting methods apply convolutional neural networks to a series of visible light sky images. Instead, this investigation uses data acquired by a novel infrared sky imager on a solar tracker, which is capable of maintaining the Sun in the center of the images throughout the day and, at the same time, reducing the scattering effect produced by the Sun's direct radiation. In addition, infrared sky images allow the derivation and extraction of physical cloud features. The cloud dynamics are analyzed in sequences of images to compute the probability of the Sun intercepting air parcels in the sky images (i.e., voxels). The method introduced in this investigation fuses sky condition information from multiple sensors (i.e., pyranometer, sky imager, solar tracker, weather station) and feature sources using a multi-task deep learning architecture based on recurrent neural networks. The proposed deterministic and Bayesian architectures reduce computation time by avoiding convolutional filters. The proposed intra-hour solar forecasting algorithm reached a forecast skill of 18.6% with a forecasting horizon of 8 min. Consequently, the proposed intra-hour solar forecasting method can potentially reduce the operational costs of power grids with high participation of solar energy.},
   author = {Guillermo Terrén-Serrano and Manel Martínez-Ramón},
   doi = {10.1016/j.inffus.2023.02.006},
   issn = {15662535},
   journal = {Information Fusion},
   keywords = {Bayesian networks,Bayesian optimization,Deep learning,Girasol dataset,Sky imaging,Solar forecasting},
   month = {7},
   pages = {42-61},
   publisher = {Elsevier B.V.},
   title = {Deep learning for intra-hour solar forecasting with fusion of features extracted from infrared sky images},
   volume = {95},
   year = {2023},
}
@article{Logothetis2022,
   abstract = {Solar forecasting constitutes a critical tool for operating, producing and storing generated power from solar farms. In the framework of the International Energy Agency’s Photovoltaic Power Systems Program Task 16, the solar irradiance nowcast algorithms, based on five all-sky imagers (ASIs), are used to investigate the feasibility of ASIs to foresee ramp events. ASIs 1–2 and ASIs 3–5 can capture the true ramp events by 26.0–51.0% and 49.0–92.0% of the cases, respectively. ASIs 1–2 provided the lowest (<10.0%) falsely documented ramp events while ASIs 3–5 recorded false ramp events up to 85.0%. On the other hand, ASIs 3–5 revealed the lowest falsely documented no ramp events (8.0–51.0%). ASIs 1–2 are developed to provide spatial solar irradiance forecasts and have been delimited only to a small area for the purposes of this benchmark, which penalizes these approaches. These findings show that ASI-based nowcasts could be considered as a valuable tool for predicting solar irradiance ramp events for a variety of solar energy technologies. The combination of physical and deep learning-based methods is identified as a potential approach to further improve the ramp event forecasts.},
   author = {Stavros Andreas Logothetis and Vasileios Salamalikis and Bijan Nouri and Jan Remund and Luis F. Zarzalejo and Yu Xie and Stefan Wilbert and Evangelos Ntavelis and Julien Nou and Niels Hendrikx and Lennard Visser and Manajit Sengupta and Mário Pó and Remi Chauvin and Stephane Grieu and Niklas Blum and Wilfried van Sark and Andreas Kazantzidis},
   doi = {10.3390/en15176191},
   issn = {19961073},
   issue = {17},
   journal = {Energies},
   keywords = {all-sky imagers,forecasting,ramp events,solar irradiance ramp event forecasting},
   month = {9},
   publisher = {MDPI},
   title = {Solar Irradiance Ramp Forecasting Based on All-Sky Imagers},
   volume = {15},
   year = {2022},
}
@article{Logothetis2022,
   abstract = {Fluctuations of the incoming solar irradiance impact the power generation from photovoltaic and concentrating solar thermal power plants. Accurate solar nowcasting becomes necessary to detect these sudden changes of generated power and to provide the desired information for optimal exploitation of solar systems. In the framework of the International Energy Agency's Photovoltaic Power Systems Program Task 16, a benchmarking exercise has been conducted relying on a bouquet of solar nowcasting methodologies by all-sky imagers (ASI). In this paper, four ASI systems nowcast the Global Horizontal Irradiance (GHI) with a time forecast ranging from 1 to 20 min during 28 days with variable cloud conditions spanning from September to November 2019 in southern Spain. All ASIs demonstrated their ability to accurately nowcast GHI, with RMSE ranging from 6.9% to 18.1%. Under cloudy conditions, all ASIs' nowcasts outperform the persistence models. Under clear skies, three ASIs are better than persistence. Discrepancies in the observed nowcasting performance become larger at increasing forecast horizons. The findings of this study highlight the feasibility of ASIs to reliably nowcast GHI at different sky conditions, time intervals and horizons. Such nowcasts can be used either to estimate solar power at distant times or detect sudden GHI fluctuations.},
   author = {Stavros Andreas Logothetis and Vasileios Salamalikis and Stefan Wilbert and Jan Remund and Luis F. Zarzalejo and Yu Xie and Bijan Nouri and Evangelos Ntavelis and Julien Nou and Niels Hendrikx and Lennard Visser and Manajit Sengupta and Mário Pó and Remi Chauvin and Stephane Grieu and Niklas Blum and Wilfried van Sark and Andreas Kazantzidis},
   doi = {10.1016/j.renene.2022.08.127},
   issn = {18790682},
   journal = {Renewable Energy},
   keywords = {All-sky imagers,Benchmarking,Forecasting,Solar irradiance nowcasting},
   month = {11},
   pages = {246-261},
   publisher = {Elsevier Ltd},
   title = {Benchmarking of solar irradiance nowcast performance derived from all-sky imagers},
   volume = {199},
   year = {2022},
}
@misc{Yang2021,
   abstract = {Forecasts are always wrong, otherwise, they are merely deterministic calculations. Besides leveraging advanced forecasting methods, post-processing has become a standard practice for solar forecasters to improve the initial forecasts. In this review, the post-processing task is divided into four categories: (1) deterministic-to-deterministic (D2D) post-processing, (2) probabilistic-to-deterministic (P2D) post-processing, (3) deterministic-to-probabilistic (D2P) post-processing, and (4) probabilistic-to-probabilistic (P2P) post-processing. Additionally, a total of ten overarching thinking tools, namely, (1) regression (D2D), (2) filtering (D2D), (3) resolution change (D2D), (4) summarizing predictive distribution (P2D), (5) combining deterministic forecasts (P2D), (6) analog ensemble (D2P), (7) method of dressing (D2P), (8) probabilistic regression (D2P), (9) calibrating ensemble forecasts (P2P), and (10) combining probabilistic forecasts (P2P), are proposed. These thinking tools can be thought of as the “style” or “mechanism” of post-processing. In that, the utilization of thinking tools circumvents the common pitfalls of classifying the literature by methods (e.g., statistics, machine-learning, or numerical weather prediction), which often leads to a “who used what method” type of roster review that is clearly ineffective, non-exhaustive, and dull. When myriads of post-processing methods are mapped to countable few thinking tools, it allows solar forecasters to enumerate the styles of adjustment that could be performed on a set of initial forecasts, which makes a post-processing task clearly goal-driven. Besides the thinking tools, this paper also emphasizes on the value of post-processing, and provides an outlook for future research. Although this paper is revolved around solar, the materials herein discussed can also be applied to wind and other forecasting areas.},
   author = {Dazhi Yang and Dennis van der Meer},
   doi = {10.1016/j.rser.2021.110735},
   issn = {18790690},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {Post-processing,Probabilistic forecasting,Review,Solar forecasting},
   month = {4},
   publisher = {Elsevier Ltd},
   title = {Post-processing in solar forecasting: Ten overarching thinking tools},
   volume = {140},
   year = {2021},
}
@article{Blum2022,
   abstract = {All-sky imagers (ASIs) can be used to model clouds and detect spatial variations of cloud attenuation. Such cloud modeling can support ASI-based nowcasting, upscaling of photovoltaic production and numeric weather predictions. A novel procedure is developed which uses a network of ASIs to model clouds and determine cloud attenuation more accurately over every location in the observed area, at a resolution of 50 m × 50 m. The approach combines images from neighboring ASIs which monitor the cloud scene from different perspectives. Areas covered by optically thick/intermediate/thin clouds are detected in the images of twelve ASIs and are transformed into maps of attenuation index. In areas monitored by multiple ASIs, an accuracy-weighted average combines the maps of attenuation index. An ASI observation’s local weight is calculated from its expected accuracy. Based on radiometer measurements, a probabilistic procedure derives a map of cloud attenuation from the combined map of attenuation index. Using two additional radiometers located 3.8 km west and south of the first radiometer, the ASI network’s estimations of direct normal (DNI) and global horizontal irradiance (GHI) are validated and benchmarked against estimations from an ASI pair and homogeneous persistence which uses a radiometer alone. The validation works without forecasted data, this way excluding sources of error which would be present in forecasting. The ASI network reduces errors notably (RMSD for DNI 136 W/m (Formula presented.), GHI 98 W/m (Formula presented.)) compared to the ASI pair (RMSD for DNI 173 W/m (Formula presented.), GHI 119 W/m (Formula presented.) and radiometer alone (RMSD for DNI 213 W/m (Formula presented.)), GHI 140 W/m (Formula presented.)). A notable reduction is found in all studied conditions, classified by irradiance variability. Thus, the ASI network detects spatial variations of cloud attenuation considerably more accurately than the state-of-the-art approaches in all atmospheric conditions.},
   author = {Niklas Benedikt Blum and Stefan Wilbert and Bijan Nouri and Jonas Stührenberg and Jorge Enrique Lezaca Galeano and Thomas Schmidt and Detlev Heinemann and Thomas Vogt and Andreas Kazantzidis and Robert Pitz-Paal},
   doi = {10.3390/rs14225685},
   issn = {20724292},
   issue = {22},
   journal = {Remote Sensing},
   keywords = {cloud camera,cloud modeling,cloud transmittance,forecast,nowcast,solar irradiance},
   month = {11},
   publisher = {MDPI},
   title = {Analyzing Spatial Variations of Cloud Attenuation by a Network of All-Sky Imagers},
   volume = {14},
   year = {2022},
}
@book{Yang2024,
   author = {Dazhi Yang and Jan Kleissl},
   city = {Boca Raton, FL},
   keywords = {Irradiance,Photovoltaic,Power,Solar},
   publisher = {CRC Press},
   title = {Solar Irradiance and Photovoltaic Power Forecasting},
   year = {2024},
}
@misc{Yang2022,
   abstract = {The ability to forecast solar irradiance plays an indispensable role in solar power forecasting, which constitutes an essential step in planning and operating power systems under high penetration of solar power generation. Since solar radiation is an atmospheric process, solar irradiance forecasting, and thus solar power forecasting, can benefit from the participation of atmospheric scientists. In this review, the two fields, namely, atmospheric science and power system engineering are jointly discussed with respect to how solar forecasting plays a part. Firstly, the state of affairs in solar forecasting is elaborated; some common misconceptions are clarified; and salient features of solar irradiance are explained. Next, five technical aspects of solar forecasting: (1) base forecasting methods, (2) post-processing, (3) irradiance-to-power conversion, (4) verification, and (5) grid-side implications, are reviewed. Following that, ten potential research topics for atmospheric scientists are enumerated; they are related to (1) data and tools, (2) numerical weather prediction, (3) forecast downscaling, (4) large eddy simulation, (5) dimming and brightening, (6) aerosols, (7) spatial forecast verification, (8) multivariate probabilistic forecast verification, (9) predictability, and (10) extreme weather events. Last but not least, a pathway towards ultra-high PV penetration is laid out, based on two recently proposed concepts of firm generation and firm forecasting. It is concluded that the collaboration between the atmospheric science community and power engineering community is necessary if we are to further increase the solar penetration while maintaining the stability and reliability of the power grid, and to achieve carbon neutrality in the long run.},
   author = {Dazhi Yang and Wenting Wang and Christian A. Gueymard and Tao Hong and Jan Kleissl and Jing Huang and Marc J. Perez and Richard Perez and Jamie M. Bright and Xiang'ao Xia and Dennis van der Meer and Ian Marius Peters},
   doi = {10.1016/j.rser.2022.112348},
   issn = {18790690},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {Atmospheric sciences,Carbon neutrality,Grid integration,Power systems,Review,Solar forecasting},
   month = {6},
   publisher = {Elsevier Ltd},
   title = {A review of solar forecasting, its dependence on atmospheric sciences and implications for grid integration: Towards carbon neutrality},
   volume = {161},
   year = {2022},
}
@article{Chu2024,
   abstract = {As a crucial component of the model chain, which facilitates irradiance-to-power conversion during solar resource assessment and forecasting, separation modeling continues to draw attention in both academia and industry. However, when evaluating even the best separation model today, one can quickly recognize its limited accuracy compared to other energy meteorology models such as transposition models. The task of separating global horizontal irradiance into diffuse and beam components does not seem soluble by any derivative effort aimed at tweaking the existing semi-physical models. As a result, an appealing alternative is to consider end-to-end data-driven models, which have demonstrated predictive capability in scenarios where the volume of data is substantial and the interaction among variables is complex. This work discusses the separation of 1-min irradiance from a data-driven perspective. In this preliminary study, a total of 10 representative data-driven separation models are developed and compared to the state-of-the-art semi-physical models, using a comprehensive 1-min irradiance database that spans five years and covers numerous climate types. The average error of the data-driven models is found to be 15.2% to 22.6% lower than that of the semi-physical models for training locations and 7.9% to 17.6% lower for completely unseen locations. Data-driven models also have significantly lower standard deviations (up to 87.2% even for completely unseen locations), highlighting their robustness. In addition, this work provides a guideline for choosing between data-driven and semi-physical models based on data availability, application needs, computational resources, interpretability, and model adaptability. Furthermore, the study underscores the challenges in accurately predicting the diffuse fraction using available input features and indicates that the incorporation of additional weather-related variables and domain knowledge could enhance the performance of data-driven separation models.},
   author = {Yinghao Chu and Dazhi Yang and Hanxin Yu and Xin Zhao and Mengying Li},
   doi = {10.1016/j.apenergy.2023.122434},
   issn = {03062619},
   journal = {Applied Energy},
   keywords = {Benchmarking data,Data-driven models,Diffuse radiation,Separation modeling,Solar radiation modeling},
   month = {2},
   publisher = {Elsevier Ltd},
   title = {Can end-to-end data-driven models outperform traditional semi-physical models in separating 1-min irradiance?},
   volume = {356},
   year = {2024},
}
@misc{Yang2021,
   abstract = {The interactions between solar forecasting strategies and grid codes have a profound impact on grid integration. In order to develop grid-integration standards, such as the forecast submission requirements or penalty schemes that are in the best interests of both the photovoltaic power plant owners and system operators, various challenges of operational solar forecasting need to be brought forward and addressed adequately. On this point, four very much overlooked technical aspects are identified in this work: (1) gauging the goodness of forecasts, (2) quantifying predictability, (3) forecast downscaling, and (4) hierarchical forecasting. The challenges associated with these aspects are discussed in concert with a case study based on the industry standards issued by regulatory bureaus of the National Energy Administration of the People's Republic of China.},
   author = {Dazhi Yang and Weixing Li and Gokhan Mert Yagli and Dipti Srinivasan},
   doi = {10.1016/j.solener.2021.04.002},
   issn = {0038092X},
   journal = {Solar Energy},
   keywords = {China grid-integration policy,Forecast downscaling,Grid integration,Hierarchical forecasting,Predictability},
   month = {8},
   pages = {930-937},
   publisher = {Elsevier Ltd},
   title = {Operational solar forecasting for grid integration: Standards, challenges, and outlook},
   volume = {224},
   year = {2021},
}
@article{Dong2024,
   abstract = {In the realm of power systems, the increasing involvement of residential users in load forecasting applications has heightened concerns about data privacy. Specifically, the load data can inadvertently reveal the daily routines of residential users, thereby posing a risk to their property security. While federated learning (FL) has been employed to safeguard user privacy by enabling model training without the exchange of raw data, these FL models have shown vulnerabilities to emerging attack techniques, such as Deep Leakage from Gradients and poisoning attacks. To counteract these, we initially employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty computation cryptographic techniques to mitigate the risk of gradient leakage. However, the introduction of SecAgg necessitates the deployment of additional sub-center servers for executing the multiparty computation protocol, thereby escalating computational complexity and reducing system robustness, especially in scenarios where one or more sub-centers are unavailable. To address these challenges, we introduce a Markovian Switching-based distributed training framework, the convergence of which is substantiated through rigorous theoretical analysis. The Distributed Markovian Switching (DMS) topology shows strong robustness towards the poisoning attacks as well. Case studies employing real-world power system load data validate the efficacy of our proposed algorithm. It not only significantly minimizes communication complexity but also maintains accuracy levels comparable to traditional FL methods, thereby enhancing the scalability of our load forecasting algorithm.},
   author = {Yi Dong and Yingjie Wang and Mariana Gama and Mustafa A. Mustafa and Geert Deconinck and Xiaowei Huang},
   month = {2},
   title = {Privacy-Preserving Distributed Learning for Residential Short-Term Load Forecasting},
   url = {http://arxiv.org/abs/2402.01546},
   year = {2024},
}
@article{Ahmad2022,
   abstract = {The main and pivot part of electric companies is the load forecasting. Decision-makers and think tank of power sectors should forecast the future need of electricity with large accuracy and small error to give uninterrupted and free of load shedding power to consumers. The demand of electricity can be forecasted amicably by many Machine Learning (ML), Deep Learning (DL) and Artificial Intelligence (AI) techniques among which hybrid methods are most popular. The present technologies of load forecasting and present work regarding combination of various ML, DL and AI algorithms are reviewed in this paper. The comprehensive review of single and hybrid forecasting models with functions; advantages and disadvantages are discussed in this paper. The comparison between the performance of the models in terms of Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) values are compared and discussed with literature of different models to support the researchers to select the best model for load prediction. This comparison validates the fact that the hybrid forecasting models will provide a more optimal solution.},
   author = {Naqash Ahmad and Yazeed Ghadi and Muhammad Adnan and Mansoor Ali},
   doi = {10.1109/ACCESS.2022.3187839},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Load forecasting,load shedding,machine learning,mean absolute percentage error,root mean squared error},
   pages = {71054-71090},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Load Forecasting Techniques for Power System: Research Challenges and Survey},
   volume = {10},
   year = {2022},
}
@article{Rafi2021,
   abstract = {In this study, a new technique is proposed to forecast short-term electrical load. Load forecasting is an integral part of power system planning and operation. Precise forecasting of load is essential for unit commitment, capacity planning, network augmentation and demand side management. Load forecasting can be generally categorized into three classes such as short-term, midterm and long-term. Short-term forecasting is usually done to predict load for next few hours to few weeks. In the literature, various methodologies such as regression analysis, machine learning approaches, deep learning methods and artificial intelligence systems have been used for short-term load forecasting. However, existing techniques may not always provide higher accuracy in short-term load forecasting. To overcome this challenge, a new approach is proposed in this paper for short-term load forecasting. The developed method is based on the integration of convolutional neural network (CNN) and long short-term memory (LSTM) network. The method is applied to Bangladesh power system to provide short-term forecasting of electrical load. Also, the effectiveness of the proposed technique is validated by comparing the forecasting errors with that of some existing approaches such as long short-term memory network, radial basis function network and extreme gradient boosting algorithm. It is found that the proposed strategy results in higher precision and accuracy in short-term load forecasting.},
   author = {Shafiul Hasan Rafi and Nahid Al-Masood and Shohana Rahman Deeba and Eklas Hossain},
   doi = {10.1109/ACCESS.2021.3060654},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Bangladesh power system,Convolutional neural network,Evaluation metrics,Long-short-term memory network,Short-term load forecasting},
   pages = {32436-32448},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A short-term load forecasting method using integrated CNN and LSTM network},
   volume = {9},
   year = {2021},
}
@article{Xiong2023,
   abstract = {Accurate load forecasting is critical for reliable and efficient planning and operation of electric power grids. In this paper, we propose a unifying deep learning framework for load forecasting, which includes time-varying feature weighting, hierarchical temporal attention, and feature-reinforced error correction. Our framework adopts a modular design with good generalization capability. First, the feature-weighting mechanism assigns input features with temporal weights. Second, a recurrent encoder-decoder structure with hierarchical attention is developed as a load predictor. The hierarchical attention enables a similar day selection, which re-evaluates the importance of historical information at each time step. Third, we develop an error correction module that explores the errors and learned feature hidden information to further improve the model's forecasting performance. Experimental results demonstrate that our proposed framework outperforms existing methods on two public datasets and performance metrics, with the feature weighting mechanism and error correction module being critical to achieving superior performance. Our framework provides an effective solution to the electric load forecasting problem, which can be further adapted to many other forecasting tasks.},
   author = {Jing Xiong and Yu Zhang},
   doi = {10.1109/ACCESS.2023.3275095},
   month = {5},
   title = {A Unifying Framework of Attention-based Neural Load Forecasting},
   url = {http://arxiv.org/abs/2305.05082 http://dx.doi.org/10.1109/ACCESS.2023.3275095},
   year = {2023},
}
@article{Emami2023,
   abstract = {Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-fine-tune paradigm for STLF. To help address this, we present BuildingsBench, which consists of: 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock; and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly well to real commercial buildings. An exploration of the effect of increasing dataset size and diversity on zero-shot commercial building performance reveals a power-law with diminishing returns. We also show that fine-tuning pretrained models on real commercial and residential buildings improves performance for a majority of target buildings. We hope that BuildingsBench encourages and facilitates future research on generalizable STLF. All datasets and code can be accessed from https://github.com/NREL/BuildingsBench.},
   author = {Patrick Emami and Abhijeet Sahu and Peter Graf},
   month = {6},
   title = {BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting},
   url = {http://arxiv.org/abs/2307.00142},
   year = {2023},
}
@article{Pelekis2023,
   abstract = {This paper presents DeepTSF, a comprehensive machine learning operations (MLOps) framework aiming to innovate time series forecasting through workflow automation and codeless modeling. DeepTSF automates key aspects of the ML lifecycle, making it an ideal tool for data scientists and MLops engineers engaged in machine learning (ML) and deep learning (DL)-based forecasting. DeepTSF empowers users with a robust and user-friendly solution, while it is designed to seamlessly integrate with existing data analysis workflows, providing enhanced productivity and compatibility. The framework offers a front-end user interface (UI) suitable for data scientists, as well as other higher-level stakeholders, enabling comprehensive understanding through insightful visualizations and evaluation metrics. DeepTSF also prioritizes security through identity management and access authorization mechanisms. The application of DeepTSF in real-life use cases of the I-NERGY project has already proven DeepTSF's efficacy in DL-based load forecasting, showcasing its significant added value in the electrical power and energy systems domain.},
   author = {Sotiris Pelekis and Evangelos Karakolis and Theodosios Pountridis and George Kormpakis and George Lampropoulos and Spiros Mouzakitis and Dimitris Askounis},
   month = {7},
   title = {DeepTSF: Codeless machine learning operations for time series forecasting},
   url = {http://arxiv.org/abs/2308.00709},
   year = {2023},
}
@article{Ullah2023,
   abstract = {Precise short-term load forecasting (STLF) plays a crucial role in the smooth operation of power systems, future capacity planning, unit commitment, and demand response. However, due to its non-stationary and its dependency on multiple cyclic and non-cyclic calendric features and non-linear highly correlated metrological features, an accurate load forecasting with already existing techniques is challenging. To overcome this challenge, a novel hybrid technique based on long short-term memory (LSTM) and a modified split-convolution (SC) neural network (LSTM-SC) is proposed for single-step and multi-step STLF. The concatenating order of LSTM and SC in the proposed hybrid network provides an excellent capability of extraction of sequence-dependent features and other hierarchical spatial features. The model is evaluated by the Pakistan National Grid load dataset recorded by the National Transmission and Dispatch Company (NTDC). The load data is preprocessed and multiple other correlated features are incorporated into the data for performance enhancement. For generalization capability, the performance of LSTMSC is evaluated on publicly available datasets of American Electric Power (AEP) and Independent System Operator New England (ISO-NE). The effect of temperature, a highly correlated input feature, on load forecasting is investigated either by removing the temperature or adding a Gaussian random noise into it. The performance evaluation in terms of RMSE, MAE, and MAPE of the proposed model on the NTDC dataset are 500.98, 372.62, and 3.72% for multi-step while 322.90, 244.22, and 2.38% for single-step load forecasting. The result shows that the proposed method has less forecasting error, strong generalization capability, and satisfactory performance on multi-horizon.},
   author = {Irshad Ullah and Syed Muhammad Hasanat and Khursheed Aurangzeb and Musaed Alhussein and Muhammad Rizwan and Muhammad Shahid Anwar},
   doi = {10.7717/peerj-cs.1487},
   issn = {23765992},
   journal = {PeerJ Computer Science},
   keywords = {CNN,Deep learning,Electrical load consumption,Hybrid,LSTM,Short-Term Load Forecasting (STLF),Smart grid,time-series forecasting},
   publisher = {PeerJ Inc.},
   title = {Multi-horizon short-term load forecasting using hybrid of LSTM and modified split convolution},
   volume = {9},
   year = {2023},
}
@article{Chen2020,
   abstract = {The novel coronavirus (COVID-19) pandemic has posed unprecedented challenges for the utilities and grid operators around the world. In this work, we focus on the problem of load forecasting. With strict social distancing restrictions, power consumption profiles around the world have shifted both in magnitude and daily patterns. These changes have caused significant difficulties in short-term load forecasting. Typically algorithms use weather, timing information and previous consumption levels as input variables, yet they cannot capture large and sudden changes in socioeconomic behavior during the pandemic. In this paper, we introduce mobility as a measure of economic activities to complement existing building blocks of forecasting algorithms. Mobility data acts as good proxies for the population-level behaviors during the implementation and subsequent easing of social distancing measures. The major challenge with such dataset is that only limited mobility records are associated with the recent pandemic. To overcome this small data problem, we design a transfer learning scheme that enables knowledge transfer between several different geographical regions. This architecture leverages the diversity across these regions and the resulting aggregated model can boost the algorithm performance in each region's day-ahead forecast. Through simulations for regions in the US and Europe, we show our proposed algorithm can outperform conventional forecasting methods by more than three-folds. In addition, we demonstrate how the proposed model can be used to project how electricity consumption would recover based on different mobility scenarios.},
   author = {Yize Chen and Weiwei Yang and Baosen Zhang},
   month = {6},
   title = {Using Mobility for Electrical Load Forecasting During the COVID-19 Pandemic},
   url = {http://arxiv.org/abs/2006.08826},
   year = {2020},
}
@article{Chen2018,
   abstract = {We present in this paper a model for forecasting short-term power loads based on deep residual networks. The proposed model is able to integrate domain knowledge and researchers' understanding of the task by virtue of different neural network building blocks. Specifically, a modified deep residual network is formulated to improve the forecast results. Further, a two-stage ensemble strategy is used to enhance the generalization capability of the proposed model. We also apply the proposed model to probabilistic load forecasting using Monte Carlo dropout. Three public datasets are used to prove the effectiveness of the proposed model. Multiple test cases and comparison with existing models show that the proposed model is able to provide accurate load forecasting results and has high generalization capability.},
   author = {Kunjin Chen and Kunlong Chen and Qin Wang and Ziyu He and Jun Hu and Jinliang He},
   month = {5},
   title = {Short-term Load Forecasting with Deep Residual Networks},
   url = {http://arxiv.org/abs/1805.11956},
   year = {2018},
}
@inproceedings{Yang2023,
   abstract = {Electric load forecasting is an indispensable component of electric power system planning and management. Inaccurate load forecasting may lead to the threat of outages or a waste of energy. Accurate electric load forecasting is challenging when there is limited data or even no data, such as load forecasting in holiday, or under extreme weather conditions. As high-stakes decision-making usually follows after load forecasting, model interpretability is crucial for the adoption of forecasting models. In this paper, we propose an interactive GAM which is not only interpretable but also can incorporate specific domain knowledge in electric power industry for improved performance. This boosting-based GAM leverages piecewise linear functions and can be learned through our efficient algorithm. In both public benchmark and electricity datasets, our interactive GAM outperforms current state-of-the-art methods and demonstrates good generalization ability in the cases of extreme weather events. We launched a user-friendly web-based tool based on interactive GAM and already incorporated it into our eForecaster product, a unified AI platform for electricity forecasting.},
   author = {Linxiao Yang and Rui Ren and Xinyue Gu and Liang Sun},
   doi = {10.1145/3580305.3599848},
   isbn = {9798400701030},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {electric load forecasting,generalized additive model,interpretability,piecewise linear function},
   month = {8},
   pages = {5393-5403},
   publisher = {Association for Computing Machinery},
   title = {Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting},
   year = {2023},
}
@article{Tzortzis2023,
   abstract = {Short-term load forecasting (STLF) is crucial for the daily operation of power grids. However, the non-linearity, non-stationarity, and randomness characterizing electricity demand time series renders STLF a challenging task. Various forecasting approaches have been proposed for improving STLF, including neural network (NN) models which are trained using data from multiple electricity demand series that may not necessary include the target series. In the present study, we investigate the performance of this special case of STLF, called transfer learning (TL), by considering a set of 27 time series that represent the national day-ahead electricity demand of indicative European countries. We employ a popular and easy-to-implement NN model and perform a clustering analysis to identify similar patterns among the series and assist TL. In this context, two different TL approaches, with and without the clustering step, are compiled and compared against each other as well as a typical NN training setup. Our results demonstrate that TL can outperform the conventional approach, especially when clustering techniques are considered.},
   author = {Alexandros-Menelaos Tzortzis and Sotiris Pelekis and Evangelos Spiliotis and Spiros Mouzakitis and John Psarras and Dimitris Askounis},
   month = {10},
   title = {Transfer learning for day-ahead load forecasting: a case study on European national electricity demand time series},
   url = {http://arxiv.org/abs/2310.15555},
   year = {2023},
}
@article{Guerra2023,
   abstract = {Some applications of deep learning require not only to provide accurate results but also to quantify the amount of confidence in their prediction. The management of an electric power grid is one of these cases: to avoid risky scenarios, decision-makers need both precise and reliable forecasts of, for example, power loads. For this reason, point forecasts are not enough hence it is necessary to adopt methods that provide an uncertainty quantification. This work focuses on reservoir computing as the core time series forecasting method, due to its computational efficiency and effectiveness in predicting time series. While the RC literature mostly focused on point forecasting, this work explores the compatibility of some popular uncertainty quantification methods with the reservoir setting. Both Bayesian and deterministic approaches to uncertainty assessment are evaluated and compared in terms of their prediction accuracy, computational resource efficiency and reliability of the estimated uncertainty, based on a set of carefully chosen performance metrics.},
   author = {Michele Guerra and Simone Scardapane and Filippo Maria Bianchi},
   month = {8},
   title = {Probabilistic load forecasting with Reservoir Computing},
   url = {http://arxiv.org/abs/2308.12844},
   year = {2023},
}
@article{Yang2023,
   abstract = {This paper proposes a generalised and robust multi-factor Gated Recurrent Unit (GRU) based Deep Learning (DL) model to forecast electricity load in distribution networks during wildfire seasons. The flexible modelling methods consider data input structure, calendar effects and correlation-based leading temperature conditions. Compared to the regular use of instantaneous temperature, the Mean Absolute Percentage Error (MAPE) is decreased by 30.73% by using the proposed input feature selection and leading temperature relationships. Our model is generalised and applied to eight real distribution networks in Victoria, Australia, during the wildfire seasons of 2015-2020. We demonstrate that the GRU-based model consistently outperforms another DL model, Long Short-Term Memory (LSTM), at every step, giving average improvements in Mean Squared Error (MSE) and MAPE of 10.06% and 12.86%, respectively. The sensitivity to large-scale climate variability in training data sets, e.g. El Ni\~no or La Ni\~na years, is considered to understand the possible consequences for load forecasting performance stability, showing minimal impact. Other factors such as regional poverty rate and large-scale off-peak electricity use are potential factors to further improve forecast performance. The proposed method achieves an average forecast MAPE of around 3%, giving a potential annual energy saving of AU\$80.46 million for the state of Victoria.},
   author = {Weijia Yang and Sarah N. Sparrow and David C. H. Wallom},
   month = {4},
   title = {A generalised multi-factor deep learning electricity load forecasting model for wildfire-prone areas},
   url = {http://arxiv.org/abs/2304.10686},
   year = {2023},
}
@article{Tavasoli2023,
   abstract = {Electricity load forecasting is crucial for effectively managing and optimizing power grids. Over the past few decades, various statistical and deep learning approaches have been used to develop load forecasting models. This paper presents an interpretable machine learning approach that identifies load dynamics using data-driven methods within an operator-theoretic framework. We represent the load data using the Koopman operator, which is inherent to the underlying dynamics. By computing the corresponding eigenfunctions, we decompose the load dynamics into coherent spatiotemporal patterns that are the most robust features of the dynamics. Each pattern evolves independently according to its single frequency, making its predictability based on linear dynamics. We emphasize that the load dynamics are constructed based on coherent spatiotemporal patterns that are intrinsic to the dynamics and are capable of encoding rich dynamical features at multiple time scales. These features are related to complex interactions over interconnected power grids and different exogenous effects. To implement the Koopman operator approach more efficiently, we cluster the load data using a modern kernel-based clustering approach and identify power stations with similar load patterns, particularly those with synchronized dynamics. We evaluate our approach using a large-scale dataset from a renewable electric power system within the continental European electricity system and show that the Koopman-based approach outperforms a deep learning (LSTM) architecture in terms of accuracy and computational efficiency. The code for this paper has been deposited in a GitHub repository, which can be accessed at the following address github.com/Shakeri-Lab/Power-Grids.},
   author = {Ali Tavasoli and Behnaz Moradijamei and Heman Shakeri},
   month = {4},
   title = {Characterizing the load profile in power grids by Koopman mode decomposition of interconnected dynamics},
   url = {http://arxiv.org/abs/2304.07832},
   year = {2023},
}
@article{Liu2023,
   abstract = {Accurate prediction of electric load is crucial in power grid planning and management. In this paper, we solve the electric load forecasting problem under extreme events such as scorching heats. One challenge for accurate forecasting is the lack of training samples under extreme conditions. Also load usually changes dramatically in these extreme conditions, which calls for interpretable model to make better decisions. In this paper, we propose a novel forecasting framework, named Self-adaptive Decomposed Interpretable framework~(SaDI), which ensembles long-term trend, short-term trend, and period modelings to capture temporal characteristics in different components. The external variable triggered loss is proposed for the imbalanced learning under extreme events. Furthermore, Generalized Additive Model (GAM) is employed in the framework for desirable interpretability. The experiments on both Central China electric load and public energy meters from buildings show that the proposed SaDI framework achieves average 22.14% improvement compared with the current state-of-the-art algorithms in forecasting under extreme events in terms of daily mean of normalized RMSE. Code, Public datasets, and Appendix are available at: https://doi.org/10.24433/CO.9696980.v1 .},
   author = {Hengbo Liu and Ziqing Ma and Linxiao Yang and Tian Zhou and Rui Xia and Yi Wang and Qingsong Wen and Liang Sun},
   month = {6},
   title = {SaDI: A Self-adaptive Decomposed Interpretable Framework for Electric Load Forecasting under Extreme Events},
   url = {http://arxiv.org/abs/2306.08299},
   year = {2023},
}
@article{Wang2023,
   abstract = {Electrical load forecasting plays a crucial role in decision-making for power systems, including unit commitment and economic dispatch. The integration of renewable energy sources and the occurrence of external events, such as the COVID-19 pandemic, have rapidly increased uncertainties in load forecasting. The uncertainties in load forecasting can be divided into two types: epistemic uncertainty and aleatoric uncertainty. Separating these types of uncertainties can help decision-makers better understand where and to what extent the uncertainty is, thereby enhancing their confidence in the following decision-making. This paper proposes a diffusion-based Seq2Seq structure to estimate epistemic uncertainty and employs the robust additive Cauchy distribution to estimate aleatoric uncertainty. Our method not only ensures the accuracy of load forecasting but also demonstrates the ability to separate the two types of uncertainties and be applicable to different levels of loads. The relevant code can be found at \url\{https://anonymous.4open.science/r/DiffLoad-4714/\}.},
   author = {Zhixian Wang and Qingsong Wen and Chaoli Zhang and Liang Sun and Yi Wang},
   month = {5},
   title = {DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model},
   url = {http://arxiv.org/abs/2306.01001},
   year = {2023},
}
@article{,
   abstract = {Electricity load forecasting is a necessary capability for power system operators and electricity market participants. The proliferation of local generation, demand response, and electrification of heat and transport are changing the fundamental drivers of electricity load and increasing the complexity of load modelling and forecasting. We address this challenge in two ways. First, our setting is adaptive; our models take into account the most recent observations available, yielding a forecasting strategy able to automatically respond to changes in the underlying process. Second, we consider probabilistic rather than point forecasting; indeed, uncertainty quantification is required to operate electricity systems efficiently and reliably. Our methodology relies on the Kalman filter, previously used successfully for adaptive point load forecasting. The probabilistic forecasts are obtained by quantile regressions on the residuals of the point forecasting model. We achieve adaptive quantile regressions using the online gradient descent; we avoid the choice of the gradient step size considering multiple learning rates and aggregation of experts. We apply the method to two data sets: the regional net-load in Great Britain and the demand of seven large cities in the United States. Adaptive procedures improve forecast performance substantially in both use cases for both point and probabilistic forecasting.},
   author = {Joseph de Vilmarest and Jethro Browell and Matteo Fasiolo and Yannig Goude and Olivier Wintenberger},
   month = {1},
   title = {Adaptive Probabilistic Forecasting of Electricity (Net-)Load},
   url = {http://arxiv.org/abs/2301.10090},
   year = {2023},
}
@article{Boussif2023,
   abstract = {Solar power harbors immense potential in mitigating climate change by substantially reducing CO$_\{2\}$ emissions. Nonetheless, the inherent variability of solar irradiance poses a significant challenge for seamlessly integrating solar power into the electrical grid. While the majority of prior research has centered on employing purely time series-based methodologies for solar forecasting, only a limited number of studies have taken into account factors such as cloud cover or the surrounding physical context. In this paper, we put forth a deep learning architecture designed to harness spatio-temporal context using satellite data, to attain highly accurate \textit\{day-ahead\} time-series forecasting for any given station, with a particular emphasis on forecasting Global Horizontal Irradiance (GHI). We also suggest a methodology to extract a distribution for each time step prediction, which can serve as a very valuable measure of uncertainty attached to the forecast. When evaluating models, we propose a testing scheme in which we separate particularly difficult examples from easy ones, in order to capture the model performances in crucial situations, which in the case of this study are the days suffering from varying cloudy conditions. Furthermore, we present a new multi-modal dataset gathering satellite imagery over a large zone and time series for solar irradiance and other related physical variables from multiple geographically diverse solar stations. Our approach exhibits robust performance in solar irradiance forecasting, including zero-shot generalization tests at unobserved solar stations, and holds great promise in promoting the effective integration of solar power into the grid.},
   author = {Oussama Boussif and Ghait Boukachab and Dan Assouline and Stefano Massaroli and Tianle Yuan and Loubna Benabbou and Yoshua Bengio},
   month = {6},
   title = {Improving day-ahead Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context},
   url = {http://arxiv.org/abs/2306.01112},
   year = {2023},
}
@article{Gupta2022,
   abstract = {A neural network-based energy management system (NN-EMS) has been proposed in this paper for islanded ac microgrids fed by multiple PV-battery based distributed generators (DG). The stochastic and unequal irradiation results in unequal PV output, which causes an unequal state-of-charge (SoC) among the batteries of the DGs. This effect may cause the difference in the SoCs to increase considerably over time, leading to some batteries reaching their SoC limits. These batteries would no longer be able to control the dc-link of the hybrid grid forming DG. The proposed NN-EMS ensures SoC balancing by learning an optimal state-action mapping using the outputs of an optimal power flow (OPF). The training dataset has been generated by executing a mixed-integer linear programming based OPF for droop-based island microgrids considering a practical generation-load profile. The resultant NN-EMS controller inherits the information of optimal states and the network behaviour. Compared to traditional time-ahead centralized methods, the proposed strategy does not require accurate generation-load forecasting. Further, it can also respond to the variations in the PV power in near-real-time without resorting to solving an OPF. The proposed NN-EMS controller has been validated by case studies on a CIGRE LV microgrid containing PV-battery hybrid DGs. The proposed concept can also be extended to synthesize decentralized controllers that can cooperate among themselves to achieve a global objective without communication.},
   author = {Yusuf Gupta and Mohammad Amin},
   month = {6},
   title = {A Neural Network-Based Energy Management System for PV-Battery Based Microgrids},
   url = {http://arxiv.org/abs/2206.06716},
   year = {2022},
}
@article{Mercier2023,
   abstract = {This paper proposes an anticipative transformer-based model for short-term solar irradiance forecasting. Given a sequence of sky images, our proposed vision transformer encodes features of consecutive images, feeding into a transformer decoder to predict irradiance values associated with future unseen sky images. We show that our model effectively learns to attend only to relevant features in images in order to forecast irradiance. Moreover, the proposed anticipative transformer captures long-range dependencies between sky images to achieve a forecasting skill of 21.45 % on a 15 minute ahead prediction for a newly introduced dataset of all-sky images when compared to a smart persistence model.},
   author = {Thomas M. Mercier and Tasmiat Rahman and Amin Sabet},
   month = {5},
   title = {Solar Irradiance Anticipative Transformer},
   url = {http://arxiv.org/abs/2305.18487},
   year = {2023},
}
@article{Wang2024,
   abstract = {Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Networks. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor.},
   author = {Zihao Wang and P S Pravin and Zhe Wu},
   month = {1},
   title = {Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks},
   url = {http://arxiv.org/abs/2401.07494},
   year = {2024},
}
@article{,
   abstract = {Solar energy is one of the most popular sources of renewable energy today. It is therefore essential to be able to predict solar power generation and adapt energy needs to these predictions. This paper uses the Transformer deep neural network model, in which the attention mechanism is typically applied in NLP or vision problems. Here, it is extended by combining features based on their spatiotemporal properties in solar irradiance prediction. The results were predicted for arbitrary long-time horizons since the prediction is always 1 day ahead, which can be included at the end along the timestep axis of the input data and the first timestep representing the oldest timestep removed. A maximum worst-case mean absolute percentage error of 3.45% for the one-day-ahead prediction was obtained, which gave better results than the directly competing methods.},
   author = {Jiří Pospíchal and Martin Kubovčík and Iveta Dirgová Luptáková},
   doi = {10.3390/app12178852},
   issn = {20763417},
   issue = {17},
   journal = {Applied Sciences (Switzerland)},
   keywords = {NASA POWER,correlations,renewable energy,sequence-to-sequence prediction,solar irradiance,transformer,weather},
   month = {9},
   publisher = {MDPI},
   title = {Solar Irradiance Forecasting with Transformer Model},
   volume = {12},
   year = {2022},
}
@article{,
   abstract = {The increasing penetration of photovoltaic systems in the power grid makes it vulnerable to cloud shadow projection. Real-time cloud segmentation in ground-based infrared images is important to reduce the noise in intra-hour global solar irradiance forecasting. We present a comparison between discriminative and generative models for cloud segmentation. The performances of supervised and unsupervised learning methods in cloud segmentation are evaluated. The discriminative models are solved in the primal formulation to make them feasible in real-time applications. The performances are compared using the j-statistic. Infrared image preprocessing to remove stationary artifacts increases the overall performance in the analyzed methods. The inclusion of features from neighboring pixels in the feature vectors leads to a performance improvement in some of the cases. Markov Random Fields achieve the best performance in both unsupervised and supervised generative models. Discriminative models solved in the primal yield a dramatically lower computing time along with high performance in the segmentation. Generative and discriminative models are comparable when preprocessing is applied to the infrared images.},
   author = {Guillermo Terrén-Serrano and Manel Martínez-Ramón},
   doi = {10.1016/j.renene.2021.04.141},
   month = {12},
   title = {Comparative Analysis of Methods for Cloud Segmentation in Ground-Based Infrared Images},
   url = {http://arxiv.org/abs/2012.06930 http://dx.doi.org/10.1016/j.renene.2021.04.141},
   year = {2020},
}
@misc{Khodayar2018,
   author = {Mahdi Khodayar and Saeed Mohammadi and Mohammad Khodayar and Jianhui Wang and Guangyi Liu},
   title = {Convolutional Graph Auto-encoder: A Deep Generative Neural Architecture for Probabilistic Spatio-temporal Solar Irradiance Forecasting},
   year = {2018},
}
@article{,
   abstract = {Horizontal atmospheric wind shear causes wind velocity fields to have different directions and speeds. In images of clouds acquired using ground-based sky imagers, clouds may be moving in different wind layers. To increase the performance of an intra-hour global solar irradiance forecasting algorithm, it is important to detect multiple layers of clouds. The information provided by a solar forecasting algorithm is necessary to optimize and schedule the solar generation resources and storage devices in a smart grid. This investigation studies the performance of unsupervised learning techniques when detecting the number of cloud layers in infrared sky images. The images are acquired using an innovative infrared sky imager mounted on a solar tracker. Different mixture models are used to infer the distribution of the cloud features. The optimal decision criterion to find the number of clusters in the mixture models is analyzed and compared between different Bayesian metrics and a sequential hidden Markov model. The motion vectors are computed using a weighted implementation of the Lucas-Kanade algorithm. The correlations between the cloud velocity vectors and temperatures are analyzed to find the method that leads to the most accurate results. We have found that the sequential hidden Markov model outperformed the detection accuracy of the Bayesian metrics.},
   author = {Guillermo Terrén-Serrano and Manel Martínez-Ramón},
   doi = {10.1016/j.knosys.2023.110628},
   month = {5},
   title = {Detection of Clouds in Multiple Wind Velocity Fields using Ground-based Infrared Sky Images},
   url = {http://arxiv.org/abs/2105.03535 http://dx.doi.org/10.1016/j.knosys.2023.110628},
   year = {2021},
}
@article{Cargan2023,
   abstract = {As the use of solar power increases, having accurate and timely forecasts will be essential for smooth grid operators. There are many proposed methods for forecasting solar irradiance / solar power production. However, many of these methods formulate the problem as a time-series, relying on near real-time access to observations at the location of interest to generate forecasts. This requires both access to a real-time stream of data and enough historical observations for these methods to be deployed. In this paper, we propose the use of Global methods to train our models in a generalised way, enabling them to generate forecasts for unseen locations. We apply this approach to both classical ML and state of the art methods. Using data from 20 locations distributed throughout the UK and widely available weather data, we show that it is possible to build systems that do not require access to this data. We utilise and compare both satellite and ground observations (e.g. temperature, pressure) of weather data. Leveraging weather observations and measurements from other locations we show it is possible to create models capable of accurately forecasting solar irradiance at new locations. This could facilitate use planning and optimisation for both newly deployed solar farms and domestic installations from the moment they come online. Additionally, we show that training a single global model for multiple locations can produce a more robust model with more consistent and accurate results across locations.},
   author = {Timothy Cargan and Dario Landa-Silva and Isaac Triguero},
   month = {3},
   title = {Local-Global Methods for Generalised Solar Irradiance Forecasting},
   url = {http://arxiv.org/abs/2303.06010},
   year = {2023},
}
@article{Mayer2023,
   abstract = {Deterministic forecasts (as opposed to ensemble or probabilistic forecasts) issued by numerical weather prediction (NWP) models require post-processing. Such corrective procedure can be viewed as a form of calibration. It is well known that, based on different objective functions, e.g., minimizing the mean square error or the mean absolute error, the calibrated forecasts have different impacts on verification. In this regard, this paper investigates how a calibration directive can affect various aspects of forecast quality outlined in the Murphy–Winkler distribution-oriented verification framework. It is argued that the correlation coefficient is the best measure for the potential performance of NWP forecast verification when linear calibration is involved, because (1) it is not affected by the directive of linear calibration, (2) it can be used to compute the skill score of the linearly calibrated forecasts, and (3) it can avoid the potential deficiency of using squared error to rank forecasts. Since no single error metric can fully represent all aspects of forecast quality, forecasters need to understand the trade-offs between different calibration strategies. To echo the increasing need to bridge atmospheric sciences, renewable energy engineering, and power system engineering, as to move toward the grand goal of carbon neutrality, this paper first provides a brief introduction to solar forecasting, and then revolves its discussion around a solar forecasting case study, such that the readers of this journal can gain further understanding on the subject and thus potentially contribute to it.},
   author = {Martin János Mayer and Dazhi Yang},
   doi = {10.1016/j.ijforecast.2022.03.008},
   issn = {01692070},
   issue = {2},
   journal = {International Journal of Forecasting},
   keywords = {Calibration,Correlation coefficient,Numerical weather prediction,Solar forecasting,Verification},
   month = {4},
   pages = {981-991},
   publisher = {Elsevier B.V.},
   title = {Calibration of deterministic NWP forecasts and its impact on verification},
   volume = {39},
   year = {2023},
}
@misc{Lin2023,
   abstract = {As the penetration of solar energy generation into power systems keeps rising, intra-hour solar forecasting (IHSF) is becoming increasingly important for the secure and economical operation of a power system. One major difficulty in providing very accurate IHSF emanates from rapid cloud changes in the sky. The ground-based sky image (GSI) provides the intuitive information of intra-hour cloud changes and has thus been widely utilized in studies on IHSF. This paper presents a systematic review of the state-of-the-art of ground-based sky image-based intra-hour solar forecasting (GSI-IHSF). To our knowledge, we first propose a generic framework of GSI-IHSF consisting of four modules, i.e., sky image acquisition, sky image preprocessing, cloud forecasting, and solar forecasting. Then, as for each module, this paper introduces its core function, shows the major challenges, briefly reviews several extensively used techniques, summarizing research trends. Finally, this paper offers a prospect of GSI-IHSF research, discusses recent advances that demonstrate the potential for a great improvement in forecast accuracy, pointing out some new requirements and challenges that should be further investigated in the future.},
   author = {Fan Lin and Yao Zhang and Jianxue Wang},
   doi = {10.1016/j.ijforecast.2021.11.002},
   issn = {01692070},
   issue = {1},
   journal = {International Journal of Forecasting},
   keywords = {Cloud forecasting,Computer vision,Ground-based sky image,Intra-hour solar forecasting,Machine learning,Ramp-down forecasting},
   month = {1},
   pages = {244-265},
   publisher = {Elsevier B.V.},
   title = {Recent advances in intra-hour solar forecasting: A review of ground-based sky image methods},
   volume = {39},
   year = {2023},
}
@article{Austnes2023,
   abstract = {Accurate and reliable electricity load forecasts are becoming increasingly important as the share of intermittent resources in the system increases. Distribution System Operators (DSOs) are called to accurately forecast their production and consumption to place optimal bids in the day-ahead market. Violations of their dispatch-plan requires activation of reserve-power which has a direct cost for the DSO, and also necessitate available reserve-capacity. Forecasts must account for the volatility of weather-parameters that impacts both the production and consumption of electricity. If DSO-loads are small or lower-granularity forecasts are needed, traditional statistical methods may fail to provide reliable performance since they rely on a priori statistical distributions of the variables to forecast. In this paper we introduce a probabilistic load forecast (PLF) method based on empirical copulas. Our model is data-driven, does not need a priori assumption on parametric distribution for variables, nor the dependence structure (copula), but employs a kernel density estimate of the underlying distribution using beta kernels that have bounded support on the unit hypercube. The method naturally supports variables with widely different distributions, such as weather data (including forecasted ones) and historic electricity consumption, and produces a conditional probability distribution for every time step in the forecast, which allows inferring the quantiles of interest. The proposed non-parametric approach is highly flexible and can produce meaningful forecasts even at very low aggregated levels (e.g. neighborhoods). We present results from an open dataset and showcase the strength of the model with respect to Quantile Regression using standard probabilistic evaluation metrics.},
   author = {Pål Forr Austnes and Celia García-Pareja and Fabio Nobile and Mario Paolone},
   month = {10},
   title = {Probabilistic Load Forecasting of Distribution Power Systems based on Empirical Copulas},
   url = {http://arxiv.org/abs/2310.03657},
   year = {2023},
}
@article{Park2024,
   abstract = {In the effort to achieve carbon neutrality through a decentralized electricity market, accurate short-term load forecasting at low aggregation levels has become increasingly crucial for various market participants' strategies. Accurate probabilistic forecasts at low aggregation levels can improve peer-to-peer energy sharing, demand response, and the operation of reliable distribution networks. However, these applications require not only probabilistic demand forecasts, which involve quantification of the forecast uncertainty, but also determining which consumers to include in the aggregation to meet electricity supply at the forecast lead time. While research papers have been proposed on the supply side, no similar research has been conducted on the demand side. This paper presents a method for creating a portfolio that optimally aggregates demand for a given energy demand, minimizing forecast inaccuracy of overall low-level aggregation. Using probabilistic load forecasts produced by either ARMA-GARCH models or kernel density estimation (KDE), we propose three approaches to creating a portfolio of residential households' demand: Forecast Validated, Seasonal Residual, and Seasonal Similarity. An evaluation of probabilistic load forecasts demonstrates that all three approaches enhance the accuracy of forecasts produced by random portfolios, with the Seasonal Residual approach for Korea and Ireland outperforming the others in terms of both accuracy and computational efficiency. Highlights: • We proposed new low-voltage aggregation techniques based on portfolio theory to improve the accuracy of probabilistic electricity demand forecasting. • The increased volatility at the low voltage level requires probabilistic forecasting to measure uncertainties. • The proposed methods were found to outperform random aggregation with the real-world data from Korea and Ireland. • A simple forecasting approach minimizing the standard deviation of deseasonalized demands was accurate and computationally efficient. • The proposed method can contribute to achieving carbon neutrality, enable peer-to-peer energy sharing, and benefit policymakers, energy companies, and consumers. 2},
   author = {Jungyeon Park and Estêvão Alvarenga and Jooyoung Jeon and Ran Li and Fotios Petropoulos and Hokyun Kim and Kwangwon Ahn},
   issue = {122109},
   journal = {Applied Energy},
   keywords = {Aggregated electricity demand,Low-aggregation load,Portfolio optimization,Probabilistic forecasts,Short-term load forecasting},
   title = {Probabilistic forecast-based portfolio optimization of electricity demand at low aggregation levels},
   volume = {353},
   year = {2024},
}
@article{Jeon2022,
   abstract = {Solar irradiance forecasting is fundamental and essential for commercializing solar energy generation by overcoming output variability. Accurate forecasting depends on historical solar irradiance data, correlations between various meteorological variables (e.g., wind speed, humidity, and cloudiness), and influences between the weather contexts of spatially adjacent regions. However, existing studies have been limited to spatiotemporal analysis of a few variables, which have clear correlations with solar irradiance (e.g., sunshine duration), and do not attempt to establish atmospheric contextual information from a variety of meteorological variables. Therefore, this study proposes a novel solar irradiance forecasting model that represents atmospheric parameters observed from multiple stations as an attributed dynamic network and analyzes temporal changes in the network by extending existing spatio-temporal graph convolutional network (ST-GCN) models. By comparing the proposed model with existing models, we also investigated the contributions of (i) the spatial adjacency of the stations, (ii) temporal changes in the meteorological variables, and (iii) the variety of variables to the forecasting performance. We evaluated the performance of the proposed and existing models by predicting the hourly solar irradiance at observation stations in the Korean Peninsula. The experimental results showed that the three features are synergistic and have correlations that are difficult to establish using single-aspect analysis.},
   author = {Hyeon Ju Jeon and Min Woo Choi and O. Joun Lee},
   doi = {10.3390/s22197179},
   issn = {14248220},
   issue = {19},
   journal = {Sensors},
   keywords = {graph neural network,multivariate spatio-temporal analysis,solar irradiance forecasting,spatio-temporal graph convolutional network,weather forecasting},
   month = {10},
   pmid = {36236280},
   publisher = {MDPI},
   title = {Day-Ahead Hourly Solar Irradiance Forecasting Based on Multi-Attributed Spatio-Temporal Graph Convolutional Network},
   volume = {22},
   year = {2022},
}
@article{Mayer2023,
   abstract = {Under the two-step framework of photovoltaic (PV) power forecasting, that is, forecasting first the irradiance and then converting it to PV power, there are two chief ways in which one can account for the uncertainty embedded in the final PV power forecast. One of those is to produce probabilistic irradiance forecast through, for example, ensemble numerical weather prediction (NWP), and the other is to pass the irradiance forecast through a collection of different irradiance-to-power conversion sequences, which are known as model chains. This work investigates, for the first time, into the question: Whether pairing ensemble NWP with ensemble model chain is better than leveraging any individual method alone? Using data from 14 utility-scale ground-mounted PV plants in Hungary and the state-of-the-art global mesoscale NWP model of the European Centre for Medium-Range Weather Forecasts, it is herein demonstrated that the best probabilistic PV power forecast needs to consider both ensemble NWP and ensemble model chain. Furthermore, owing to the higher-quality probabilistic forecasts, the point forecast accuracy is also improved substantially through pairing. Overall, the recommended paring strategy achieves a mean-normalized continuous ranked probability score and a root mean square error of 18.4% and 42.1%, respectively.},
   author = {Martin János Mayer and Dazhi Yang},
   doi = {10.1016/j.rser.2023.113171},
   issn = {18790690},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {ECMWF,Ensemble NWP,Photovoltaic power,Physical model chains,Probabilistic solar forecasting,Quantile regression},
   month = {4},
   publisher = {Elsevier Ltd},
   title = {Pairing ensemble numerical weather prediction with ensemble physical model chain for probabilistic photovoltaic power forecasting},
   volume = {175},
   year = {2023},
}
@article{Bayram2023,
   abstract = {Load forecasting is a crucial topic in energy management systems (EMS) due to its vital role in optimizing energy scheduling and enabling more flexible and intelligent power grid systems. As a result, these systems allow power utility companies to respond promptly to demands in the electricity market. Deep learning (DL) models have been commonly employed in load forecasting problems supported by adaptation mechanisms to cope with the changing pattern of consumption by customers, known as concept drift. A drift magnitude threshold should be defined to design change detection methods to identify drifts. While the drift magnitude in load forecasting problems can vary significantly over time, existing literature often assumes a fixed drift magnitude threshold, which should be dynamically adjusted rather than fixed during system evolution. To address this gap, in this paper, we propose a dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting. We integrate several strategies into the framework based on active and passive adaptation approaches. To evaluate DA-LSTM in real-life settings, we thoroughly analyze the proposed framework and deploy it in a real-world problem through a cloud-based environment. Efficiency is evaluated in terms of the prediction performance of each approach and computational cost. The experiments show performance improvements on multiple evaluation metrics achieved by our framework compared to baseline methods from the literature. Finally, we present a trade-off analysis between prediction performance and computational costs.},
   author = {Firas Bayram and Phil Aupke and Bestoun S. Ahmed and Andreas Kassler and Andreas Theocharis and Jonas Forsman},
   month = {5},
   title = {DA-LSTM: A Dynamic Drift-Adaptive Learning Framework for Interval Load Forecasting with LSTM Networks},
   url = {http://arxiv.org/abs/2305.08767},
   year = {2023},
}
@article{Yang2015,
   abstract = {We find an application of the lasso (least absolute shrinkage and selection operator) in sub-5-min solar irradiance forecasting using a monitoring network. Lasso is a variable shrinkage and selection method for linear regression. In addition to the sum of squares error minimization, it considers the sum of ℓ1-norms of the regression coefficients as penalty. This bias-variance trade-off very often leads to better predictions.One second irradiance time series data are collected using a dense monitoring network in Oahu, Hawaii. As clouds propagate over the network, highly correlated lagged time series can be observed among station pairs. Lasso is used to automatically shrink and select the most appropriate lagged time series for regression. Since only lagged time series are used as predictors, the regression provides true out-of-sample forecasts. It is found that the proposed model outperforms univariate time series models and ordinary least squares regression significantly, especially when training data are few and predictors are many. Very short-term irradiance forecasting is useful in managing the variability within a central PV power plant.},
   author = {Dazhi Yang and Zhen Ye and Li Hong Idris Lim and Zibo Dong},
   doi = {10.1016/j.solener.2015.01.016},
   issn = {0038092X},
   journal = {Solar Energy},
   keywords = {Irradiance forecasting,Lasso,Monitoring network,Parameter shrinkage},
   month = {4},
   pages = {314-326},
   publisher = {Elsevier Ltd},
   title = {Very short term irradiance forecasting using the lasso},
   volume = {114},
   year = {2015},
}
@article{Haben2021,
   abstract = {The increased digitalisation and monitoring of the energy system opens up numerous opportunities to decarbonise the energy system. Applications on low voltage, local networks, such as community energy markets and smart storage will facilitate decarbonisation, but they will require advanced control and management. Reliable forecasting will be a necessary component of many of these systems to anticipate key features and uncertainties. Despite this urgent need, there has not yet been an extensive investigation into the current state-of-the-art of low voltage level forecasts, other than at the smart meter level. This paper aims to provide a comprehensive overview of the landscape, current approaches, core applications, challenges and recommendations. Another aim of this paper is to facilitate the continued improvement and advancement in this area. To this end, the paper also surveys some of the most relevant and promising trends. It establishes an open, community-driven list of the known low voltage level open datasets to encourage further research and development.},
   author = {Stephen Haben and Siddharth Arora and Georgios Giasemidis and Marcus Voss and Danica Vukadinović Greetham},
   doi = {10.1016/j.apenergy.2021.117798},
   issn = {03062619},
   journal = {Applied Energy},
   keywords = {Demand forecasting,Load forecasting,Low voltage,Machine learning,Neural networks,Review,Smart grid,Smart meter,Substations,Survey,Time series},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {Review of low voltage load forecasting: Methods, applications, and recommendations},
   volume = {304},
   year = {2021},
}
@inproceedings{Prabowo2023,
   abstract = {In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the second longest total lockdown duration globally during the pandemic. Results underscore the crucial role of continual learning in accurate energy forecasting, particularly during Out-of-Distribution periods. Secondary data such as mobility and temperature provided ancillary support to the primary forecasting model. More importantly, while traditional methods struggled to adapt during lockdowns, models featuring at least online learning demonstrated resilience, with lockdown periods posing fewer challenges once armed with adaptive learning techniques. This study contributes valuable methodologies and insights to the ongoing effort to improve energy load forecasting during future Out-of-Distribution periods.},
   author = {Arian Prabowo and Kaixuan Chen and Hao Xue and Subbu Sethuvenkatraman and Flora D. Salim},
   doi = {10.1145/3600100.3623726},
   isbn = {9798400702303},
   journal = {BuildSys 2023 - Proceedings of the10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
   keywords = {benchmarking,continual learning,electricity use,energy use,out-of-distribution,timeseries forecasting},
   month = {11},
   pages = {41-50},
   publisher = {Association for Computing Machinery, Inc},
   title = {Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: Benchmarking energy load forecasting models without and with continual learning},
   year = {2023},
}
@article{Smyl2021,
   abstract = {Short-term load forecasting (STLF) is challenging due to complex time series (TS) which express three seasonal patterns and a nonlinear trend. This paper proposes a novel hybrid hierarchical deep learning model that deals with multiple seasonality and produces both point forecasts and predictive intervals (PIs). It combines exponential smoothing (ES) and a recurrent neural network (RNN). ES extracts dynamically the main components of each individual TS and enables on-the-fly deseasonalization, which is particularly useful when operating on a relatively small data set. A multi-layer RNN is equipped with a new type of dilated recurrent cell designed to efficiently model both short and long-term dependencies in TS. To improve the internal TS representation and thus the model's performance, RNN learns simultaneously both the ES parameters and the main mapping function transforming inputs into forecasts. We compare our approach against several baseline methods, including classical statistical methods and machine learning (ML) approaches, on STLF problems for 35 European countries. The empirical study clearly shows that the proposed model has high expressive power to solve nonlinear stochastic forecasting problems with TS including multiple seasonality and significant random fluctuations. In fact, it outperforms both statistical and state-of-the-art ML models in terms of accuracy.},
   author = {Slawek Smyl and Grzegorz Dudek and Paweł Pełka},
   month = {12},
   title = {ES-dRNN: A Hybrid Exponential Smoothing and Dilated Recurrent Neural Network Model for Short-Term Load Forecasting},
   url = {http://arxiv.org/abs/2112.02663},
   year = {2021},
}
@article{Haben2021,
   abstract = {The increased digitalisation and monitoring of the energy system opens up numerous opportunities to decarbonise the energy system. Applications on low voltage, local networks, such as community energy markets and smart storage will facilitate decarbonisation, but they will require advanced control and management. Reliable forecasting will be a necessary component of many of these systems to anticipate key features and uncertainties. Despite this urgent need, there has not yet been an extensive investigation into the current state-of-the-art of low voltage level forecasts, other than at the smart meter level. This paper aims to provide a comprehensive overview of the landscape, current approaches, core applications, challenges and recommendations. Another aim of this paper is to facilitate the continued improvement and advancement in this area. To this end, the paper also surveys some of the most relevant and promising trends. It establishes an open, community-driven list of the known low voltage level open datasets to encourage further research and development.},
   author = {Stephen Haben and Siddharth Arora and Georgios Giasemidis and Marcus Voss and Danica Vukadinovic Greetham},
   doi = {10.1016/j.apenergy.2021.117798},
   month = {5},
   title = {Review of Low Voltage Load Forecasting: Methods, Applications, and Recommendations},
   url = {http://arxiv.org/abs/2106.00006 http://dx.doi.org/10.1016/j.apenergy.2021.117798},
   year = {2021},
}
@article{Arpogaus2023,
   abstract = {The transition to a fully renewable energy grid requires better forecasting of demand at the low-voltage level to increase efficiency and ensure reliable control. However, high fluctuations and increasing electrification cause huge forecast variability, not reflected in traditional point estimates. Probabilistic load forecasts take uncertainties into account and thus allow more informed decision-making for the planning and operation of low-carbon energy systems. We propose an approach for flexible conditional density forecasting of short-term load based on Bernstein polynomial normalizing flows, where a neural network controls the parameters of the flow. In an empirical study with 3639 smart meter customers, our density predictions for 24h-ahead load forecasting compare favorably against Gaussian and Gaussian mixture densities. Furthermore, they outperform a non-parametric approach based on the pinball loss, especially in low-data scenarios.},
   author = {Marcel Arpogaus and Marcus Voss and Beate Sick and Mark Nigge-Uricher and Oliver Durr},
   doi = {10.1109/TSG.2023.3254890},
   issn = {19493061},
   issue = {6},
   journal = {IEEE Transactions on Smart Grid},
   keywords = {Normalizing flows,deep learning,low-voltage,probabilistic load forecasting,probabilistic regression},
   month = {11},
   pages = {4902-4911},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Short-Term Density Forecasting of Low-Voltage Load Using Bernstein-Polynomial Normalizing Flows},
   volume = {14},
   year = {2023},
}
@article{Gasparin2019,
   abstract = {Management and efficient operations in critical infrastructure such as Smart Grids take huge advantage of accurate power load forecasting which, due to its nonlinear nature, remains a challenging task. Recently, deep learning has emerged in the machine learning field achieving impressive performance in a vast range of tasks, from image classification to machine translation. Applications of deep learning models to the electric load forecasting problem are gaining interest among researchers as well as the industry, but a comprehensive and sound comparison among different architectures is not yet available in the literature. This work aims at filling the gap by reviewing and experimentally evaluating on two real-world datasets the most recent trends in electric load forecasting, by contrasting deep learning architectures on short term forecast (one day ahead prediction). Specifically, we focus on feedforward and recurrent neural networks, sequence to sequence models and temporal convolutional neural networks along with architectural variants, which are known in the signal processing community but are novel to the load forecasting one.},
   author = {Alberto Gasparin and Slobodan Lukovic and Cesare Alippi},
   month = {7},
   title = {Deep Learning for Time Series Forecasting: The Electric Load Case},
   url = {http://arxiv.org/abs/1907.09207},
   year = {2019},
}
@article{Mayer2022,
   abstract = {Physical model chain is a step-by-step modeling framework for the conversion of irradiance to photovoltaic (PV) power. When a model chain is fed with irradiance forecasts, it provides the corresponding PV power forecasts. Despite its advantages, forecasting with model chains has yet to receive the attention that it deserves. In several recent works, however, the idea of model-chain-based solar forecasting has been formally modernized, though the framework was restricted to deterministic forecasting. In this work, the model-chain-based forecasting framework is extended to the probability space, in that, a calibrated ensemble of model chains is used to generate probabilistic PV power forecasts. Using two-year data from eight PV plants in Hungary, alongside professional weather forecasts issued by the Hungarian Meteorological Services, it is empirically shown that the raw model-chain ensemble forecasts tend to be underdispered, but adequate post-processing is able to improve calibration and reduce the continuous ranked probability score of raw ensembles by 20%. Given the fact that uncertainty quantification has a cardinal importance to grid integration, this probabilistic extension of the model-chain-based solar forecasting framework is thought beneficial.},
   author = {Martin János Mayer and Dazhi Yang},
   doi = {10.1016/j.rser.2022.112821},
   issn = {18790690},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {Irradiance-to-power conversion,Photovoltaic power,Physical model chains,Probabilistic solar forecasting,Quantile regression},
   month = {10},
   publisher = {Elsevier Ltd},
   title = {Probabilistic photovoltaic power forecasting using a calibrated ensemble of model chains},
   volume = {168},
   year = {2022},
}
@article{Yang2022,
   abstract = {Solar forecast verification must incorporate a notion of “relativity.” Particularly for deterministic forecasts, one wishes to know the upper and lower bounds of the mean square error (MSE) for the forecasting situation of concern, such that the performance of some forecasts of interest can be quantified relative to these bounds. This work proposes a method for estimating these bounds. For the upper bound of MSE, the theory builds upon the premise that no forecast worse than that provided by the standard of reference should be used. Under some mild assumptions, the MSE of the standard of reference, which is herein defined to be the optimal convex combination of climatology and persistence, varies solely with the lag-h autocorrelation of the clear-sky index. Then, by fitting a theoretical isotropic correlogram with a nugget effect to the empirical autocorrelations, the upper bound of MSE, as a function of forecast horizon, can be derived. For the lower bound of MSE, it can be approximated by the predictability error growth, which is the difference between the controlled and perturbed forecasts from a perfect dynamical weather model. The empirical part of this work considers the irradiance forecasts from the European Centre for Medium-Range Weather Forecasts, at seven locations in United States, over a period of two years (2019–2020). Various MSEs reported in the literature are put in perspective of the estimated bounds of MSE. The proposed bounds have a profound impact on predictability quantification and skill score computation, which are essential for comparative forecast verification.},
   author = {Dazhi Yang},
   doi = {10.1016/j.rser.2022.112736},
   issn = {18790690},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {Bounds of MSE,ECMWF,Forecast verification,Predictability,Solar forecasting,Solar irradiance},
   month = {10},
   publisher = {Elsevier Ltd},
   title = {Correlogram, predictability error growth, and bounds of mean square error of solar irradiance forecasts},
   volume = {167},
   year = {2022},
}
@article{Xu2023,
   abstract = {Data privacy and security have become a non-negligible factor in load forecasting. Previous researches mainly focus on training stage enhancement. However, once the model is trained and deployed, it may need to `forget' (i.e., remove the impact of) part of training data if the data is found to be malicious or as requested by the data owner. This paper introduces machine unlearning algorithm which is specifically designed to remove the influence of part of the original dataset on an already trained forecaster. However, direct unlearning inevitably degrades the model generalization ability. To balance between unlearning completeness and performance degradation, a performance-aware algorithm is proposed by evaluating the sensitivity of local model parameter change using influence function and sample re-weighting. Moreover, we observe that the statistic criterion cannot fully reflect the operation cost of down-stream tasks. Therefore, a task-aware machine unlearning is proposed whose objective is a tri-level optimization with dispatch and redispatch problems considered. We theoretically prove the existence of the gradient of such objective, which is key to re-weighting the remaining samples. We test the unlearning algorithms on linear and neural network load forecasters with realistic load dataset. The simulation demonstrates the balance on unlearning completeness and operational cost. All codes can be found at https://github.com/xuwkk/task_aware_machine_unlearning.},
   author = {Wangkun Xu and Fei Teng},
   month = {8},
   title = {Task-Aware Machine Unlearning and Its Application in Load Forecasting},
   url = {http://arxiv.org/abs/2308.14412},
   year = {2023},
}
@article{Wang2023,
   abstract = {Load forecasting is of great significance in the power industry as it can provide a reference for subsequent tasks such as power grid dispatch, thus bringing huge economic benefits. However, there are many differences between load forecasting and traditional time series forecasting. On the one hand, load forecasting aims to minimize the cost of subsequent tasks such as power grid dispatch, rather than simply pursuing prediction accuracy. On the other hand, the load is largely influenced by many external factors, such as temperature or calendar variables. In addition, the scale of predictions (such as building-level loads and aggregated-level loads) can also significantly impact the predicted results. In this paper, we provide a comprehensive load forecasting archive, which includes load domain-specific feature engineering to help forecasting models better model load data. In addition, different from the traditional loss function which only aims for accuracy, we also provide a method to customize the loss function based on the forecasting error, integrating it into our forecasting framework. Based on this, we conducted extensive experiments on load data at different levels, providing a reference for researchers to compare different load forecasting models.},
   author = {Zhixian Wang and Qingsong Wen and Chaoli Zhang and Liang Sun and Leandro Von Krannichfeldt and Yi Wang},
   month = {7},
   title = {Benchmarks and Custom Package for Electrical Load Forecasting},
   url = {http://arxiv.org/abs/2307.07191},
   year = {2023},
}
@article{Smyl2022,
   abstract = {In this paper, we propose a new short-term load forecasting (STLF) model based on contextually enhanced hybrid and hierarchical architecture combining exponential smoothing (ES) and a recurrent neural network (RNN). The model is composed of two simultaneously trained tracks: the context track and the main track. The context track introduces additional information to the main track. It is extracted from representative series and dynamically modulated to adjust to the individual series forecasted by the main track. The RNN architecture consists of multiple recurrent layers stacked with hierarchical dilations and equipped with recently proposed attentive dilated recurrent cells. These cells enable the model to capture short-term, long-term and seasonal dependencies across time series as well as to weight dynamically the input information. The model produces both point forecasts and predictive intervals. The experimental part of the work performed on 35 forecasting problems shows that the proposed model outperforms in terms of accuracy its predecessor as well as standard statistical models and state-of-the-art machine learning models.},
   author = {Slawek Smyl and Grzegorz Dudek and Paweł Pełka},
   month = {12},
   title = {Contextually Enhanced ES-dRNN with Dynamic Attention for Short-Term Load Forecasting},
   url = {http://arxiv.org/abs/2212.09030},
   year = {2022},
}
@article{Smyl2022,
   abstract = {Short-term load forecasting (STLF) is a challenging problem due to the complex nature of the time series expressing multiple seasonality and varying variance. This paper proposes an extension of a hybrid forecasting model combining exponential smoothing and dilated recurrent neural network (ES-dRNN) with a mechanism for dynamic attention. We propose a new gated recurrent cell -- attentive dilated recurrent cell, which implements an attention mechanism for dynamic weighting of input vector components. The most relevant components are assigned greater weights, which are subsequently dynamically fine-tuned. This attention mechanism helps the model to select input information and, along with other mechanisms implemented in ES-dRNN, such as adaptive time series processing, cross-learning, and multiple dilation, leads to a significant improvement in accuracy when compared to well-established statistical and state-of-the-art machine learning forecasting models. This was confirmed in the extensive experimental study concerning STLF for 35 European countries.},
   author = {Slawek Smyl and Grzegorz Dudek and Paweł Pełka},
   month = {3},
   title = {ES-dRNN with Dynamic Attention for Short-Term Load Forecasting},
   url = {http://arxiv.org/abs/2203.00937},
   year = {2022},
}
@article{Zhang2022,
   abstract = {Prediction intervals offer an effective tool for quantifying the uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to unforeseen changes in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles. It relies on the online learning ability of reinforcement learning to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve the quality of PIs. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.},
   author = {Yufan Zhang and Honglin Wen and Qiuwei Wu and Qian Ai},
   doi = {10.1109/TSG.2022.3226423},
   month = {5},
   title = {Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning},
   url = {http://arxiv.org/abs/2205.08698 http://dx.doi.org/10.1109/TSG.2022.3226423},
   year = {2022},
}
@misc{Singla2022,
   abstract = {In the last two decades, renewable energy has been paid immeasurable attention to toward the attainment of electricity requirements for domestic, industrial, and agriculture sectors. Solar forecasting plays a vital role in smooth operation, scheduling, and balancing of electricity production by standalone PV plants as well as grid interconnected solar PV plants. Numerous models and techniques have been developed in short, mid and long-term solar forecasting. This paper analyzes some of the potential solar forecasting models based on various methodologies discussed in literature, by mainly focusing on investigating the influence of meteorological variables, time horizon, climatic zone, pre-processing techniques, air pollution, and sample size on the complexity and accuracy of the model. To make the paper reader-friendly, it presents all-important parameters and findings of the models revealed from different studies in a tabular mode having the year of publication, time resolution, input parameters, forecasted parameters, error metrics, and performance. The literature studied showed that ANN-based models outperform the others due to their nonlinear complex problemsolving capabilities. Their accuracy can be further improved by hybridization of the two models or by performing pre-processing on the input data. Besides, it also discusses the diverse key constituents that affect the accuracy of a model. It has been observed that the proper selection of training and testing period along with the correlated dependent variables also enhances the accuracy of the model.},
   author = {Pardeep Singla and Manoj Duhan and Sumit Saroha},
   doi = {10.1007/s11708-021-0722-7},
   issn = {20951698},
   issue = {2},
   journal = {Frontiers in Energy},
   keywords = {error metric,forecasting techniques,hybrid models,neural network,solar forecasting,support vector machine (SVM)},
   month = {4},
   pages = {187-223},
   publisher = {Higher Education Press Limited Company},
   title = {A comprehensive review and analysis of solar forecasting techniques},
   volume = {16},
   year = {2022},
}
@article{,
   abstract = {Time series (TS) occur in many scientific and commercial applications, ranging from earth surveillance to industry automation to the smart grids. An important type of TS analysis is classification, which can, for instance, improve energy load forecasting in smart grids by detecting the types of electronic devices based on their energy consumption profiles recorded by automatic sensors. Such sensor-driven applications are very often characterized by (a) very long TS and (b) very large TS datasets needing classification. However, current methods to time series classification (TSC) cannot cope with such data volumes at acceptable accuracy; they are either scalable but offer only inferior classification quality, or they achieve state-of-the-art classification quality but cannot scale to large data volumes. In this paper, we present WEASEL (Word ExtrAction for time SEries cLassification), a novel TSC method which is both scalable and accurate. Like other state-of-the-art TSC methods, WEASEL transforms time series into feature vectors, using a sliding-window approach, which are then analyzed through a machine learning classifier. The novelty of WEASEL lies in its specific method for deriving features, resulting in a much smaller yet much more discriminative feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more accurate than the best current non-ensemble algorithms at orders-of-magnitude lower classification and training times, and it is almost as accurate as ensemble classifiers, whose computational complexity makes them inapplicable even for mid-size datasets. The outstanding robustness of WEASEL is also confirmed by experiments on two real smart grid datasets, where it out-of-the-box achieves almost the same accuracy as highly tuned, domain-specific methods.},
   author = {Patrick Schäfer and Ulf Leser},
   doi = {10.1145/3132847.3132980},
   month = {1},
   title = {Fast and Accurate Time Series Classification with WEASEL},
   url = {http://arxiv.org/abs/1701.07681 http://dx.doi.org/10.1145/3132847.3132980},
   year = {2017},
}
@article{Yang2023,
   author = {Jonathan, Yang and Mingjian Tuo and Jin Lu and Xingpeng Li},
   journal = {arXiv preprint},
   title = {Analysis of Weather and Time Features in Machine Learning-aided ERCOT Load Forecasting},
   year = {2023},
}
@article{,
   abstract = {Load forecasting is crucial for multiple energy management tasks such as scheduling generation capacity, planning supply and demand, and minimizing energy trade costs. Such relevance has increased even more in recent years due to the integration of renewable energies, electric cars, and microgrids. Conventional load forecasting techniques obtain single-value load forecasts by exploiting consumption patterns of past load demand. However, such techniques cannot assess intrinsic uncertainties in load demand, and cannot capture dynamic changes in consumption patterns. To address these problems, this paper presents a method for probabilistic load forecasting based on the adaptive online learning of hidden Markov models. We propose learning and forecasting techniques with theoretical guarantees, and experimentally assess their performance in multiple scenarios. In particular, we develop adaptive online learning techniques that update model parameters recursively, and sequential prediction techniques that obtain probabilistic forecasts using the most recent parameters. The performance of the method is evaluated using multiple datasets corresponding with regions that have different sizes and display assorted time-varying consumption patterns. The results show that the proposed method can significantly improve the performance of existing techniques for a wide range of scenarios.},
   author = {Verónica Álvarez and Santiago Mazuelas and José A. Lozano},
   doi = {10.1109/TPWRS.2021.3050837},
   month = {11},
   title = {Probabilistic Load Forecasting Based on Adaptive Online Learning},
   url = {http://arxiv.org/abs/2011.14721 http://dx.doi.org/10.1109/TPWRS.2021.3050837},
   year = {2020},
}
@article{Rahimi2023,
   abstract = {With increasing demand for energy, the penetration of alternative sources such as renewable energy in power grids has increased. Solar energy is one of the most common and well-known sources of energy in existing networks. But because of its non-stationary and non-linear characteristics, it needs to predict solar irradiance to provide more reliable Photovoltaic (PV) plants and manage the power of supply and demand. Although there are various methods to predict the solar irradiance. This paper gives the overview of recent studies with focus on solar irradiance forecasting with ensemble methods which are divided into two main categories: competitive and cooperative ensemble forecasting. In addition, parameter diversity and data diversity are considered as competitive ensemble forecasting and also preprocessing and post-processing are as cooperative ensemble forecasting. All these ensemble forecasting methods are investigated in this study. In the end, the conclusion has been drawn and the recommendations for future studies have been discussed.},
   author = {Negar Rahimi and Sejun Park and Wonseok Choi and Byoungryul Oh and Sookyung Kim and Young ho Cho and Sunghyun Ahn and Chulho Chong and Daewon Kim and Cheong Jin and Duehee Lee},
   doi = {10.1007/s42835-023-01378-2},
   issn = {20937423},
   issue = {2},
   journal = {Journal of Electrical Engineering and Technology},
   keywords = {Cooperative ensemble forecasting,Ensemble methods,Solar forecasting},
   month = {3},
   pages = {719-733},
   publisher = {Korean Institute of Electrical Engineers},
   title = {A Comprehensive Review on Ensemble Solar Power Forecasting Algorithms},
   volume = {18},
   year = {2023},
}
@article{Zhang2022,
   abstract = {Solar forecasters have hitherto been restricting the application of numerical weather prediction (NWP) to day-ahead forecasting scenarios. With the hourly updated NWP models, such as the National Oceanic and Atmospheric Administration's Rapid Refresh (RAP) or High-Resolution Rapid Refresh (HRRR), it is theoretically possible to utilize NWP for hour-ahead solar forecasting. Nonetheless, for NWP-based hourly forecasts to be useful, they ought to be post-processed, because such forecasts are almost always affected by the inherent bias of dynamical weather models. In this regard, Kalman filtering is herein used as a post-processing tool. Using one year of RAP and HRRR forecasts and ground-based observations made at seven geographically diverse locations in the contiguous United States, it is demonstrated that the post-processed versions of hourly updated NWP forecasts are sometimes able to attain a higher accuracy than those from classic time-series families of models, not only at long-range, but also at short-range forecast horizons. Although these improvements are marginal in terms of squared loss (about 1%–2%), since NWP models have a very different forecast-generating mechanism (solving the governing equations of motion) from that of time series methods (extrapolating data), NWP-based forecasts can be expected to be less correlated with those forecasts from time series models. Consequently, one should find this diversity profoundly rewarding during forecast combination, for the combined forecasts are able to consistently result in smaller error than the best component forecasts.},
   author = {Gang Zhang and Dazhi Yang and George Galanis and Emmanouil Androulakis},
   doi = {10.1016/j.rser.2021.111768},
   issn = {18790690},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {Hourly solar forecasting,Kalman filtering,Numerical weather prediction,Post-processing},
   month = {2},
   publisher = {Elsevier Ltd},
   title = {Solar forecasting with hourly updated numerical weather prediction},
   volume = {154},
   year = {2022},
}
@article{Mishra2019,
   abstract = {The amount of energy generation from renewable energy sources, particularly from wind and photovoltaic plants, has seen a rapid rise in the last decade. Reliable and economic operation of power systems thus requires an accurate estimate of the power generated from renewable generation plants, particularly those that are intermittent in nature. This has accentuated the need to find an efficient and scalable scheme for forecasting meteorological parameters, such as solar radiation, with better accuracy. For short-term solar irradiance forecasting, the traditional point forecasting methods are rendered less useful due to the non-stationary characteristic of solar power. The amount of operating reserves required to maintain reliable operation of the electric grid rises due to the variability of solar energy and the uncertainty of the forecasts. The ramp event caused by the variability of the solar resource magnifies the problem because the conventional large-generation plants cannot follow these rapid ramps. The higher the uncertainty in the generation, the greater the operating-reserve requirements, which translates to an increased cost of operation. In this research work, we propose a unified architecture for multi-timescale predictions for intra-day solar irradiance forecasting using recurrent neural networks (RNN) and long-short-term memory networks (LSTMs). This paper also lays out a framework for extending this modeling approach to intra-hour forecasting horizons thus, making it a multi-time-horizon forecasting approach, capable of predicting intra-hour as well as intra-day solar irradiance. We develop an end-to-end pipeline to effectuate the proposed architecture. The performance of the prediction model is tested and validated by the methodical implementation. The robustness of the approach is demonstrated with case studies conducted for geographically scattered sites across the United States. The predictions demonstrate that our proposed unified architecture-based approach is effective for multi-timescale solar forecasts and achieves a lower root-mean-square prediction error when benchmarked against the best-performing methods documented in the literature that use separate models for each timescale during the day. Our proposed method results in a 71.5% reduction in the mean RMSE averaged across all the test sites compared to the ML-based best-performing method reported in the literature. Additionally, the proposed method enables multi-time-horizon forecasts with real-time inputs, which have a significant potential for practical industry applications in the evolving grid.},
   author = {Sakshi Mishra and Praveen Palanisamy},
   journal = {arXiv preprint },
   keywords = {Deep Learning,LSTM,Multi-Time-Scale Solar Forecasting,Photovoltaic Prediction,Predictive Analytics,Smart Grid},
   title = {An Integrated Multi-Time-Scale Modeling for Solar Irradiance Forecasting Using Deep Learning},
   url = {https://www.rtoinsider.com/caiso-regulation-requirements-29299/},
   year = {2019},
}
@article{Yang2023,
   abstract = {On the one hand, grid integration of solar and wind power often requires just point (as opposed to probabilistic) forecasts at the individual plant level to be submitted to grid operators. On the other hand, solar and wind power forecasting can benefit greatly from dynamical ensemble forecasts from numerical weather prediction (NWP) models. Combining these two facts, this study is concerned with drawing out point forecasts from NWP ensembles. The scoring function for penalizing bad forecasts (or equivalently, rewarding good forecasts), in most scenarios, is specified by grid operators ex ante. The optimal point forecast therefore should be an elicitable functional of the predictive distribution, for which the specified scoring function is strictly consistent. Stated differently, the optimal way to summarize a predictive distribution depends on how the point forecast is to be penalized. Using solar irradiance forecasts issued by the ECMWF's Ensemble Prediction System, the statistical theory on consistency and elicitability is validated empirically with extensive data. The results show that the optimal point forecasts elicited from ensembles have constantly higher accuracy than the best-guess forecasts, regardless of the choice of scoring function. Surprisingly, however, the correspondence between the two types of goodness of forecasts, namely, quality and value, is neither linear nor monotone, but depends on the penalty triggers and schemes specified by grid operators. In other words, using the optimally elicited forecasts, in many scenarios, would lead to lower economic values.},
   author = {Dazhi Yang and Jan Kleissl},
   doi = {10.1016/j.ijforecast.2022.08.002},
   issn = {01692070},
   issue = {4},
   journal = {International Journal of Forecasting},
   keywords = {Consistency,ECMWF,Economic value,Ensemble NWP,Grid integration,Penalty scheme},
   month = {10},
   pages = {1640-1654},
   publisher = {Elsevier B.V.},
   title = {Summarizing ensemble NWP forecasts for grid operators: Consistency, elicitability, and economic value},
   volume = {39},
   year = {2023},
}
@misc{Wang2022,
   abstract = {In this paper, a comprehensive review is presented for mid-term load forecasting. The basic loads and effective factors are studied, and then several classifications are presented for forecasting approaches. The main advantages and drawbacks of the approaches are analyzed. The neuro-fuzzy-based approaches are investigated in more detail, and their limitations are studied. Finally, some aspects are presented in the use of neuro-fuzzy systems for load forecasting. The main contributions are that: (1) A comprehensive review is presented such that both classical methods and new neuro-fuzzy approaches are investigated. (2) The basic methods are studied in details, and their achievements and drawbacks are discussed. (3) Some models and suggestions are presented for future practical applications. (4) Some categories are introduced for better evaluation of various methods.},
   author = {Hong Wang and Khalid A. Alattas and Ardashir Mohammadzadeh and Mohammad Hosein Sabzalian and Ayman A. Aly and Amir Mosavi},
   doi = {10.1016/j.egyr.2022.10.016},
   issn = {23524847},
   journal = {Energy Reports},
   keywords = {Electrical load,Fuzzy systems,Learning algorithms,Load forecasting,Mid-term load forecasting,Neural networks},
   month = {11},
   pages = {13189-13198},
   publisher = {Elsevier Ltd},
   title = {Comprehensive review of load forecasting with emphasis on intelligent computing approaches},
   volume = {8},
   year = {2022},
}
@article{Xu2023,
   abstract = {Successful machine learning involves a complete pipeline of data, model, and downstream applications. Instead of treating them separately, there has been a prominent increase of attention within the constrained optimization (CO) and machine learning (ML) communities towards combining prediction and optimization models. The so-called end-to-end (E2E) learning captures the task-based objective for which they will be used for decision making. Although a large variety of E2E algorithms have been presented, it has not been fully investigated how to systematically address uncertainties involved in such models. Most of the existing work considers the uncertainties of ML in the input space and improves robustness through adversarial training. We extend this idea to E2E learning and prove that there is a robustness certification procedure by solving augmented integer programming. Furthermore, we show that neglecting the uncertainty of COs during training causes a new trigger for generalization errors. To include all these components, we propose a unified framework that covers the uncertainties emerging in both the input feature space of the ML models and the COs. The framework is described as a robust optimization problem and is practically solved via end-to-end adversarial training (E2E-AT). Finally, the performance of E2E-AT is evaluated by a real-world end-to-end power system operation problem, including load forecasting and sequential scheduling tasks.},
   author = {Wangkun Xu and Jianhong Wang and Fei Teng},
   month = {12},
   title = {E2E-AT: A Unified Framework for Tackling Uncertainty in Task-aware End-to-end Learning},
   url = {http://arxiv.org/abs/2312.10587},
   year = {2023},
}
@inproceedings{Gupta2021,
   abstract = {After the oil crisis in 1973, the planet has to believe in alternate sources of energy aside from conventional energy resources. Among different renewable resources, solar energy is one of the most famous and attractive suppliers of electric power generation due to zero emission of CO2. However, the forecasting of a solar component in advance is one of the crucial task due to the uncertain behavior of climatic parameters. This paper provides an analysis of the techniques used in the literature to forecast solar irradiance. The study's main goal is to see how meteorological input parameters, time horizons, pre-processing methodology, optimization, and sample size affect the model's complexity and accuracy. Different important parameters & findings of studies are presented in tabular mode. The paper provides key findings based on studied literature to select the optimal model for a specific site. This paper also discusses the metrics used to measure the efficiency of the forecasted model.},
   author = {Anuj Gupta and Kapil Gupta and Sumit Saroha},
   doi = {10.1016/j.matpr.2021.04.491},
   issn = {22147853},
   journal = {Materials Today: Proceedings},
   keywords = {Error Metrics,Forecasting techniques,Markov Chain,Neural network,Numerical Weather Prediction,Support vector machine},
   pages = {2420-2425},
   publisher = {Elsevier Ltd},
   title = {A review and evaluation of solar forecasting technologies},
   volume = {47},
   year = {2021},
}
@inproceedings{Bohara2022,
   abstract = {Higher penetration of renewable and smart home technologies at the residential level challenges grid stability as utility-customer interactions add complexity to power system operations. In response, short-term residential load forecasting has become an increasing area of focus. However, forecasting at the residential level is challenging due to the higher uncertainties involved. Recently deep neural networks have been leveraged to address this issue. This paper investigates the capabilities of a bidirectional long short-term memory (BiLSTM) and a convolutional neural network-based BiLSTM (CNN-BiLSTM) to provide a day ahead (24 hr.) forecasting at an hourly resolution while minimizing the root mean squared error (RMSE) between the actual and predicted load demand. Using a publicly available dataset consisting of 34 homes, the BiLSTM and CNN-BiLSTM models are trained to forecast the aggregated active power demand for each hour within a 24 hr. span, given the previous 24 hr. load data. The BiLSTM model achieved the lowest RMSE of 1.4842 for the overall daily forecast. In addition, standard LSTM and CNN-LSTM models are trained and compared with the BiLSTM architecture. The RMSE of BiLSTM is 5.60%, 2.85% and 2.60% lower than LSTM, CNN-LSTM and CNN-BiLSTM models respectively.},
   author = {Bharat Bohara and Raymond I. Fernandez and Vysali Gollapudi and Xingpeng Li},
   doi = {10.1109/3ICT56508.2022.9990696},
   isbn = {9781665451932},
   journal = {2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies, 3ICT 2022},
   keywords = {Bidirectional long short-term memory (BiLSTM),Deep Neural Networks (DNN),Energy Management,Load forecasting,Long short-term memory (LSTM),Optimal load dispatch},
   pages = {37-43},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Short-Term Aggregated Residential Load Forecasting using BiLSTM and CNN-BiLSTM},
   year = {2022},
}
@article{Sen2023,
   abstract = {Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Neural Network model integrated with different metaheuristic algorithms by their performance in Load forecasting based on numerical metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE). Our findings demonstrate the potential of metaheuristic-enhanced Transformer-based Neural Network models in Load forecasting accuracy and provide optimal hyperparameters for each model.},
   author = {Anuvab Sen and Arul Rhik Mazumder and Udayon Sen},
   month = {7},
   title = {Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting},
   url = {http://arxiv.org/abs/2307.15299},
   year = {2023},
}
@inproceedings{Nawar2023,
   abstract = {Precise load forecasting in buildings could increase the bill savings potential and facilitate optimized strategies for power generation planning. With the rapid evolution of computer science, data-driven techniques, in particular the Deep Learning models, have become a promising solution for the load forecasting problem. These models have showed accurate forecasting results; however, they need abundance amount of historical data to maintain the performance. Considering the new buildings and buildings with low resolution measuring equipment, it is difficult to get enough historical data from them, leading to poor forecasting performance. In order to adapt Deep Learning models for buildings with limited and scarce data, this paper proposes a Building-to-Building Transfer Learning framework to overcome the problem and enhance the performance of Deep Learning models. The transfer learning approach was applied to a new technique known as Transformer model due to its efficacy in capturing data trends. The performance of the algorithm was tested on a large commercial building with limited data. The result showed that the proposed approach improved the forecasting accuracy by 56.8% compared to the case of conventional deep learning where training from scratch is used. The paper also compared the proposed Transformer model to other sequential deep learning models such as Long-short Term Memory (LSTM) and Recurrent Neural Network (RNN). The accuracy of the transformer model outperformed other models by reducing the root mean square error to 0.009, compared to LSTM with 0.011 and RNN with 0.051.},
   author = {Menna Nawar and Moustafa Shomer and Samy Faddel and Huangjie Gong},
   journal = {SoutheastCon 2023},
   keywords = {Deep Learning,Load Forecasting,Sequential Models,Transfer Learning,Transformer},
   pages = {532-538},
   title = {Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data},
   year = {2023},
}
@article{Xu2023,
   abstract = {The forecast of electrical loads is essential for the planning and operation of the power system. Recently, advances in deep learning have enabled more accurate forecasts. However, deep neural networks are prone to adversarial attacks. Although most of the literature focuses on integrity-based attacks, this paper proposes availability-based adversarial attacks, which can be more easily implemented by attackers. For each forecast instance, the availability attack position is optimally solved by mixed-integer reformulation of the artificial neural network. To tackle this attack, an adversarial training algorithm is proposed. In simulation, a realistic load forecasting dataset is considered and the attack performance is compared to the integrity-based attack. Meanwhile, the adversarial training algorithm is shown to significantly improve robustness against availability attacks. All codes are available at https://github.com/xuwkk/AAA_Load_Forecast.},
   author = {Wangkun Xu and Fei Teng},
   month = {1},
   title = {Availability Adversarial Attack and Countermeasures for Deep Learning-based Load Forecasting},
   url = {http://arxiv.org/abs/2301.01832},
   year = {2023},
}
@article{Weber2021,
   abstract = {A cornerstone of the worldwide transition to smart grids are smart meters. Smart meters typically collect and provide energy time series that are vital for various applications, such as grid simulations, fault-detection, load forecasting, load analysis, and load management. Unfortunately, these time series are often characterized by missing values that must be handled before the data can be used. A common approach to handle missing values in time series is imputation. However, existing imputation methods are designed for power time series and do not take into account the total energy of gaps, resulting in jumps or constant shifts when imputing energy time series. In order to overcome these issues, the present paper introduces the new Copy-Paste Imputation (CPI) method for energy time series. The CPI method copies data blocks with similar properties and pastes them into gaps of the time series while preserving the total energy of each gap. The new method is evaluated on a real-world dataset that contains six shares of artificially inserted missing values between 1 and 30%. It outperforms by far the three benchmark imputation methods selected for comparison. The comparison furthermore shows that the CPI method uses matching patterns and preserves the total energy of each gap while requiring only a moderate run-time.},
   author = {Moritz Weber and Marian Turowski and Hüseyin K. Çakmak and Ralf Mikut and Uwe Kühnapfel and Veit Hagenmeyer},
   doi = {10.1109/TSG.2021.3101831},
   month = {1},
   title = {Data-Driven Copy-Paste Imputation for Energy Time Series},
   url = {http://arxiv.org/abs/2101.01423 http://dx.doi.org/10.1109/TSG.2021.3101831},
   year = {2021},
}
@article{Bean2022,
   abstract = {This report provides a description of the methodology I used in the IEEE-CIS 3rd Technical Challenge. For the forecast, I used a quantile regression forest approach using the solar variables provided by the Bureau of Meterology of Australia (BOM) and many of the weather variables from the European Centre for Medium-Range Weather Forecasting (ECMWF). Groups of buildings and all of the solar instances were trained together as they were observed to be closely correlated over time. Other variables used included Fourier values based on hour of day and day of year, and binary variables for combinations of days of the week. The start dates for the time series were carefully tuned based on phase 1 and cleaning and thresholding was used to reduce the observed error rate for each time series. For the optimization, a four-step approach was used using the forecast developed. First, a mixed-integer program (MIP) was solved for the recurring and recurring plus once-off activities, then each of these was extended using a mixed-integer quadratic program (MIQP). The general strategy was chosen from one of two ("array" from the "array" and "tuples" approaches) while the specific step improvement strategy was chosen from one of five ("no forced discharge").},
   author = {Richard Bean},
   month = {2},
   title = {Methodology for forecasting and optimization in IEEE-CIS 3rd Technical Challenge},
   url = {http://arxiv.org/abs/2202.00894},
   year = {2022},
}
@article{Wahab2020,
   abstract = {Electricity load forecasting enables the grid operators to optimally implement the smart grid's most essential features such as demand response and energy efficiency. Electricity demand profiles can vary drastically from one region to another on diurnal, seasonal and yearly scale. Hence to devise a load forecasting technique that can yield the best estimates on diverse datasets, specially when the training data is limited, is a big challenge. This paper presents a deep learning architecture for short-term load forecasting based on bidirectional sequential models in conjunction with feature engineering that extracts the hand-crafted derived features in order to aid the model for better learning and predictions. In the proposed architecture, named as Deep Derived Feature Fusion (DeepDeFF), the raw input and hand-crafted features are trained at separate levels and then their respective outputs are combined to make the final prediction. The efficacy of the proposed methodology is evaluated on datasets from five countries with completely different patterns. The results demonstrate that the proposed technique is superior to the existing state of the art.},
   author = {Abdul Wahab and Muhammad Anas Tahir and Naveed Iqbal and Faisal Shafait and Syed Muhammad Raza Kazmi},
   doi = {10.1109/ACCESS.2021.3093481},
   month = {11},
   title = {Short-Term Load Forecasting using Bi-directional Sequential Models and Feature Engineering for Small Datasets},
   url = {http://arxiv.org/abs/2011.14137 http://dx.doi.org/10.1109/ACCESS.2021.3093481},
   year = {2020},
}
@article{Oreshkin2020,
   abstract = {This paper addresses the mid-term electricity load forecasting problem. Solving this problem is necessary for power system operation and planning as well as for negotiating forward contracts in deregulated energy markets. We show that our proposed deep neural network modeling approach based on the deep neural architecture is effective at solving the mid-term electricity load forecasting problem. Proposed neural network has high expressive power to solve non-linear stochastic forecasting problems with time series including trends, seasonality and significant random fluctuations. At the same time, it is simple to implement and train, it does not require signal preprocessing, and it is equipped with a forecast bias reduction mechanism. We compare our approach against ten baseline methods, including classical statistical methods, machine learning and hybrid approaches, on 35 monthly electricity demand time series for European countries. The empirical study shows that proposed neural network clearly outperforms all competitors in terms of both accuracy and forecast bias. Code is available here: https://github.com/boreshkinai/nbeats-midterm.},
   author = {Boris N. Oreshkin and Grzegorz Dudek and Paweł Pełka and Ekaterina Turkina},
   month = {9},
   title = {N-BEATS neural network for mid-term electricity load forecasting},
   url = {http://arxiv.org/abs/2009.11961},
   year = {2020},
}
@article{,
   abstract = {A novel bioinspired metaheuristic is proposed in this work, simulating how the coronavirus spreads and infects healthy people. From an initial individual (the patient zero), the coronavirus infects new patients at known rates, creating new populations of infected people. Every individual can either die or infect and, afterwards, be sent to the recovered population. Relevant terms such as re-infection probability, super-spreading rate or traveling rate are introduced in the model in order to simulate as accurately as possible the coronavirus activity. The Coronavirus Optimization Algorithm has two major advantages compared to other similar strategies. First, the input parameters are already set according to the disease statistics, preventing researchers from initializing them with arbitrary values. Second, the approach has the ability of ending after several iterations, without setting this value either. Infected population initially grows at an exponential rate but after some iterations, when considering social isolation measures and the high number recovered and dead people, the number of infected people starts decreasing in subsequent iterations. Furthermore, a parallel multi-virus version is proposed in which several coronavirus strains evolve over time and explore wider search space areas in less iterations. Finally, the metaheuristic has been combined with deep learning models, in order to find optimal hyperparameters during the training phase. As application case, the problem of electricity load time series forecasting has been addressed, showing quite remarkable performance.},
   author = {F. Martínez-Álvarez and G. Asencio-Cortés and J. F. Torres and D. Gutiérrez-Avilés and L. Melgar-García and R. Pérez-Chacón and C. Rubio-Escudero and J. C. Riquelme and A. Troncoso},
   doi = {10.1089/big.2020.0051},
   month = {3},
   title = {Coronavirus Optimization Algorithm: A bioinspired metaheuristic based on the COVID-19 propagation model},
   url = {http://arxiv.org/abs/2003.13633 http://dx.doi.org/10.1089/big.2020.0051},
   year = {2020},
}
@article{Passalis2019,
   abstract = {Deep Learning (DL) models can be used to tackle time series analysis tasks with great success. However, the performance of DL models can degenerate rapidly if the data are not appropriately normalized. This issue is even more apparent when DL is used for financial time series forecasting tasks, where the non-stationary and multimodal nature of the data pose significant challenges and severely affect the performance of DL models. In this work, a simple, yet effective, neural layer, that is capable of adaptively normalizing the input time series, while taking into account the distribution of the data, is proposed. The proposed layer is trained in an end-to-end fashion using back-propagation and leads to significant performance improvements compared to other evaluated normalization schemes. The proposed method differs from traditional normalization methods since it learns how to perform normalization for a given task instead of using a fixed normalization scheme. At the same time, it can be directly applied to any new time series without requiring re-training. The effectiveness of the proposed method is demonstrated using a large-scale limit order book dataset, as well as a load forecasting dataset.},
   author = {Nikolaos Passalis and Anastasios Tefas and Juho Kanniainen and Moncef Gabbouj and Alexandros Iosifidis},
   month = {2},
   title = {Deep Adaptive Input Normalization for Time Series Forecasting},
   url = {http://arxiv.org/abs/1902.07892},
   year = {2019},
}
@article{Bose2023,
   abstract = {The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting (STLF) models. In response to privacy concerns, federated learning (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous. In this paper we alleviate this drawback using personalization layers, wherein certain layers of an STLF model in an FL framework are trained exclusively on the clients' own data. To that end, we propose a personalized FL algorithm (PL-FL) enabling FL to handle personalization layers. The PL-FL algorithm is implemented by using the Argonne Privacy-Preserving Federated Learning package. We test the forecast performance of models trained on the NREL ComStock dataset, which contains heterogeneous energy consumption data of multiple commercial buildings. Superior performance of models trained with PL-FL demonstrates that personalization layers enable classical FL algorithms to handle clients with heterogeneous data.},
   author = {Shourya Bose and Kibaek Kim},
   month = {9},
   title = {Federated Short-Term Load Forecasting with Personalization Layers for Heterogeneous Clients},
   url = {http://arxiv.org/abs/2309.13194},
   year = {2023},
}
@inproceedings{Gokhale2023,
   abstract = {Increasingly, homeowners opt for photovoltaic (PV) systems and/or battery storage to minimize their energy bills and maximize renewable energy usage. This has spurred the development of advanced control algorithms that maximally achieve those goals. However, a common challenge faced while developing such controllers is the unavailability of accurate forecasts of household power consumption, especially for shorter time resolutions (15 minutes) and in a data-efficient manner. In this paper, we analyze how transfer learning can help by exploiting data from multiple households to improve a single house's load forecasting. Specifically, we train an advanced forecasting model (a temporal fusion transformer) using data from multiple different households, and then finetune this global model on a new household with limited data (i.e., only a few days). The obtained models are used for forecasting power consumption of the household for the next 24 hours (day-ahead) at a time resolution of 15 minutes, with the intention of using these forecasts in advanced controllers such as Model Predictive Control. We show the benefit of this transfer learning setup versus solely using the individual new household's data, both in terms of using real-world household data.},
   author = {Gargya Gokhale and Jonas Van Gompel and Bert Claessens and Chris Develder},
   doi = {10.1145/3600100.3626635},
   isbn = {9798400702303},
   journal = {BuildSys 2023 - Proceedings of the10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
   keywords = {Demand Forecasting,Home Energy Management,Temporal Fusion Transformer,Transfer Learning},
   month = {11},
   pages = {458-462},
   publisher = {Association for Computing Machinery, Inc},
   title = {Transfer Learning in Transformer-Based Demand Forecasting For Home Energy Management System},
   year = {2023},
}
@article{Quansah2023,
   abstract = {Short-term load forecasting is of paramount importance in the efficient operation and planning of power systems, given its inherent non-linear and dynamic nature. Recent strides in deep learning have shown promise in addressing this challenge. However, these methods often grapple with hyperparameter sensitivity, opaqueness in interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that surmounts these obstacles. Our approach harnesses the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to discern the salient features crucial for accurate forecasting, and a streamlined framework for computational efficiency. Our method undergoes rigorous evaluation using a genuine electricity demand dataset. The results underscore its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error of 1.9376 marks a significant advancement over existing state-of-the-art approaches, heralding a new era in short-term load forecasting.},
   author = {Paapa Kwesi Quansah and Edwin Kwesi Ansah Tenkorang},
   month = {9},
   title = {Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network},
   url = {http://arxiv.org/abs/2309.03694},
   year = {2023},
}
@article{Giacomazzi2023,
   abstract = {Recent developments related to the energy transition pose particular challenges for distribution grids. Hence, precise load forecasts become more and more important for effective grid management. Novel modeling approaches such as the Transformer architecture, in particular the Temporal Fusion Transformer (TFT), have emerged as promising methods for time series forecasting. To date, just a handful of studies apply TFTs to electricity load forecasting problems, mostly considering only single datasets and a few covariates. Therefore, we examine the potential of the TFT architecture for hourly short-term load forecasting across different time horizons (day-ahead and week-ahead) and network levels (grid and substation level). We find that the TFT architecture does not offer higher predictive performance than a state-of-the-art LSTM model for day-ahead forecasting on the entire grid. However, the results display significant improvements for the TFT when applied at the substation level with a subsequent aggregation to the upper grid-level, resulting in a prediction error of 2.43% (MAPE) for the best-performing scenario. In addition, the TFT appears to offer remarkable improvements over the LSTM approach for week-ahead forecasting (yielding a predictive error of 2.52% (MAPE) at the lowest). We outline avenues for future research using the TFT approach for load forecasting, including the exploration of various grid levels (e.g., grid, substation, and household level).},
   author = {Elena Giacomazzi and Felix Haag and Konstantin Hopf},
   doi = {10.1145/10.1145/3575813.3597345},
   month = {5},
   title = {Short-Term Electricity Load Forecasting Using the Temporal Fusion Transformer: Effect of Grid Hierarchies and Data Sources},
   url = {http://arxiv.org/abs/2305.10559 http://dx.doi.org/10.1145/10.1145/3575813.3597345},
   year = {2023},
}
@article{Zhang2023,
   abstract = {Mid-term electricity load forecasting (LF) plays a critical role in power system planning and operation. To address the issue of error accumulation and transfer during the operation of existing LF models, a novel model called error correction based LF (EC LF) is proposed in this paper, which is designed to provide more accurate and stable LF. Firstly, time series analysis and feature engineering act on the original data to decompose load data into three components and extract relevant features. Then, based on the idea of stacking ensemble, long short-term memory is employed as an error correction module to forecast the components separately, and the forecast results are treated as new features to be fed into extreme gradient boosting for the second-step forecasting. Finally, the component sub-series forecast results are reconstructed to obtain the final LF results. The proposed model is evaluated on real-world electricity load data from two cities in China, and the experimental results demonstrate its superior performance compared to the other benchmark models.},
   author = {Liping Zhang and Di Wu and Xin Luo},
   journal = {arxiv preprint},
   keywords = {electricity load forecasting,error correction,extreme gradient boosting,long short-term memory,stacking,time series analysis},
   title = {An Error Correction Mid-term Electricity Load Forecasting Model Based on Seasonal Decomposition},
   year = {2023},
}
@article{Giese2022,
   abstract = {Load forecasts have become an integral part of energy security. Due to the various influencing factors that can be considered in such a forecast, there is also a wide range of models that attempt to integrate these parameters into a system in various ways. Due to the growing importance of probabilistic load forecast models, different approaches are presented in this analysis. The focus is on different models from the short-term sector. After that, another model from the long-term sector is presented. Then, the presented models are put in relation to each other and examined with reference to advantages and disadvantages. Afterwards, the presented papers are analyzed with focus on their comparability to each other. Finally, an outlook on further areas of development in the literature will be discussed.},
   author = {Philipp Giese},
   month = {10},
   title = {Probabilistic Forecasting Methods for System-Level Electricity Load Forecasting},
   url = {http://arxiv.org/abs/2210.09399},
   year = {2022},
}
@article{Ramachandran2022,
   abstract = {One of the primal challenges faced by utility companies is ensuring efficient supply with minimal greenhouse gas emissions. The advent of smart meters and smart grids provide an unprecedented advantage in realizing an optimised supply of thermal energies through proactive techniques such as load forecasting. In this paper, we propose a forecasting framework for heat demand based on neural networks where the time series are encoded as scalograms equipped with the capacity of embedding exogenous variables such as weather, and holiday/non-holiday. Subsequently, CNNs are utilized to predict the heat load multi-step ahead. Finally, the proposed framework is compared with other state-of-the-art methods, such as SARIMAX and LSTM. The quantitative results from retrospective experiments show that the proposed framework consistently outperforms the state-of-the-art baseline method with real-world data acquired from Denmark. A minimal mean error of 7.54% for MAPE and 417kW for RMSE is achieved with the proposed framework in comparison to all other methods.},
   author = {Adithya Ramachandran and Satyaki Chatterjee and Siming Bayer and Andreas Maier and Thorkil Flensmark},
   month = {10},
   title = {Heat Demand Forecasting with Multi-Resolutional Representation of Heterogeneous Temporal Ensemble},
   url = {http://arxiv.org/abs/2210.13108},
   year = {2022},
}
@article{Zelikman2020,
   abstract = {Advancing probabilistic solar forecasting methods is essential to supporting the integration of solar energy into the electricity grid. In this work, we develop a variety of state-of-the-art probabilistic models for forecasting solar irradiance. We investigate the use of post-hoc calibration techniques for ensuring well-calibrated probabilistic predictions. We train and evaluate the models using public data from seven stations in the SURFRAD network, and demonstrate that the best model, NGBoost, achieves higher performance at an intra-hourly resolution than the best benchmark solar irradiance forecasting model across all stations. Further, we show that NGBoost with CRUDE post-hoc calibration achieves comparable performance to a numerical weather prediction model on hourly-resolution forecasting.},
   author = {Eric Zelikman and Sharon Zhou and Jeremy Irvin and Cooper Raterink and Hao Sheng and Anand Avati and Jack Kelly and Ram Rajagopal and Andrew Y. Ng and David Gagne},
   month = {10},
   title = {Short-Term Solar Irradiance Forecasting Using Calibrated Probabilistic Models},
   url = {http://arxiv.org/abs/2010.04715},
   year = {2020},
}
@article{Sharma2021,
   abstract = {Amongst all the renewable energy resources (RES), solar is the most popular form of energy source and is of particular interest for its widely integration into the power grid. However, due to the intermittent nature of solar source, it is of the greatest significance to forecast solar irradiance to ensure uninterrupted and reliable power supply to serve the energy demand. There are several approaches to perform solar irradiance forecasting, for instance satellite-based methods, sky image-based methods, machine learning-based methods, and numerical weather prediction-based methods. In this paper, we present a review on short-term intra-hour solar prediction techniques known as nowcasting methods using sky images. Along with this, we also report and discuss which sky image features are significant for the nowcasting methods.},
   author = {Ekanki Sharma and Wilfried Elmenreich},
   month = {4},
   title = {A review on physical and data-driven based nowcasting methods using sky images},
   url = {http://arxiv.org/abs/2105.02959},
   year = {2021},
}
@article{Soleymani2023,
   abstract = {The increasing global demand for clean and environmentally friendly energy resources has caused increased interest in harnessing solar power through photovoltaic (PV) systems for smart grids and homes. However, the inherent unpredictability of PV generation poses problems associated with smart grid planning and management, energy trading and market participation, demand response, reliability, etc. Therefore, solar irradiance forecasting is essential for optimizing PV system utilization. This study proposes the next-generation machine learning algorithms such as random forests, Extreme Gradient Boosting (XGBoost), Light Gradient Boosted Machine (lightGBM) ensemble, CatBoost, and Multilayer Perceptron Artificial Neural Networks (MLP-ANNs) to forecast solar irradiance. Besides, Bayesian optimization is applied to hyperparameter tuning. Unlike tree-based ensemble algorithms that select the features intrinsically, MLP-ANN needs feature selection as a separate step. The simulation results indicate that the performance of the MLP-ANNs improves when feature selection is applied. Besides, the random forest outperforms the other learning algorithms.},
   author = {Saman Soleymani and Shima Mohammadzadeh},
   journal = {arxiv preprint},
   keywords = {Index Terms-Smart grids,feature selection,machine learning,solar irradiance forecasting},
   title = {Comparative Analysis of Machine Learning Algorithms for Solar Irradiance Forecasting in Smart Grids},
   year = {2023},
}
@article{Chauhan2022,
   abstract = {Concept bottleneck models (CBMs) are interpretable neural networks that first predict labels for human-interpretable concepts relevant to the prediction task, and then predict the final label based on the concept label predictions. We extend CBMs to interactive prediction settings where the model can query a human collaborator for the label to some concepts. We develop an interaction policy that, at prediction time, chooses which concepts to request a label for so as to maximally improve the final prediction. We demonstrate that a simple policy combining concept prediction uncertainty and influence of the concept on the final prediction achieves strong performance and outperforms static approaches as well as active feature acquisition methods proposed in the literature. We show that the interactive CBM can achieve accuracy gains of 5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD Birds, CheXpert and OAI datasets.},
   author = {Kushal Chauhan and Rishabh Tiwari and Jan Freyberg and Pradeep Shenoy and Krishnamurthy Dvijotham},
   month = {12},
   title = {Interactive Concept Bottleneck Models},
   url = {http://arxiv.org/abs/2212.07430},
   year = {2022},
}
@article{Ovadia2019,
   abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive \{\em uncertainty\}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
   author = {Yaniv Ovadia and Emily Fertig and Jie Ren and Zachary Nado and D Sculley and Sebastian Nowozin and Joshua V. Dillon and Balaji Lakshminarayanan and Jasper Snoek},
   month = {6},
   title = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift},
   url = {http://arxiv.org/abs/1906.02530},
   year = {2019},
}
@misc{,
   author = {Zhiyuan Chen and Bing Liu and Inc Bing Liu},
   title = {Lifelong Machine Learning Lifelong Machine Learning, Second Edition},
}
@article{Guo2019,
   abstract = {There is a growing interest in designing models that can deal with images from different visual domains. If there exists a universal structure in different visual domains that can be captured via a common parameterization, then we can use a single model for all domains rather than one model per domain. A model aware of the relationships between different domains can also be trained to work on new domains with less resources. However, to identify the reusable structure in a model is not easy. In this paper, we propose a multi-domain learning architecture based on depthwise separable convolution. The proposed approach is based on the assumption that images from different domains share cross-channel correlations but have domain-specific spatial correlations. The proposed model is compact and has minimal overhead when being applied to new domains. Additionally, we introduce a gating mechanism to promote soft sharing between different domains. We evaluate our approach on Visual Decathlon Challenge, a benchmark for testing the ability of multi-domain models. The experiments show that our approach can achieve the highest score while only requiring 50% of the parameters compared with the state-of-the-art approaches.},
   author = {Yunhui Guo and Yandong Li and Rogerio Feris and Liqiang Wang and Tajana Rosing},
   month = {2},
   title = {Depthwise Convolution is All You Need for Learning Multiple Visual Domains},
   url = {http://arxiv.org/abs/1902.00927},
   year = {2019},
}
@article{Qu2023,
   abstract = {Deep neural networks (DNNs) often perform poorly in the presence of domain shift and category shift. How to upcycle DNNs and adapt them to the target task remains an important open problem. Unsupervised Domain Adaptation (UDA), especially recently proposed Source-free Domain Adaptation (SFDA), has become a promising technology to address this issue. Nevertheless, existing SFDA methods require that the source domain and target domain share the same label space, consequently being only applicable to the vanilla closed-set setting. In this paper, we take one step further and explore the Source-free Universal Domain Adaptation (SF-UniDA). The goal is to identify "known" data samples under both domain and category shift, and reject those "unknown" data samples (not present in source classes), with only the knowledge from standard pre-trained source model. To this end, we introduce an innovative global and local clustering learning technique (GLC). Specifically, we design a novel, adaptive one-vs-all global clustering algorithm to achieve the distinction across different target classes and introduce a local k-NN clustering strategy to alleviate negative transfer. We examine the superiority of our GLC on multiple benchmarks with different category shift scenarios, including partial-set, open-set, and open-partial-set DA. Remarkably, in the most challenging open-partial-set DA scenario, GLC outperforms UMAD by 14.8\% on the VisDA benchmark. The code is available at https://github.com/ispc-lab/GLC.},
   author = {Sanqing Qu and Tianpei Zou and Florian Roehrbein and Cewu Lu and Guang Chen and Dacheng Tao and Changjun Jiang},
   month = {3},
   title = {Upcycling Models under Domain and Category Shift},
   url = {http://arxiv.org/abs/2303.07110},
   year = {2023},
}
@article{Ye2019,
   abstract = {Existing domain adaptation methods aim at learning features that can be generalized among domains. These methods commonly require to update source classifier to adapt to the target domain and do not properly handle the trade off between the source domain and the target domain. In this work, instead of training a classifier to adapt to the target domain, we use a separable component called data calibrator to help the fixed source classifier recover discrimination power in the target domain, while preserving the source domain's performance. When the difference between two domains is small, the source classifier's representation is sufficient to perform well in the target domain and outperforms GAN-based methods in digits. Otherwise, the proposed method can leverage synthetic images generated by GANs to boost performance and achieve state-of-the-art performance in digits datasets and driving scene semantic segmentation. Our method empirically reveals that certain intriguing hints, which can be mitigated by adversarial attack to domain discriminators, are one of the sources for performance degradation under the domain shift.},
   author = {Shaokai Ye and Kailu Wu and Mu Zhou and Yunfei Yang and Sia huat Tan and Kaidi Xu and Jiebo Song and Chenglong Bao and Kaisheng Ma},
   month = {11},
   title = {Light-weight Calibrator: a Separable Component for Unsupervised Domain Adaptation},
   url = {http://arxiv.org/abs/1911.12796},
   year = {2019},
}
@article{Wang2017,
   abstract = {Recent successes in learning-based image classification, however, heavily rely on the large number of annotated training samples, which may require considerable human efforts. In this paper, we propose a novel active learning framework, which is capable of building a competitive classifier with optimal feature representation via a limited amount of labeled training instances in an incremental learning manner. Our approach advances the existing active learning methods in two aspects. First, we incorporate deep convolutional neural networks into active learning. Through the properly designed framework, the feature representation and the classifier can be simultaneously updated with progressively annotated informative samples. Second, we present a cost-effective sample selection strategy to improve the classification performance with less manual annotations. Unlike traditional methods focusing on only the uncertain samples of low prediction confidence, we especially discover the large amount of high confidence samples from the unlabeled set for feature learning. Specifically, these high confidence samples are automatically selected and iteratively assigned pseudo-labels. We thus call our framework "Cost-Effective Active Learning" (CEAL) standing for the two advantages.Extensive experiments demonstrate that the proposed CEAL framework can achieve promising results on two challenging image classification datasets, i.e., face recognition on CACD database [1] and object categorization on Caltech-256 [2].},
   author = {Keze Wang and Dongyu Zhang and Ya Li and Ruimao Zhang and Liang Lin},
   doi = {10.1109/TCSVT.2016.2589879},
   month = {1},
   title = {Cost-Effective Active Learning for Deep Image Classification},
   url = {http://arxiv.org/abs/1701.03551 http://dx.doi.org/10.1109/TCSVT.2016.2589879},
   year = {2017},
}
@article{Wang2020,
   abstract = {Human intelligence is characterized not only by the capacity to learn complex skills, but the ability to rapidly adapt and acquire new skills within an ever-changing environment. In this work we study how the learning of modular solutions can allow for effective generalization to both unseen and potentially differently distributed data. Our main postulate is that the combination of task segmentation, modular learning and memory-based ensembling can give rise to generalization on an exponentially growing number of unseen tasks. We provide a concrete instantiation of this idea using a combination of: (1) the Forget-Me-Not Process, for task segmentation and memory based ensembling; and (2) Gated Linear Networks, which in contrast to contemporary deep learning techniques use a modular and local learning mechanism. We demonstrate that this system exhibits a number of desirable continual learning properties: robustness to catastrophic forgetting, no negative transfer and increasing levels of positive transfer as more tasks are seen. We show competitive performance against both offline and online methods on standard continual learning benchmarks.},
   author = {Jianan Wang and Eren Sezener and David Budden and Marcus Hutter and Joel Veness},
   month = {10},
   title = {A Combinatorial Perspective on Transfer Learning},
   url = {http://arxiv.org/abs/2010.12268},
   year = {2020},
}
@article{Dohare2023,
   abstract = {Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to learn on new examples, a phenomenon called loss of plasticity. We provide direct demonstrations of loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89\% accuracy on an early task down to 77\%, about the level of a linear network, on the 2000th task. Loss of plasticity occurred with a wide range of deep network architectures, optimizers, activation functions, batch normalization, dropout, but was substantially eased by $L^2$-regularization, particularly when combined with weight perturbation. Further, we introduce a new algorithm -- continual backpropagation -- which slightly modifies conventional backpropagation to reinitialize a small fraction of less-used units after each example and appears to maintain plasticity indefinitely.},
   author = {Shibhansh Dohare and J. Fernando Hernandez-Garcia and Parash Rahman and Richard S. Sutton and A. Rupam Mahmood},
   month = {6},
   title = {Loss of Plasticity in Deep Continual Learning},
   url = {http://arxiv.org/abs/2306.13812},
   year = {2023},
}
@article{Rusu2016,
   abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
   author = {Andrei A. Rusu and Neil C. Rabinowitz and Guillaume Desjardins and Hubert Soyer and James Kirkpatrick and Koray Kavukcuoglu and Razvan Pascanu and Raia Hadsell},
   month = {6},
   title = {Progressive Neural Networks},
   url = {http://arxiv.org/abs/1606.04671},
   year = {2016},
}
@article{Iyer2022,
   abstract = {A key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows: first, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results in both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.},
   author = {Abhiram Iyer and Karan Grewal and Akash Velu and Lucas Oliveira Souza and Jeremy Forest and Subutai Ahmad},
   doi = {10.3389/fnbot.2022.846219},
   issn = {16625218},
   journal = {Frontiers in Neurorobotics},
   keywords = {continual learning,dendrites,embodied cognition,neuroscience,reinforcement learning},
   month = {4},
   publisher = {Frontiers Media S.A.},
   title = {Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments},
   volume = {16},
   year = {2022},
}
@article{,
   abstract = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets.},
   author = {Johannes von Oswald and Christian Henning and Benjamin F. Grewe and João Sacramento},
   month = {6},
   title = {Continual learning with hypernetworks},
   url = {http://arxiv.org/abs/1906.00695},
   year = {2019},
}
@article{,
   abstract = {A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as "soft targets") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.},
   author = {Gido M. van de Ven and Andreas S. Tolias},
   month = {9},
   title = {Generative replay with feedback connections as a general strategy for continual learning},
   url = {http://arxiv.org/abs/1809.10635},
   year = {2018},
}
@article{Beaulieu2020,
   abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
   author = {Shawn Beaulieu and Lapo Frati and Thomas Miconi and Joel Lehman and Kenneth O. Stanley and Jeff Clune and Nick Cheney},
   month = {2},
   title = {Learning to Continually Learn},
   url = {http://arxiv.org/abs/2002.09571},
   year = {2020},
}
@article{Zhao2020,
   abstract = {As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.},
   author = {Bo Zhao and Konda Reddy Mopuri and Hakan Bilen},
   month = {6},
   title = {Dataset Condensation with Gradient Matching},
   url = {http://arxiv.org/abs/2006.05929},
   year = {2020},
}
@article{Pellegrini2019,
   abstract = {Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting before anything else. In this paper we introduce an original technique named "Latent Replay" where, instead of storing a portion of past data in the input space, we store activations volumes at some intermediate layer. This can significantly reduce the computation and storage required by native rehearsal. To keep the representation stable and the stored activations valid we propose to slow-down learning at all the layers below the latent replay one, leaving the layers above free to learn at full pace. In our experiments we show that Latent Replay, combined with existing continual learning techniques, achieves state-of-the-art performance on complex video benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d. batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly real-time continual learning on the edge through the deployment of the proposed technique on a smartphone device.},
   author = {Lorenzo Pellegrini and Gabriele Graffieti and Vincenzo Lomonaco and Davide Maltoni},
   month = {12},
   title = {Latent Replay for Real-Time Continual Learning},
   url = {http://arxiv.org/abs/1912.01100},
   year = {2019},
}
@article{Mundt2022,
   abstract = {Modern deep neural networks are well known to be brittle in the face of unknown data instances and recognition of the latter remains a challenge. Although it is inevitable for continual-learning systems to encounter such unseen concepts, the corresponding literature appears to nonethe-less focus primarily on alleviating catastrophic interference with learned representations. In this work, we introduce a probabilistic approach that connects these perspectives based on variational inference in a single deep autoencoder model. Specifically, we propose to bound the approximate posterior by fitting regions of high density on the basis of correctly classified data points. These bounds are shown to serve a dual purpose: unseen unknown out-of-distribution data can be distinguished from already trained known tasks towards robust application. Simultaneously, to retain already acquired knowledge, a generative replay process can be narrowed to strictly in-distribution samples, in order to significantly alleviate catastrophic interference.},
   author = {Martin Mundt and Iuliia Pliushch and Sagnik Majumder and Yongwon Hong and Visvanathan Ramesh},
   doi = {10.3390/jimaging8040093},
   issn = {2313433X},
   issue = {4},
   journal = {Journal of Imaging},
   keywords = {catastrophic forgetting,continual deep learning,deep generative models,open-set recognition,variational inference},
   month = {4},
   publisher = {MDPI},
   title = {Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition},
   volume = {8},
   year = {2022},
}
@article{Ehret2020,
   abstract = {While a diverse collection of continual learning (CL) methods has been proposed to prevent catastrophic forgetting, a thorough investigation of their effectiveness for processing sequential data with recurrent neural networks (RNNs) is lacking. Here, we provide the first comprehensive evaluation of established CL methods on a variety of sequential data benchmarks. Specifically, we shed light on the particularities that arise when applying weight-importance methods, such as elastic weight consolidation, to RNNs. In contrast to feedforward networks, RNNs iteratively reuse a shared set of weights and require working memory to process input samples. We show that the performance of weight-importance methods is not directly affected by the length of the processed sequences, but rather by high working memory requirements, which lead to an increased need for stability at the cost of decreased plasticity for learning subsequent tasks. We additionally provide theoretical arguments supporting this interpretation by studying linear RNNs. Our study shows that established CL methods can be successfully ported to the recurrent case, and that a recent regularization approach based on hypernetworks outperforms weight-importance methods, thus emerging as a promising candidate for CL in RNNs. Overall, we provide insights on the differences between CL in feedforward networks and RNNs, while guiding towards effective solutions to tackle CL on sequential data.},
   author = {Benjamin Ehret and Christian Henning and Maria R. Cervera and Alexander Meulemans and Johannes von Oswald and Benjamin F. Grewe},
   month = {6},
   title = {Continual Learning in Recurrent Neural Networks},
   url = {http://arxiv.org/abs/2006.12109},
   year = {2020},
}
@article{Lomonaco2017,
   abstract = {Continuous/Lifelong learning of high-dimensional data streams is a challenging research problem. In fact, fully retraining models each time new data become available is infeasible, due to computational and storage issues, while na\"ive incremental strategies have been shown to suffer from catastrophic forgetting. In the context of real-world object recognition applications (e.g., robotic vision), where continuous learning is crucial, very few datasets and benchmarks are available to evaluate and compare emerging techniques. In this work we propose a new dataset and benchmark CORe50, specifically designed for continuous object recognition, and introduce baseline approaches for different continuous learning scenarios.},
   author = {Vincenzo Lomonaco and Davide Maltoni},
   month = {5},
   title = {CORe50: a New Dataset and Benchmark for Continuous Object Recognition},
   url = {http://arxiv.org/abs/1705.03550},
   year = {2017},
}
@article{Zhan2020,
   abstract = {Joint clustering and feature learning methods have shown remarkable performance in unsupervised representation learning. However, the training schedule alternating between feature clustering and network parameters update leads to unstable learning of visual representations. To overcome this challenge, we propose Online Deep Clustering (ODC) that performs clustering and network update simultaneously rather than alternatingly. Our key insight is that the cluster centroids should evolve steadily in keeping the classifier stably updated. Specifically, we design and maintain two dynamic memory modules, i.e., samples memory to store samples labels and features, and centroids memory for centroids evolution. We break down the abrupt global clustering into steady memory update and batch-wise label re-assignment. The process is integrated into network update iterations. In this way, labels and the network evolve shoulder-to-shoulder rather than alternatingly. Extensive experiments demonstrate that ODC stabilizes the training process and boosts the performance effectively. Code: https://github.com/open-mmlab/OpenSelfSup.},
   author = {Xiaohang Zhan and Jiahao Xie and Ziwei Liu and Yew Soon Ong and Chen Change Loy},
   month = {6},
   title = {Online Deep Clustering for Unsupervised Representation Learning},
   url = {http://arxiv.org/abs/2006.10645},
   year = {2020},
}
@article{Nair2020,
   abstract = {Reinforcement learning (RL) provides an appealing formalism for learning control policies from experience. However, the classic active formulation of RL necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings such as robotic control. If we can instead allow RL algorithms to effectively use previously collected data to aid the online learning process, such applications could be made substantially more practical: the prior data would provide a starting point that mitigates challenges due to exploration and sample complexity, while the online training enables the agent to perfect the desired skill. Such prior data could either constitute expert demonstrations or sub-optimal prior data that illustrates potentially useful transitions. While a number of prior methods have either used optimal demonstrations to bootstrap RL, or have used sub-optimal data to train purely offline, it remains exceptionally difficult to train a policy with offline data and actually continue to improve it further with online RL. In this paper we analyze why this problem is so challenging, and propose an algorithm that combines sample efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of RL policies. We show that our method, advantage weighted actor critic (AWAC), enables rapid learning of skills with a combination of prior demonstration data and online experience. We demonstrate these benefits on simulated and real-world robotics domains, including dexterous manipulation with a real multi-fingered hand, drawer opening with a robotic arm, and rotating a valve. Our results show that incorporating prior data can reduce the time required to learn a range of robotic skills to practical time-scales.},
   author = {Ashvin Nair and Abhishek Gupta and Murtaza Dalal and Sergey Levine},
   month = {6},
   title = {AWAC: Accelerating Online Reinforcement Learning with Offline Datasets},
   url = {http://arxiv.org/abs/2006.09359},
   year = {2020},
}
@article{,
   abstract = {Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the results and basic experience replay, when properly tuned and implemented, is a very strong baseline. We release our modular and extensible codebase at https://github.com/AlbinSou/ocl_survey based on the avalanche framework to reproduce our results and encourage future research.},
   author = {Albin Soutif--Cormerais and Antonio Carta and Andrea Cossu and Julio Hurtado and Hamed Hemati and Vincenzo Lomonaco and Joost Van de Weijer},
   month = {8},
   title = {A Comprehensive Empirical Evaluation on Online Continual Learning},
   url = {http://arxiv.org/abs/2308.10328},
   year = {2023},
}
@article{Mei2023,
   abstract = {Second order stochastic optimizers allow parameter update step size and direction to adapt to loss curvature, but have traditionally required too much memory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018] introduced a Kronecker factored preconditioner to reduce these requirements: it is used for large deep models [Anil et al., 2020] and in production [Anil et al., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices. This requires 64-bit precision, imposing strong hardware constraints. In this paper, we propose a novel factorization, Kronecker Approximation-Domination (KrAD). Using KrAD, we update a matrix that directly approximates the inverse empirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and hence 64-bit precision. We then propose KrADagrad$^\star$, with similar computational costs to Shampoo and the same regret. Synthetic ill-conditioned experiments show improved performance over Shampoo for 32-bit precision, while for several real datasets we have comparable or better generalization.},
   author = {Jonathan Mei and Alexander Moreno and Luke Walters},
   month = {5},
   title = {KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned Stochastic Optimization},
   url = {http://arxiv.org/abs/2305.19416},
   year = {2023},
}
@article{Hihn2021,
   abstract = {One weakness of machine learning algorithms is the poor ability of models to solve new problems without forgetting previously acquired knowledge. The Continual Learning (CL) paradigm has emerged as a protocol to systematically investigate settings where the model sequentially observes samples generated by a series of tasks. In this work, we take a task-agnostic view of continual learning and develop a hierarchical information-theoretic optimality principle that facilitates a trade-off between learning and forgetting. We discuss this principle from a Bayesian perspective and show its connections to previous approaches to CL. Based on this principle, we propose a neural network layer, called the Mixture-of-Variational-Experts layer, that alleviates forgetting by creating a set of information processing paths through the network which is governed by a gating policy. Due to the general formulation based on generic utility functions, we can apply this optimality principle to a large variety of learning problems, including supervised learning, reinforcement learning, and generative modeling. We demonstrate the competitive performance of our method in continual supervised learning and in continual reinforcement learning.},
   author = {Heinke Hihn and Daniel A. Braun},
   month = {10},
   title = {Mixture-of-Variational-Experts for Continual Learning},
   url = {http://arxiv.org/abs/2110.12667},
   year = {2021},
}
@article{Lomonaco2019,
   abstract = {Robotic vision is a field where continual learning can play a significant role. An embodied agent operating in a complex environment subject to frequent and unpredictable changes is required to learn and adapt continuously. In the context of object recognition, for example, a robot should be able to learn (without forgetting) objects of never before seen classes as well as improving its recognition capabilities as new instances of already known classes are discovered. Ideally, continual learning should be triggered by the availability of short videos of single objects and performed on-line on on-board hardware with fine-grained updates. In this paper, we introduce a novel continual learning protocol based on the CORe50 benchmark and propose two rehearsal-free continual learning techniques, CWR* and AR1*, that can learn effectively even in the challenging case of nearly 400 small non-i.i.d. incremental batches. In particular, our experiments show that AR1* can outperform other state-of-the-art rehearsal-free techniques by more than 15% accuracy in some cases, with a very light and constant computational and memory overhead across training batches.},
   author = {Vincenzo Lomonaco and Davide Maltoni and Lorenzo Pellegrini},
   month = {7},
   title = {Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches},
   url = {http://arxiv.org/abs/1907.03799},
   year = {2019},
}
@article{Park2019,
   abstract = {Catastrophic forgetting is a critical challenge in training deep neural networks. Although continual learning has been investigated as a countermeasure to the problem, it often suffers from the requirements of additional network components and the limited scalability to a large number of tasks. We propose a novel approach to continual learning by approximating a true loss function using an asymmetric quadratic function with one of its sides overestimated. Our algorithm is motivated by the empirical observation that the network parameter updates affect the target loss functions asymmetrically. In the proposed continual learning framework, we estimate an asymmetric loss function for the tasks considered in the past through a proper overestimation of its unobserved sides in training new tasks, while deriving the accurate model parameter for the observable sides. In contrast to existing approaches, our method is free from the side effects and achieves the state-of-the-art accuracy that is even close to the upper-bound performance on several challenging benchmark datasets.},
   author = {Dongmin Park and Seokil Hong and Bohyung Han and Kyoung Mu Lee},
   month = {8},
   title = {Continual Learning by Asymmetric Loss Approximation with Single-Side Overestimation},
   url = {http://arxiv.org/abs/1908.02984},
   year = {2019},
}
@article{Bengio2012,
   abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
   author = {Yoshua Bengio and Aaron Courville and Pascal Vincent},
   month = {6},
   title = {Representation Learning: A Review and New Perspectives},
   url = {http://arxiv.org/abs/1206.5538},
   year = {2012},
}
@article{Gu2023,
   abstract = {Replay-based methods have proved their effectiveness on online continual learning by rehearsing past samples from an auxiliary memory. With many efforts made on improving training schemes based on the memory, however, the information carried by each sample in the memory remains under-investigated. Under circumstances with restricted storage space, the informativeness of the memory becomes critical for effective replay. Although some works design specific strategies to select representative samples, by only employing a small number of original images, the storage space is still not well utilized. To this end, we propose to Summarize the knowledge from the Stream Data (SSD) into more informative samples by distilling the training characteristics of real images. Through maintaining the consistency of training gradients and relationship to the past tasks, the summarized samples are more representative for the stream data compared to the original images. Extensive experiments are conducted on multiple online continual learning benchmarks to support that the proposed SSD method significantly enhances the replay effects. We demonstrate that with limited extra computational overhead, SSD provides more than 3% accuracy boost for sequential CIFAR-100 under extremely restricted memory buffer. Code in https://github.com/vimar-gu/SSD.},
   author = {Jianyang Gu and Kai Wang and Wei Jiang and Yang You},
   month = {5},
   title = {Summarizing Stream Data for Memory-Constrained Online Continual Learning},
   url = {http://arxiv.org/abs/2305.16645},
   year = {2023},
}
@article{Nguyen2017,
   abstract = {This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.},
   author = {Cuong V. Nguyen and Yingzhen Li and Thang D. Bui and Richard E. Turner},
   month = {10},
   title = {Variational Continual Learning},
   url = {http://arxiv.org/abs/1710.10628},
   year = {2017},
}
@article{Shim2020,
   abstract = {As image-based deep learning becomes pervasive on every device, from cell phones to smart watches, there is a growing need to develop methods that continually learn from data while minimizing memory footprint and power consumption. While memory replay techniques have shown exceptional promise for this task of continual learning, the best method for selecting which buffered images to replay is still an open question. In this paper, we specifically focus on the online class-incremental setting where a model needs to learn new classes continually from an online data stream. To this end, we contribute a novel Adversarial Shapley value scoring method that scores memory data samples according to their ability to preserve latent decision boundaries for previously observed classes (to maintain learning stability and avoid forgetting) while interfering with latent decision boundaries of current classes being learned (to encourage plasticity and optimal learning of new class boundaries). Overall, we observe that our proposed ASER method provides competitive or improved performance compared to state-of-the-art replay-based continual learning methods on a variety of datasets.},
   author = {Dongsub Shim and Zheda Mai and Jihwan Jeong and Scott Sanner and Hyunwoo Kim and Jongseong Jang},
   month = {8},
   title = {Online Class-Incremental Continual Learning with Adversarial Shapley Value},
   url = {http://arxiv.org/abs/2009.00093},
   year = {2020},
}
@article{Wu2019,
   abstract = {Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1% and 13.2% respectively.},
   author = {Yue Wu and Yinpeng Chen and Lijuan Wang and Yuancheng Ye and Zicheng Liu and Yandong Guo and Yun Fu},
   month = {5},
   title = {Large Scale Incremental Learning},
   url = {http://arxiv.org/abs/1905.13260},
   year = {2019},
}
@article{Yao2022,
   abstract = {Distribution shift occurs when the test distribution differs from the training distribution, and it can considerably degrade performance of machine learning models deployed in the real world. Temporal shifts -- distribution shifts arising from the passage of time -- often occur gradually and have the additional structure of timestamp metadata. By leveraging timestamp metadata, models can potentially learn from trends in past distribution shifts and extrapolate into the future. While recent works have studied distribution shifts, temporal shifts remain underexplored. To address this gap, we curate Wild-Time, a benchmark of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including patient prognosis and news classification. On these datasets, we systematically benchmark 13 prior approaches, including methods in domain generalization, continual learning, self-supervised learning, and ensemble learning. We use two evaluation strategies: evaluation with a fixed time split (Eval-Fix) and evaluation with a data stream (Eval-Stream). Eval-Fix, our primary evaluation strategy, aims to provide a simple evaluation protocol, while Eval-Stream is more realistic for certain real-world applications. Under both evaluation strategies, we observe an average performance drop of 20% from in-distribution to out-of-distribution data. Existing methods are unable to close this gap. Code is available at https://wild-time.github.io/.},
   author = {Huaxiu Yao and Caroline Choi and Bochuan Cao and Yoonho Lee and Pang Wei Koh and Chelsea Finn},
   month = {11},
   title = {Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time},
   url = {http://arxiv.org/abs/2211.14238},
   year = {2022},
}
@article{Zhao2021,
   abstract = {Computational cost of training state-of-the-art deep models in many learning problems is rapidly increasing due to more sophisticated models and larger datasets. A recent promising direction for reducing training cost is dataset condensation that aims to replace the original large training set with a significantly smaller learned synthetic set while preserving the original information. While training deep models on the small set of condensed images can be extremely fast, their synthesis remains computationally expensive due to the complex bi-level optimization and second-order derivative computation. In this work, we propose a simple yet effective method that synthesizes condensed images by matching feature distributions of the synthetic and original training images in many sampled embedding spaces. Our method significantly reduces the synthesis cost while achieving comparable or better performance. Thanks to its efficiency, we apply our method to more realistic and larger datasets with sophisticated neural architectures and obtain a significant performance boost. We also show promising practical benefits of our method in continual learning and neural architecture search.},
   author = {Bo Zhao and Hakan Bilen},
   month = {10},
   title = {Dataset Condensation with Distribution Matching},
   url = {http://arxiv.org/abs/2110.04181},
   year = {2021},
}
@article{Sokar2021,
   abstract = {Continual learning aims to provide intelligent agents capable of learning multiple tasks sequentially with neural networks. One of its main challenging, catastrophic forgetting, is caused by the neural networks non-optimal ability to learn in non-stationary distributions. In most settings of the current approaches, the agent starts from randomly initialized parameters and is optimized to master the current task regardless of the usefulness of the learned representation for future tasks. Moreover, each of the future tasks uses all the previously learned knowledge although parts of this knowledge might not be helpful for its learning. These cause interference among tasks, especially when the data of previous tasks is not accessible. In this paper, we propose a new method, named Self-Attention Meta-Learner (SAM), which learns a prior knowledge for continual learning that permits learning a sequence of tasks, while avoiding catastrophic forgetting. SAM incorporates an attention mechanism that learns to select the particular relevant representation for each future task. Each task builds a specific representation branch on top of the selected knowledge, avoiding the interference between tasks. We evaluate the proposed method on the Split CIFAR-10/100 and Split MNIST benchmarks in the task agnostic inference. We empirically show that we can achieve a better performance than several state-of-the-art methods for continual learning by building on the top of selected representation learned by SAM. We also show the role of the meta-attention mechanism in boosting informative features corresponding to the input data and identifying the correct target in the task agnostic inference. Finally, we demonstrate that popular existing continual learning methods gain a performance boost when they adopt SAM as a starting point.},
   author = {Ghada Sokar and Decebal Constantin Mocanu and Mykola Pechenizkiy},
   month = {1},
   title = {Self-Attention Meta-Learner for Continual Learning},
   url = {http://arxiv.org/abs/2101.12136},
   year = {2021},
}
@misc{Parisi2019,
   abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
   author = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
   doi = {10.1016/j.neunet.2019.01.012},
   issn = {18792782},
   journal = {Neural Networks},
   keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation},
   month = {5},
   pages = {54-71},
   pmid = {30780045},
   publisher = {Elsevier Ltd},
   title = {Continual lifelong learning with neural networks: A review},
   volume = {113},
   year = {2019},
}
@article{Veniat2020,
   abstract = {Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work.},
   author = {Tom Veniat and Ludovic Denoyer and Marc'Aurelio Ranzato},
   month = {12},
   title = {Efficient Continual Learning with Modular Networks and Task-Driven Priors},
   url = {http://arxiv.org/abs/2012.12631},
   year = {2020},
}
@article{Yun2019,
   abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .},
   author = {Sangdoo Yun and Dongyoon Han and Seong Joon Oh and Sanghyuk Chun and Junsuk Choe and Youngjoon Yoo},
   month = {5},
   title = {CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features},
   url = {http://arxiv.org/abs/1905.04899},
   year = {2019},
}
@article{Hsu2018,
   abstract = {Continual learning has received a great deal of attention recently with several approaches being proposed. However, evaluations involve a diverse set of scenarios making meaningful comparison difficult. This work provides a systematic categorization of the scenarios and evaluates them within a consistent framework including strong baselines and state-of-the-art methods. The results provide an understanding of the relative difficulty of the scenarios and that simple baselines (Adagrad, L2 regularization, and naive rehearsal strategies) can surprisingly achieve similar performance to current mainstream methods. We conclude with several suggestions for creating harder evaluation scenarios and future research directions. The code is available at https://github.com/GT-RIPL/Continual-Learning-Benchmark},
   author = {Yen-Chang Hsu and Yen-Cheng Liu and Anita Ramasamy and Zsolt Kira},
   month = {10},
   title = {Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines},
   url = {http://arxiv.org/abs/1810.12488},
   year = {2018},
}
@article{French2017,
   abstract = {This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.},
   author = {Geoffrey French and Michal Mackiewicz and Mark Fisher},
   month = {6},
   title = {Self-ensembling for visual domain adaptation},
   url = {http://arxiv.org/abs/1706.05208},
   year = {2017},
}
@article{Kirkpatrick2016,
   abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
   author = {James Kirkpatrick and Razvan Pascanu and Neil Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},
   doi = {10.1073/pnas.1611835114},
   month = {12},
   title = {Overcoming catastrophic forgetting in neural networks},
   url = {http://arxiv.org/abs/1612.00796 http://dx.doi.org/10.1073/pnas.1611835114},
   year = {2016},
}
@article{Schwarz2021,
   abstract = {The training of sparse neural networks is becoming an increasingly important tool for reducing the computational footprint of models at training and evaluation, as well enabling the effective scaling up of models. Whereas much work over the years has been dedicated to specialised pruning techniques, little attention has been paid to the inherent effect of gradient based training on model sparsity. In this work, we introduce Powerpropagation, a new weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, our method gives rise to weight updates exhibiting a "rich get richer" dynamic, leaving low-magnitude parameters largely unaffected by learning. Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely. Powerpropagation is general, intuitive, cheap and straight-forward to implement and can readily be combined with various other techniques. To highlight its versatility, we explore it in two very different settings: Firstly, following a recent line of work, we investigate its effect on sparse training for resource-constrained settings. Here, we combine Powerpropagation with a traditional weight-pruning technique as well as recent state-of-the-art sparse-to-sparse algorithms, showing superior performance on the ImageNet benchmark. Secondly, we advocate the use of sparsity in overcoming catastrophic forgetting, where compressed representations allow accommodating a large number of tasks at fixed model capacity. In all cases our reparameterisation considerably increases the efficacy of the off-the-shelf methods.},
   author = {Jonathan Schwarz and Siddhant M. Jayakumar and Razvan Pascanu and Peter E. Latham and Yee Whye Teh},
   month = {10},
   title = {Powerpropagation: A sparsity inducing weight reparameterisation},
   url = {http://arxiv.org/abs/2110.00296},
   year = {2021},
}
@article{Castro2018,
   abstract = {Although deep learning approaches have stood out in recent years due to their state-of-the-art results, they continue to suffer from catastrophic forgetting, a dramatic decrease in overall performance when training with new classes added incrementally. This is due to current neural network architectures requiring the entire dataset, consisting of all the samples from the old as well as the new classes, to update the model -a requirement that becomes easily unsustainable as the number of classes grows. We address this issue with our approach to learn deep neural networks incrementally, using new data and only a small exemplar set corresponding to samples from the old classes. This is based on a loss composed of a distillation measure to retain the knowledge acquired from the old classes, and a cross-entropy loss to learn the new classes. Our incremental training is achieved while keeping the entire framework end-to-end, i.e., learning the data representation and the classifier jointly, unlike recent methods with no such guarantees. We evaluate our method extensively on the CIFAR-100 and ImageNet (ILSVRC 2012) image classification datasets, and show state-of-the-art performance.},
   author = {Francisco M. Castro and Manuel J. Marín-Jiménez and Nicolás Guil and Cordelia Schmid and Karteek Alahari},
   month = {7},
   title = {End-to-End Incremental Learning},
   url = {http://arxiv.org/abs/1807.09536},
   year = {2018},
}
@article{Javed2019,
   abstract = {A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite---they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. We release an implementation of our method at https://github.com/khurramjaved96/mrcl .},
   author = {Khurram Javed and Martha White},
   month = {5},
   title = {Meta-Learning Representations for Continual Learning},
   url = {http://arxiv.org/abs/1905.12588},
   year = {2019},
}
@article{,
   abstract = {Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and--in case it is not--whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.},
   author = {Gido M. van de Ven and Andreas S. Tolias},
   month = {4},
   title = {Three scenarios for continual learning},
   url = {http://arxiv.org/abs/1904.07734},
   year = {2019},
}
@article{Zenke2017,
   abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.},
   author = {Friedemann Zenke and Ben Poole and Surya Ganguli},
   month = {3},
   title = {Continual Learning Through Synaptic Intelligence},
   url = {http://arxiv.org/abs/1703.04200},
   year = {2017},
}
@article{Morgado2019,
   abstract = {Real-world applications of object recognition often require the solution of multiple tasks in a single platform. Under the standard paradigm of network fine-tuning, an entirely new CNN is learned per task, and the final network size is independent of task complexity. This is wasteful, since simple tasks require smaller networks than more complex tasks, and limits the number of tasks that can be solved simultaneously. To address these problems, we propose a transfer learning procedure, denoted NetTailor, in which layers of a pre-trained CNN are used as universal blocks that can be combined with small task-specific layers to generate new networks. Besides minimizing classification error, the new network is trained to mimic the internal activations of a strong unconstrained CNN, and minimize its complexity by the combination of 1) a soft-attention mechanism over blocks and 2) complexity regularization constraints. In this way, NetTailor can adapt the network architecture, not just its weights, to the target task. Experiments show that networks adapted to simple tasks, such as character or traffic sign recognition, become significantly smaller than those adapted to hard tasks, such as fine-grained recognition. More importantly, due to the modular nature of the procedure, this reduction in network complexity is achieved without compromise of either parameter sharing across tasks, or classification accuracy.},
   author = {Pedro Morgado and Nuno Vasconcelos},
   month = {6},
   title = {NetTailor: Tuning the Architecture, Not Just the Weights},
   url = {http://arxiv.org/abs/1907.00274},
   year = {2019},
}
@article{,
   abstract = {Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and--in case it is not--whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.},
   author = {Gido M. van de Ven and Andreas S. Tolias},
   month = {4},
   title = {Three scenarios for continual learning},
   url = {http://arxiv.org/abs/1904.07734},
   year = {2019},
}
@article{Zhang2023,
   abstract = {Offline-to-online Reinforcement Learning (O2O RL) aims to improve the performance of offline pretrained policy using only a few online samples. Built on offline RL algorithms, most O2O methods focus on the balance between RL objective and pessimism, or the utilization of offline and online samples. In this paper, from a novel perspective, we systematically study the challenges that remain in O2O RL and identify that the reason behind the slow improvement of the performance and the instability of online finetuning lies in the inaccurate Q-value estimation inherited from offline pretraining. Specifically, we demonstrate that the estimation bias and the inaccurate rank of Q-value cause a misleading signal for the policy update, making the standard offline RL algorithms, such as CQL and TD3-BC, ineffective in the online finetuning. Based on this observation, we address the problem of Q-value estimation by two techniques: (1) perturbed value update and (2) increased frequency of Q-value updates. The first technique smooths out biased Q-value estimation with sharp peaks, preventing early-stage policy exploitation of sub-optimal actions. The second one alleviates the estimation bias inherited from offline pretraining by accelerating learning. Extensive experiments on the MuJoco and Adroit environments demonstrate that the proposed method, named SO2, significantly alleviates Q-value estimation issues, and consistently improves the performance against the state-of-the-art methods by up to 83.1%.},
   author = {Yinmin Zhang and Jie Liu and Chuming Li and Yazhe Niu and Yaodong Yang and Yu Liu and Wanli Ouyang},
   month = {12},
   title = {A Perspective of Q-value Estimation on Offline-to-Online Reinforcement Learning},
   url = {http://arxiv.org/abs/2312.07685},
   year = {2023},
}
@article{Mallya2018,
   abstract = {This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that piggyback on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Unlike prior work, we do not suffer from catastrophic forgetting or competition between tasks, and our performance is agnostic to task ordering. Code available at https://github.com/arunmallya/piggyback.},
   author = {Arun Mallya and Dillon Davis and Svetlana Lazebnik},
   month = {1},
   title = {Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights},
   url = {http://arxiv.org/abs/1801.06519},
   year = {2018},
}
@article{Li2016,
   abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
   author = {Zhizhong Li and Derek Hoiem},
   month = {6},
   title = {Learning without Forgetting},
   url = {http://arxiv.org/abs/1606.09282},
   year = {2016},
}
@article{Mirzadeh2020,
   abstract = {Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks. This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with different degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes -- learning rate, batch size, regularization method-- can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks' local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.},
   author = {Seyed Iman Mirzadeh and Mehrdad Farajtabar and Razvan Pascanu and Hassan Ghasemzadeh},
   month = {6},
   title = {Understanding the Role of Training Regimes in Continual Learning},
   url = {http://arxiv.org/abs/2006.06958},
   year = {2020},
}
@article{Sokar2021,
   abstract = {Continual learning aims to provide intelligent agents that are capable of learning continually a sequence of tasks, building on previously learned knowledge. A key challenge in this learning paradigm is catastrophically forgetting previously learned tasks when the agent faces a new one. Current rehearsal-based methods show their success in mitigating the catastrophic forgetting problem by replaying samples from previous tasks during learning a new one. However, these methods are infeasible when the data of previous tasks is not accessible. In this work, we propose a new pseudo-rehearsal-based method, named learning Invariant Representation for Continual Learning (IRCL), in which class-invariant representation is disentangled from a conditional generative model and jointly used with class-specific representation to learn the sequential tasks. Disentangling the shared invariant representation helps to learn continually a sequence of tasks, while being more robust to forgetting and having better knowledge transfer. We focus on class incremental learning where there is no knowledge about task identity during inference. We empirically evaluate our proposed method on two well-known benchmarks for continual learning: split MNIST and split Fashion MNIST. The experimental results show that our proposed method outperforms regularization-based methods by a big margin and is better than the state-of-the-art pseudo-rehearsal-based method. Finally, we analyze the role of the shared invariant representation in mitigating the forgetting problem especially when the number of replayed samples for each previous task is small.},
   author = {Ghada Sokar and Decebal Constantin Mocanu and Mykola Pechenizkiy},
   month = {1},
   title = {Learning Invariant Representation for Continual Learning},
   url = {http://arxiv.org/abs/2101.06162},
   year = {2021},
}
@article{Liang2020,
   abstract = {Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named \emph\{Source HypOthesis Transfer\} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.},
   author = {Jian Liang and Dapeng Hu and Jiashi Feng},
   month = {2},
   title = {Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation},
   url = {http://arxiv.org/abs/2002.08546},
   year = {2020},
}
@article{Farquhar2018,
   abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
   author = {Sebastian Farquhar and Yarin Gal},
   month = {5},
   title = {Towards Robust Evaluations of Continual Learning},
   url = {http://arxiv.org/abs/1805.09733},
   year = {2018},
}
@article{Goodfellow2013,
   abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
   author = {Ian J. Goodfellow and Mehdi Mirza and Da Xiao and Aaron Courville and Yoshua Bengio},
   month = {12},
   title = {An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks},
   url = {http://arxiv.org/abs/1312.6211},
   year = {2013},
}
@article{Perozzi2014,
   abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
   author = {Bryan Perozzi and Rami Al-Rfou and Steven Skiena},
   doi = {10.1145/2623330.2623732},
   month = {3},
   title = {DeepWalk: Online Learning of Social Representations},
   url = {http://arxiv.org/abs/1403.6652 http://dx.doi.org/10.1145/2623330.2623732},
   year = {2014},
}
@article{Lomonaco2021,
   abstract = {Learning continually from non-stationary data streams is a long-standing goal and a challenging problem in machine learning. Recently, we have witnessed a renewed and fast-growing interest in continual learning, especially within the deep learning community. However, algorithmic solutions are often difficult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose Avalanche, an open-source end-to-end library for continual learning research based on PyTorch. Avalanche is designed to provide a shared and collaborative codebase for fast prototyping, training, and reproducible evaluation of continual learning algorithms.},
   author = {Vincenzo Lomonaco and Lorenzo Pellegrini and Andrea Cossu and Antonio Carta and Gabriele Graffieti and Tyler L. Hayes and Matthias De Lange and Marc Masana and Jary Pomponi and Gido van de Ven and Martin Mundt and Qi She and Keiland Cooper and Jeremy Forest and Eden Belouadah and Simone Calderara and German I. Parisi and Fabio Cuzzolin and Andreas Tolias and Simone Scardapane and Luca Antiga and Subutai Amhad and Adrian Popescu and Christopher Kanan and Joost van de Weijer and Tinne Tuytelaars and Davide Bacciu and Davide Maltoni},
   month = {4},
   title = {Avalanche: an End-to-End Library for Continual Learning},
   url = {http://arxiv.org/abs/2104.00405},
   year = {2021},
}
@article{Aljundi2019,
   abstract = {A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer. In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is to select a fixed subset of constraints that best approximate the feasible region defined by the original constraints. We show that it is equivalent to maximizing the diversity of samples in the replay buffer with parameters gradient as the feature. We further develop a greedy alternative that is cheap and efficient. The advantage of the proposed method is demonstrated by comparing to other alternatives under the continual learning setting. Further comparisons are made against state of the art methods that rely on task boundaries which show comparable or even better results for our method.},
   author = {Rahaf Aljundi and Min Lin and Baptiste Goujaud and Yoshua Bengio},
   month = {3},
   title = {Gradient based sample selection for online continual learning},
   url = {http://arxiv.org/abs/1903.08671},
   year = {2019},
}
@article{Chaudhry2019,
   abstract = {In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, significantly outperforms specifically designed CL approaches with and without episodic memory. Interestingly, we find that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7\% and 17\% when the memory is populated with a single example per class.},
   author = {Arslan Chaudhry and Marcus Rohrbach and Mohamed Elhoseiny and Thalaiyasingam Ajanthan and Puneet K. Dokania and Philip H. S. Torr and Marc'Aurelio Ranzato},
   month = {2},
   title = {On Tiny Episodic Memories in Continual Learning},
   url = {http://arxiv.org/abs/1902.10486},
   year = {2019},
}
@article{Rebuffi2018,
   abstract = {A practical limitation of deep neural networks is their high degree of specialization to a single task and visual domain. Recently, inspired by the successes of transfer learning, several authors have proposed to learn instead universal, fixed feature extractors that, used as the first stage of any deep network, work well for several tasks and domains simultaneously. Nevertheless, such universal features are still somewhat inferior to specialized networks. To overcome this limitation, in this paper we propose to consider instead universal parametric families of neural networks, which still contain specialized problem-specific models, but differing only by a small number of parameters. We study different designs for such parametrizations, including series and parallel residual adapters, joint adapter compression, and parameter allocations, and empirically identify the ones that yield the highest compression. We show that, in order to maximize performance, it is necessary to adapt both shallow and deep layers of a deep network, but the required changes are very small. We also show that these universal parametrization are very effective for transfer learning, where they outperform traditional fine-tuning techniques.},
   author = {Sylvestre-Alvise Rebuffi and Hakan Bilen and Andrea Vedaldi},
   month = {3},
   title = {Efficient parametrization of multi-domain deep neural networks},
   url = {http://arxiv.org/abs/1803.10082},
   year = {2018},
}
@article{,
   abstract = {Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streaming data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space during the learning process. Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. As an additional contribution, we generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework. We obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.},
   author = {Matthias De Lange and Tinne Tuytelaars},
   month = {9},
   title = {Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},
   url = {http://arxiv.org/abs/2009.00919},
   year = {2020},
}
@article{Zhang2020,
   abstract = {It is well known that featuremap attention and multi-path representation are important for visual recognition. In this paper, we present a modularized architecture, which applies the channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. Our design results in a simple and unified computation block, which can be parameterized using only a few variables. Our model, named ResNeSt, outperforms EfficientNet in accuracy and latency trade-off on image classification. In addition, ResNeSt has achieved superior transfer learning results on several public benchmarks serving as the backbone, and has been adopted by the winning entries of COCO-LVIS challenge. The source code for complete system and pretrained models are publicly available.},
   author = {Hang Zhang and Chongruo Wu and Zhongyue Zhang and Yi Zhu and Haibin Lin and Zhi Zhang and Yue Sun and Tong He and Jonas Mueller and R. Manmatha and Mu Li and Alexander Smola},
   month = {4},
   title = {ResNeSt: Split-Attention Networks},
   url = {http://arxiv.org/abs/2004.08955},
   year = {2020},
}
@article{Hochreiter1997,
   abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
   author = {Sepp Hochreiter and Jürgen Schmidhuber},
   doi = {10.1162/neco.1997.9.8.1735},
   issn = {08997667},
   issue = {8},
   journal = {Neural Computation},
   month = {11},
   pages = {1735-1780},
   pmid = {9377276},
   publisher = {MIT Press Journals},
   title = {Long Short-Term Memory},
   volume = {9},
   year = {1997},
}
@article{Rebuffi2017,
   abstract = {There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.},
   author = {Sylvestre-Alvise Rebuffi and Hakan Bilen and Andrea Vedaldi},
   month = {5},
   title = {Learning multiple visual domains with residual adapters},
   url = {http://arxiv.org/abs/1705.08045},
   year = {2017},
}
@article{,
   abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
   author = {David Lopez-Paz and Marc'Aurelio Ranzato},
   month = {6},
   title = {Gradient Episodic Memory for Continual Learning},
   url = {http://arxiv.org/abs/1706.08840},
   year = {2017},
}
@article{Farahani2020,
   abstract = {Classical machine learning assumes that the training and test sets come from the same distributions. Therefore, a model learned from the labeled training data is expected to perform well on the test data. However, This assumption may not always hold in real-world applications where the training and the test data fall from different distributions, due to many factors, e.g., collecting the training and test sets from different sources, or having an out-dated training set due to the change of data over time. In this case, there would be a discrepancy across domain distributions, and naively applying the trained model on the new dataset may cause degradation in the performance. Domain adaptation is a sub-field within machine learning that aims to cope with these types of problems by aligning the disparity between domains such that the trained model can be generalized into the domain of interest. This paper focuses on unsupervised domain adaptation, where the labels are only available in the source domain. It addresses the categorization of domain adaptation from different viewpoints. Besides, It presents some successful shallow and deep domain adaptation approaches that aim to deal with domain adaptation problems.},
   author = {Abolfazl Farahani and Sahar Voghoei and Khaled Rasheed and Hamid R. Arabnia},
   month = {10},
   title = {A Brief Review of Domain Adaptation},
   url = {http://arxiv.org/abs/2010.03978},
   year = {2020},
}
@article{,
   abstract = {An interesting but not extensively studied question in active learning is that of sample reusability: to what extent can samples selected for one learner be reused by another? This paper explains why sample reusability is of practical interest, why reusability can be a problem, how reusability could be improved by importance-weighted active learning, and which obstacles to universal reusability remain. With theoretical arguments and practical demonstrations, this paper argues that universal reusability is impossible. Because every active learning strategy must undersample some areas of the sample space, learners that depend on the samples in those areas will learn more from a random sample selection. This paper describes several experiments with importance-weighted active learning that show the impact of the reusability problem in practice. The experiments confirmed that universal reusability does not exist, although in some cases -- on some datasets and with some pairs of classifiers -- there is sample reusability. Finally, this paper explores the conditions that could guarantee the reusability between two classifiers.},
   author = {Gijs van Tulder and Marco Loog},
   month = {6},
   title = {On the reusability of samples in active learning},
   url = {http://arxiv.org/abs/2206.06276},
   year = {2022},
}
@article{Hendrycks2018,
   abstract = {It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.},
   author = {Dan Hendrycks and Mantas Mazeika and Thomas Dietterich},
   month = {12},
   title = {Deep Anomaly Detection with Outlier Exposure},
   url = {http://arxiv.org/abs/1812.04606},
   year = {2018},
}
@article{Sangermano2022,
   abstract = {Online Continual learning is a challenging learning scenario where the model must learn from a non-stationary stream of data where each sample is seen only once. The main challenge is to incrementally learn while avoiding catastrophic forgetting, namely the problem of forgetting previously acquired knowledge while learning from new data. A popular solution in these scenario is to use a small memory to retain old data and rehearse them over time. Unfortunately, due to the limited memory size, the quality of the memory will deteriorate over time. In this paper we propose OLCGM, a novel replay-based continual learning strategy that uses knowledge condensation techniques to continuously compress the memory and achieve a better use of its limited size. The sample condensation step compresses old samples, instead of removing them like other replay strategies. As a result, the experiments show that, whenever the memory budget is limited compared to the complexity of the data, OLCGM improves the final accuracy compared to state-of-the-art replay strategies.},
   author = {Mattia Sangermano and Antonio Carta and Andrea Cossu and Davide Bacciu},
   month = {6},
   title = {Sample Condensation in Online Continual Learning},
   url = {http://arxiv.org/abs/2206.11849},
   year = {2022},
}
@article{Ross2013,
   abstract = {We introduce online learning algorithms which are independent of feature scales, proving regret bounds dependent on the ratio of scales existent in the data rather than the absolute scale. This has several useful effects: there is no need to pre-normalize data, the test-time and test-space complexity are reduced, and the algorithms are more robust.},
   author = {Stephane Ross and Paul Mineiro and John Langford},
   month = {5},
   title = {Normalized Online Learning},
   url = {http://arxiv.org/abs/1305.6646},
   year = {2013},
}
@article{Hendrycks2016,
   abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
   author = {Dan Hendrycks and Kevin Gimpel},
   month = {10},
   title = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
   url = {http://arxiv.org/abs/1610.02136},
   year = {2016},
}
@article{Koleva2021,
   abstract = {Hydrogen has the potential to be a key contributor toward a low-carbon economy. Generating hydrogen by electrolysis using renewable energy is one way to support a decarbonized economy; however, its cost is not typically competitive with the carbon-emitting incumbent technology, steam methane reforming. The ability of electrolysis to integrate with electricity markets presents a unique cost reduction opportunity due to the perceived future availability of low and zero-marginal cost renewable energy sources. Additionally, as renewables, and particularly, photovoltaics are installed on the grid, they have a value deflation effect. This work evaluates solar-electrolysis configurations using a mathematical programming framework to maximize system net present value. The framework has been tested with specific weather conditions and financial mechanisms in California. Our findings indicate that a spectrum of potential cost competitive solutions is available for systems that (i) have market configurations resembling hybrid retail/wholesale, resulting in a hydrogen production cost range of US$6.2 kg−1–US$6.6 kg−1, or full wholesale market participation, reducing production cost to US$2.6 kg−1–US$3.1 kg−1, and (ii) achieve projected future cost reductions.},
   author = {Mariya Koleva and Omar J. Guerra and Joshua Eichman and Bri Mathias Hodge and Jennifer Kurtz},
   doi = {10.1016/j.jpowsour.2020.229183},
   issn = {03787753},
   journal = {Journal of Power Sources},
   keywords = {Decarbonized economy,Electricity markets,Electrons-to-molecules,Power-to-gas,Renewable hydrogen,Solar electrolysis},
   month = {1},
   publisher = {Elsevier B.V.},
   title = {Optimal design of solar-driven electrolytic hydrogen production systems within electricity markets},
   volume = {483},
   year = {2021},
}
@misc{Manogaran2024,
   author = {Indu Manogaran and Amanda Farthing and Jeff Maguire and Kenny Gruchalla},
   title = {Savings in Action: Lessons Learned from a Vermont Community with Solar Plus Storage},
   url = {https://www.nrel.gov/docs/fy24osti/84660.pdf.},
   year = {2024},
}
@article{Blundell2015,
   abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
   author = {Charles Blundell and Julien Cornebise and Koray Kavukcuoglu and Daan Wierstra},
   month = {5},
   title = {Weight Uncertainty in Neural Networks},
   url = {http://arxiv.org/abs/1505.05424},
   year = {2015},
}
@misc{Knoblauch2022,
   abstract = {We advocate an optimization-centric view of Bayesian inference. Our inspiration is the representation of Bayes' rule as infinite-dimensional optimization (Csiszár, 1975; Donsker and Varadhan, 1975; Zellner, 1988). Equipped with this perspective, we study Bayesian inference when one does not have access to (1) well-specified priors, (2) well-specified likelihoods, (3) infinite computing power. While these three assumptions underlie the standard Bayesian paradigm, they are typically inappropriate for modern Machine Learning applications. We propose addressing this through an optimization-centric generalization of Bayesian posteriors that we call the Rule of Three (RoT). The RoT can be justified axiomatically and recovers Bayesian, PAC-Bayesian and VI posteriors as special cases. While the RoT is primarily a conceptual and theoretical device, it also encompasses a novel sub-class of tractable posteriors which we call Generalized Variational Inference (GVI) posteriors. Just as the RoT, GVI posteriors are specified by three arguments: a loss, a divergence and a variational family. They also possess a number of desirable properties, including modularity, Frequentist consistency and an interpretation as approximate ELBO. We explore applications of GVI posteriors, and show that they can be used to improve robustness and posterior marginals on Bayesian Neural Networks and Deep Gaussian Processes.},
   author = {Jeremias Knoblauch and Jack Jewson},
   journal = {Journal of Machine Learning Research},
   keywords = {Bayesian Inference,Bayesian Neural Networks,Deep Gaussian Proceses,Generalized Bayesian Inference,Variational Inference},
   pages = {1-109},
   title = {An Optimization-centric View on Bayes' Rule: Reviewing and Generalizing Variational Inference},
   volume = {23},
   year = {2022},
}
@article{Ruck1992,
   author = {Dennis W. Ruck and Steven K. Rogers and Matthew Kabrisky and Peter S. Maybeck and Mark E. Oxley},
   issue = {06},
   journal = {IEEE transactions on pattern analysis & machine intelligence},
   pages = {686-691},
   title = {Comparative Analysis of Backpropagation and the Extended Kalman Filter for Training Multilayer Perceptrons},
   volume = {14},
   year = {1992},
}
@misc{Sharp2023,
   author = {Justin Sharp and Michael Milligan and Hannah Bloomfield and et. al.},
   title = {Weather Dataset Needs for Planning and Analyzing Modern Power Systems},
   url = {www.esig.energy/reports-briefs.},
   year = {2023},
}
@misc{Turner2024,
   author = {Dave Turner},
   institution = {NOAA},
   title = {NOAA's 3-km Rapid Refresh Weather Forecasting Models and Renewable Energy Forecasts Motivation},
   year = {2024},
}
@article{Lew2020,
   abstract = {This study examines experiences of grid operators to successfully integrate very high penetrations of wind and solar photovoltaic (PV) resources. The variability of these resources creates challenges in balancing the system generation and demand, and ensuring resource adequacy and essential reliability services. The inverter-based nature of wind and solar PV leads to challenges in frequency, transient, and small-signal stability. In this study, seven system operators demonstrate the ability to manage these challenges in a variety of power systems, from stand-alone island systems to larger island systems that are interconnected to neighbours, to balancing authorities that are strongly interconnected within very large synchronous systems. They operate within a variety of market constructs, from full regional transmission operators to vertically-integrated utilities. All are experiencing increases in the penetration of inverter-based, variable energy resources and finding creative solutions to these challenges.},
   author = {Debra Lew and Drake Bartlett and Andrew Groom and Peter Jorgensen and Jon O'sullivan and Ryan Quint and Bruce Rew and Brad Rockwell and Sandip Sharma},
   doi = {10.1049/iet-rpg.2020.0573},
   issn = {17521424},
   issue = {19},
   journal = {IET Renewable Power Generation},
   month = {12},
   pages = {3899-3907},
   publisher = {John Wiley and Sons Inc},
   title = {Getting to 100% renewables: Operating experiences with very high penetrations of variable energy resources},
   volume = {14},
   year = {2020},
}
@article{Guerra2024,
   abstract = {Energy storage technologies, including short-duration, long-duration, and seasonal storage, are seen as technologies that can facilitate the integration of larger shares of variable renewable energy, such as wind and solar photovoltaics, in power systems. However, despite recent advances in the techno-economic modeling of energy storage (particularly for short-duration applications), the operation and economics of long-duration energy storage are still incomplete in power systems modeling platforms. For instance, existing modeling approaches for long-duration storage are often based either on an oversimplified representation of power system operations or limited representation of storage technologies, e.g., evaluation of only a single application. This manuscript presents an overview of the challenges of modeling long-duration energy storage technologies, as well as a discussion regarding the capabilities and limitations of existing approaches. We used two test power systems with high shares of both solar photovoltaics-and wind (70%-90% annual variable renewable energy shares) to assess long-duration energy storage dispatch approaches. Our results estimate that better dispatch modeling of long-duration energy storage could increase the associated operational value by 4%-14% and increase the standard capacity credit by 14%-34%. Thus, a better long-duration energy storage dispatch could represent significant cost saving opportunities for electric utilities and system operators. In addition, existing long-duration dispatch modeling approaches were tested in terms of both improved system value (e.g., based on production cost and standard capacity credit) and scalability (e.g., based on central processing unit time and peak memory usage). Both copper plate and nodal representations of the power system were considered. Although the end volume target dispatch approach, i.e., based on 2 mid-term scheduling, showed promising performance in terms of both improved system value and scalability, there is a need for robust and scalable dispatch approaches for long-duration energy storage in transmission-constrained electric grids. Moreover, more research is required to better understand the optimal operation of long-duration storage considering extreme climate/weather events, reliability applications, and power system operational uncertainties. Highlights • Long-duration energy storage dispatch approaches are reviewed. • Performance of energy storage dispatch approaches is assessed. • A novel metric for energy storage capacity credit estimation is proposed. • Future research directions for modeling the dispatch of energy storage are discussed.},
   author = {Omar J Guerra and Sourabh Dalvi and Amogh Thatte and Brady Cowiestoll and Jennie Jorgenson and Bri-Mathias Hodge},
   journal = {arxiv preprint},
   title = {Towards Robust and Scalable Dispatch Modeling of Long-Duration Energy Storage},
   year = {2024},
}
@misc{Matevosyan2023,
   author = {Julia Matevosyan},
   title = {Capturing the Reliability Benefits of Grid-Forming Batteries Brief for Decisionmakers},
   url = {https://emp.lbl.gov/queues},
   year = {2023},
}
@misc{Ledur2024,
   abstract = {Low-carbon hydrogen is expected to be crucial to the energy transition in the coming years. However, its production is not yet competitive with fossil-fuel-based hydrogen production. This paper proposes an economic analysis of an electrolyzer providing grid services combined with multi-market participation to fully exploit the flexibility potential of this technology. Optimization under uncertainty is combined with a rolling horizon algorithm to simulate the day-today trading decisions of the plant, first in a deterministic, then in a robust fashion. The impact of the multiple uncertainty sources on the production cost reduction is assessed. The case of German markets combined with the ENTSO-E harmonization project for secondary frequency reserve is considered. The effects of the 2021 energy crisis on the different strategies is also analyzed. The results show that both uncertain approaches behave similarly when adding the reserve, with a cash flow increase of around 10% before the crisis but a higher exposure to risk. The addition of reserve provision during the crisis drastically improves the performance of uncertain strategies with close to a 300% increase in cash flow. The robust approach greatly reduces the imbalances caused by wind power generation uncertainty compared to its deterministic counterpart. Finally, the addition of reserve provision induces a notable decrease in the green H2 breakeven price, but not enough to compensate for the costs associated with high electricity market prices.},
   author = {Sylvain Ledur and Robin Molinier and Simon Camal and Moulay-Driss Elalaouifaris and Georges Kariniotakis and Georges Kariniotakis Op-},
   keywords = {Ancillary Services,Industrial Flexibility,Low-carbon Hydrogen,Multi-market Electricity Provision,Robust Optimization},
   title = {Optimal Scheduling of a Large-scale Electrolyzer for Grid Services Provision and Renewable Sourced-Hydrogen Production},
   url = {https://hal.science/hal-04427029},
   year = {2024},
}
@misc{Cole2024,
   author = {Wesley Cole and Caitlin Murphy and Akash Karmakar},
   title = {Beginner's Guide to Understanding Power System Model Results for Long-Term Resource Plans},
   year = {2024},
}
@misc{Barbose2020,
   author = {Galen Barbose and Naïm Darghouth and Eric O'shaughnessy and Sydney Forrester},
   title = {Distributed Solar 2020 Data Update*},
   year = {2020},
}
@article{Paulson2024,
   abstract = {Bayesian optimization (BO) is a powerful technology for optimizing noisy expensive-to-evaluate black-box functions, with a broad range of real-world applications in science, engineering, economics, manufacturing, and beyond. In this paper, we provide an overview of recent developments, challenges, and opportunities in BO for design of next-generation process systems. After describing several motivating applications, we discuss how advanced BO methods have been developed to more efficiently tackle important problems in these applications. We conclude the paper with a summary of challenges and opportunities related to improving the quality of the probabilistic model, the choice of internal optimization procedure used to select the next sample point, and the exploitation of problem structure to improve sample efficiency.},
   author = {Joel A. Paulson and Calvin Tsay},
   month = {1},
   title = {Bayesian optimization as a flexible and efficient design framework for sustainable process systems},
   url = {http://arxiv.org/abs/2401.16373},
   year = {2024},
}
@misc{Frick2021,
   author = {Natalie Mims Frick and Snuller Price and Lisa Schwartz and Nichole Hanus and Ben Shapiro},
   title = {Locational Value of Distributed Energy Resources},
   year = {2021},
}
@misc{Mills2021,
   author = {Andrew D Mills and Joachim Seel and Dev Millstein and James Hyungkwan Kim and Mark Bolinger and Will Gorman and Yuhan Wang and Seongeun Jeong and Ryan Wiser},
   title = {Solar-to-Grid: Trends in System Impacts, Reliability, and Market Value in the United States with Data Through 2019},
   url = {https://emp.lbl.gov/renewable-grid-insights.},
   year = {2021},
}
@misc{Ela2023,
   abstract = {Efficiency and Renewable Energy for supporting their time to organize and participate in the workshop. Disclaimer The meeting organizers and note-takers made every effort to accurately convey the conversations that took place among the participants. Any incorrect or misheard statements are the responsibility of the organizing team.},
   author = {Erik Ela and Alexandre Moreira and Nikita Singhal},
   title = {Electricity Markets Under Deep Decarbonization: Summary of Workshop Conversations Note-takers},
   url = {https://www.esig.energy/market-evolution-for-100-},
   year = {2023},
}
@article{Evin2021,
   abstract = {Height of new snow (HN) forecasts help to prevent critical failures of infrastructures in mountain areas, e.g. transport networks and ski resorts. The French national meteorological service, Méteó-France, operates a probabilistic forecasting system based on ensemble meteorological forecasts and a detailed snowpack model to provide ensembles of HN forecasts. These forecasts are, however, biased and underdispersed. As for many weather variables, post-processing methods can be used to alleviate these drawbacks and obtain meaningful 1 to 4d HN forecasts. In this paper, we compare the skill of two post-processing methods. The first approach is an ensemble model output statistics (EMOS) method, which can be described as a nonhomogeneous regression with a censored shifted Gamma distribution. The second approach is based on quantile regression forests, using different meteorological and snow predictors. Both approaches are evaluated using a 22 year reforecast. Thanks to a larger number of predictors, the quantile regression forest is shown to be a powerful alternative to EMOS for the post-processing of HN ensemble forecasts. The gain of performance is large in all situations but is particularly marked when raw forecasts completely miss the snow event. This type of situation happens when the rain-snow transition elevation is overestimated by the raw forecasts (rain instead of snow in the raw forecasts) or when there is no precipitation in the forecast. In that case, quantile regression forests improve the predictions using the other weather predictors (wind, temperature, and specific humidity).},
   author = {Guillaume Evin and Matthieu Lafaysse and Maxime Taillardat and Michaël Zamo},
   doi = {10.5194/npg-28-467-2021},
   issn = {16077946},
   issue = {3},
   journal = {Nonlinear Processes in Geophysics},
   month = {9},
   pages = {467-480},
   publisher = {Copernicus GmbH},
   title = {Calibrated ensemble forecasts of the height of new snow using quantile regression forests and ensemble model output statistics},
   volume = {28},
   year = {2021},
}
@article{Bai2020,
   abstract = {The control effect of various intelligent terminals is affected by the data sensing precision. The filtering method has been the typical soft computing method used to promote the sensing level. Due to the difficult recognition of the practical system and the empirical parameter estimation in the traditional Kalman filter, a neuron-based Kalman filter was proposed in the paper. Firstly, the framework of the improved Kalman filter was designed, in which the neuro units were introduced. Secondly, the functions of the neuro units were excavated with the nonlinear autoregressive model. The neuro units optimized the filtering process to reduce the effect of the unpractical system model and hypothetical parameters. Thirdly, the adaptive filtering algorithm was proposed based on the new Kalman filter. Finally, the filter was verified with the simulation signals and practical measurements. The results proved that the filter was effective in noise elimination within the soft computing solution.},
   author = {Yu Ting Bai and Xiao Yi Wang and Xue Bo Jin and Zhi Yao Zhao and Bai Hai Zhang},
   doi = {10.3390/s20010299},
   issn = {14248220},
   issue = {1},
   journal = {Sensors (Switzerland)},
   keywords = {Kalman filter,Neural network,Noise filtering,Nonlinear autoregressive},
   month = {1},
   pmid = {31948060},
   publisher = {MDPI AG},
   title = {A neuron-based kalman filter with nonlinear autoregressive model},
   volume = {20},
   year = {2020},
}
@article{Demaeyer2023,
   abstract = {Statistical postprocessing of medium-range weather forecasts is an important component of modern forecasting systems. Since the beginning of modern data science, numerous new postprocessing methods have been proposed, complementing an already very diverse field. However, one of the questions that frequently arises when considering different methods in the framework of implementing operational postprocessing is the relative performance of the methods for a given specific task. It is particularly challenging to find or construct a common comprehensive dataset that can be used to perform such comparisons. Here, we introduce the first version of EUPPBench (EUMETNET postprocessing benchmark), a dataset of time-aligned forecasts and observations, with the aim to facilitate and standardize this process. This dataset is publicly available at https://github.com/EUPP-benchmark/climetlab-eumetnet-postprocessing-benchmark (31 December 2022) and on Zenodo (10.5281/zenodo.7429236, and 10.5281/zenodo.7708362, ). We provide examples showing how to download and use the data, we propose a set of evaluation methods, and we perform a first benchmark of several methods for the correction of 2ĝ€¯m temperature forecasts.},
   author = {Jonathan Demaeyer and Jonas Bhend and Sebastian Lerch and Cristina Primo and Bert Van Schaeybroeck and Aitor Atencia and Zied Ben Bouallègue and Jieyu Chen and Markus Dabernig and Gavin Evans and Jana Faganeli Pucer and Ben Hooper and Nina Horat and David Jobst and Janko Merše and Peter Mlakar and Annette Möller and Olivier Mestre and Maxime Taillardat and Stéphane Vannitsem},
   doi = {10.5194/essd-15-2635-2023},
   issn = {18663516},
   issue = {6},
   journal = {Earth System Science Data},
   month = {6},
   pages = {2635-2653},
   publisher = {Copernicus Publications},
   title = {The EUPPBench postprocessing benchmark dataset v1.0},
   volume = {15},
   year = {2023},
}
@article{Kovachki2021,
   abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
   author = {Nikola Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
   month = {8},
   title = {Neural Operator: Learning Maps Between Function Spaces},
   url = {http://arxiv.org/abs/2108.08481},
   year = {2021},
}
@article{Chapman2022,
   abstract = {Deep-learning (DL) postprocessing methods are examined to obtain reliable and accurate probabilistic forecasts from single-member numerical weather predictions of integrated vapor transport (IVT). Using a 34-yr reforecast, based on the Center for Western Weather and Water Extremes West-WRF mesoscale model of North American West Coast IVT, the dynamically/statistically derived 0-120-h probabilistic forecasts for IVT under atmospheric river (AR) conditions are tested. These predictions are compared with the Global Ensemble Forecast System (GEFS) dynamic model and the GEFS calibrated with a neural network. In addition, the DL methods are tested against an established, but more rigid, statistical-dynamical ensemble method (the analog ensemble). The findings show, using continuous ranked probability skill score and Brier skill score as verification metrics, that the DL methods compete with or outperform the calibrated GEFS system at lead times from 0 to 48 h and again from 72 to 120 h for AR vapor transport events. In addition, the DL methods generate reliable and skillful probabilistic forecasts. The implications of varying the length of the training dataset are examined, and the results show that the DL methods learn relatively quickly and ∼10 years of hindcast data are required to compete with the GEFS ensemble.},
   author = {William E Chapman and Luca Delle Monache and Stefano Alessandrini and Aneesh C Subramanian and F Martin Ralph and Shang-Ping Xie and Sebastian Lerch and Negin Hayatbini},
   doi = {10.1175/MWR-D-21},
   issue = {1},
   journal = {Monthly Weather Review},
   keywords = {Artificial intelligence,Atmospheric river,Deep learning,Error analysis,Machine learning,Neural networks,Numerical analysis/modeling,Numerical weather prediction/forecasting,Other artificial intelligence/machine learning,Probability forecasts/ models/distribution,Regression,Short-range prediction,Uncertainty},
   pages = {215-234},
   title = {Probabilistic Predictions from Deterministic Atmospheric River Forecasts with Deep Learning},
   volume = {150},
   url = {https://doi.org/10.1175/MWR-D-21-},
   year = {2022},
}
@article{Li2020,
   abstract = {The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.},
   author = {Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
   month = {3},
   title = {Neural Operator: Graph Kernel Network for Partial Differential Equations},
   url = {http://arxiv.org/abs/2003.03485},
   year = {2020},
}
@article{Phipps2022,
   abstract = {Capturing the uncertainty in probabilistic wind power forecasts is challenging, especially when uncertain input variables, such as the weather, play a role. Since ensemble weather predictions aim to capture the uncertainty in the weather system, they can be used to propagate this uncertainty through to subsequent wind power forecasting models. However, as weather ensemble systems are known to be biassed and underdispersed, meteorologists post-process the ensembles. This post-processing can successfully correct the biasses in the weather variables but has not been evaluated thoroughly in the context of subsequent forecasts, such as wind power generation forecasts. The present paper evaluates multiple strategies for applying ensemble post-processing to probabilistic wind power forecasts. We use Ensemble Model Output Statistics (EMOS) as the post-processing method and evaluate four possible strategies: only using the raw ensembles without post-processing, a one-step strategy where only the weather ensembles are post-processed, a one-step strategy where we only post-process the power ensembles and a two-step strategy where we post-process both the weather and power ensembles. Results show that post-processing the final wind power ensemble improves forecast performance regarding both calibration and sharpness whilst only post-processing the weather ensembles does not necessarily lead to increased forecast performance.},
   author = {Kaleb Phipps and Sebastian Lerch and Maria Andersson and Ralf Mikut and Veit Hagenmeyer and Nicole Ludwig},
   doi = {10.1002/we.2736},
   issn = {10991824},
   issue = {8},
   journal = {Wind Energy},
   keywords = {energy time series,ensemble post-processing,probabilistic wind power forecasting},
   month = {8},
   pages = {1379-1405},
   publisher = {John Wiley and Sons Ltd},
   title = {Evaluating ensemble post-processing for wind power forecasts},
   volume = {25},
   year = {2022},
}
@article{,
   abstract = {Operator learning aims to discover properties of an underlying dynamical system or partial differential equation (PDE) from data. Here, we present a step-by-step guide to operator learning. We explain the types of problems and PDEs amenable to operator learning, discuss various neural network architectures, and explain how to employ numerical PDE solvers effectively. We also give advice on how to create and manage training data and conduct optimization. We offer intuition behind the various neural network architectures employed in operator learning by motivating them from the point-of-view of numerical linear algebra.},
   author = {Nicolas Boullé and Alex Townsend},
   month = {12},
   title = {A Mathematical Guide to Operator Learning},
   url = {http://arxiv.org/abs/2312.14688},
   year = {2023},
}
@inproceedings{Heimes1998,
   abstract = {It is well known that the Extended Kalman Filter (EKF) neural network training algorithm is superior to the standard backpropagation algorithm. However. there are many variations on the EKF implementation that can significantly effect its performance. For example, improper initialization of three parameters cause the algorithm to perform poorly. There are also two advanced methods, de-coupling and multi-streaming, which need to be properly applied based on the specifics of the problem. This paper presents the results of extensive experimentation in applying the EKF training method for recurrent and static neural networks. The goal is to demonstrate how different variations on its implementation effect performance and to find methods to optimize performance. The paper examines the effects of decoupling, multi-streaming, and initial values of constants used by the algorithm. Three new ideas are suggested that can lead to improved performance. These ideas are: initializing parameters to values outside the range previously suggested, a new decoupling strategy, and reducing the update rate of the error covariance matrix for faster training.},
   author = {Felix Heimes and Lockheed Martin},
   journal = {SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics},
   pages = {1639-1644},
   title = {Extended Kalman Filter Neural Network Training: Experimental Results and Algorithm Improvements},
   year = {1998},
}
@article{Malik2011,
   abstract = {The Kalman filter is commonly used in neural interface systems to decode neural activity and estimate the desired movement kinematics. We analyze a low-complexity Kalman filter implementation in which the filter gain is approximated by its steady-state form, computed offline before real-time decoding commences. We evaluate its performance using human motor cortical spike train data obtained from an intracortical recording array as part of an ongoing pilot clinical trial. We demonstrate that the standard Kalman filter gain converges to within 95% of the steady-state filter gain in 1.5±0.5 s (mean ± s.d.). The difference in the intended movement velocity decoded by the two filters vanishes within 5 s, with a correlation coefficient of 0.99 between the two decoded velocities over the session length. We also find that the steady-state Kalman filter reduces the computational load (algorithm execution time) for decoding the firing rates of 25±3 single units by a factor of 7.0±0.9. We expect that the gain in computational efficiency will be much higher in systems with larger neural ensembles. The steady-state filter can thus provide substantial runtime efficiency at little cost in terms of estimation accuracy. This far more efficient neural decoding approach will facilitate the practical implementation of future large-dimensional, multisignal neural interface systems. © 2010 IEEE.},
   author = {Wasim Q. Malik and Wilson Truccolo and Emery N. Brown and Leigh R. Hochberg},
   doi = {10.1109/TNSRE.2010.2092443},
   issn = {15344320},
   issue = {1},
   journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
   keywords = {Braincomputer interfaces (BCI),Kalman filter,brainmachine interfaces (BMI),motor cortex,neural decoding,paralysis,spinal cord injury,state-space models,tetraplegia},
   month = {2},
   pages = {25-34},
   pmid = {21078582},
   title = {Efficient decoding with steady-state kalman filter in neural interface systems},
   volume = {19},
   year = {2011},
}
@article{Millidge2021,
   abstract = {The Kalman filter is a fundamental filtering algorithm that fuses noisy sensory data, a previous state estimate, and a dynamics model to produce a principled estimate of the current state. It assumes, and is optimal for, linear models and white Gaussian noise. Due to its relative simplicity and general effectiveness, the Kalman filter is widely used in engineering applications. Since many sensory problems the brain faces are, at their core, filtering problems, it is possible that the brain possesses neural circuitry that implements equivalent computations to the Kalman filter. The standard approach to Kalman filtering requires complex matrix computations that are unlikely to be directly implementable in neural circuits. In this paper, we show that a gradient-descent approximation to the Kalman filter requires only local computations with variance weighted prediction errors. Moreover, we show that it is possible under the same scheme to adaptively learn the dynamics model with a learning rule that corresponds directly to Hebbian plasticity. We demonstrate the performance of our method on a simple Kalman filtering task, and propose a neural implementation of the required equations.},
   author = {Beren Millidge and Alexander Tschantz and Anil Seth and Christopher Buckley},
   month = {2},
   title = {Neural Kalman Filtering},
   url = {http://arxiv.org/abs/2102.10021},
   year = {2021},
}
@inbook{Puskorius2001,
   abstract = {This chapter presents a brief discussion of the types of feedforward and recurrent network architectures that are to be considered for training by extended Kalman filter (EKF) methods. A discussion of the global EKF training method, followed by recommendations for setting of parameters for EKF methods, including the relationship of the choice of learning rate to the initialization of the error covariance matrix is included. Treatments of the decoupled extended Kalman filter (DEKF) method are provided. An extensive discussion is given on a variety of issues relating to computer implementation, including derivative calculations, computationally efficient formulations, methods for avoiding matrix inversions, and square-root filtering for computational stability. An overview is provided of applications of EKF methods to a series of problems in control, diagnosis, and modeling of automotive powertrain systems. The chapter concludes with a discussion of the virtues and limitations of EKF training methods, and provides a series of guidelines for implementation and use.},
   author = {Gintaras V. Puskorius and Lee A. Feldkamp},
   doi = {10.1002/0471221546.ch2},
   journal = {Kalman Filtering and Neural Networks},
   month = {10},
   pages = {23-67},
   publisher = {Wiley},
   title = {Parameter‐Based Kalman Filter Training: Theory and Implementation},
   year = {2001},
}
@book{,
   abstract = {First edition.},
   author = {Alex Becker},
   isbn = {9789655984392},
   title = {Kilman filter : from the ground up},
}
@article{Li2019,
   abstract = {Learning and adapting to new distributions or learning new tasks sequentially without forgetting the previously learned knowledge is a challenging phenomenon in continual learning models. Most of the conventional deep learning models are not capable of learning new tasks sequentially in one model without forgetting the previously learned ones. We address this issue by using a Kalman Optimiser. The Kalman Optimiser divides the neural network into two parts: the long-term and short-term memory units. The long-term memory unit is used to remember the learned tasks and the short-term memory unit is to adapt to the new task. We have evaluated our method on MNIST, CIFAR10, CIFAR100 datasets and compare our results with state-of-the-art baseline models. The results show that our approach enables the model to continually learn and adapt to the new changes without forgetting the previously learned tasks.},
   author = {Honglin Li and Shirin Enshaeifar and Frieder Ganz and Payam Barnaghi},
   month = {5},
   title = {Continual Learning in Deep Neural Network by Using a Kalman Optimiser},
   url = {http://arxiv.org/abs/1905.08119},
   year = {2019},
}
@misc{Simon2002,
   abstract = {Radial basis function (RBF) neural networks provide attractive possibilities for solving signal processing and pattern classiÿcation problems. Several algorithms have been proposed for choosing the RBF prototypes and training the network. The selection of the RBF prototypes and the network weights can be viewed as a system identiÿcation problem. As such, this paper proposes the use of the extended Kalman ÿlter for the learning procedure. After the user chooses how many prototypes to include in the network, the Kalman ÿlter simultaneously solves for the prototype vectors and the weight matrix. A decoupled extended Kalman ÿlter is then proposed in order to decrease the computational eeort of the training algorithm. Simulation results are presented on reformulated radial basis neural networks as applied to the Iris classiÿcation problem. It is shown that the use of the Kalman ÿlter results in better learning than conventional RBF networks and faster learning than gradient descent.},
   author = {Dan Simon},
   journal = {Neurocomputing},
   keywords = {Gradient descent,Kalman ÿlter,Optimization,Radial basis function (RBF),Training},
   pages = {455-475},
   title = {Training radial basis neural networks with the extended Kalman ÿlter},
   volume = {48},
   url = {www.elsevier.com/locate/neucom},
   year = {2002},
}
@misc{,
   abstract = {Recent experimental evidence suggests that the brain is capable of approximating Bayesian inference in the face of noisy input stimuli. Despite this progress, the neural underpinnings of this computation are still poorly understood. In this paper we focus on the Bayesian filtering of stochastic time series and introduce a novel neural network, derived from a line attractor architecture, whose dynamics map directly onto those of the Kalman filter in the limit of small prediction error. When the prediction error is large we show that the network responds robustly to changepoints in a way that is qualitatively compatible with the optimal Bayesian model. The model suggests ways in which probability distributions are encoded in the brain and makes a number of testable experimental predictions.},
   author = {Robert C Wilson and Leif H Finkel},
   title = {A Neural Implementation of the Kalman Filter},
}
@misc{,
   author = {Sebastian Raschka},
   title = {M ACH I N E L E A R N I N G Q A N D A I 3 0 E S S E N T I A L Q U E S T I O N S A N D A N S W E R S O N M A C H I N E L E A R N I N G A N D A I S E B A S T I A N R A S C H K A T H E F I N E S T I N G E E K E N T E RTA I N M E N T ™},
   url = {https://sebastianraschka.com.},
}
@misc{,
   abstract = {Kalman filter based training algorithms for recurrent neural networks provide a clever alternative to the standard backpropagation in time. Howeve r, these algorithms do not take into account the optimization of the hidden state variables of the recurrent network. In addition, their formulation requires Jacobian evaluations over the entire network, adding to their computational complexity. In this paper, we propose a spatial-temporal extended Kalman filter algorithm for training recurrent neural network weights and internal states. This new formulation also reduces the computational complexity of Jacobian evaluations drastically by decoupling the gradients of each layer. Monte Carlo comparisons with backpropagation through time point out the robust and fast convergence of the algorithm.},
   author = {Deniz Erdogmus and Justin C Sanchez and Jose C Principe},
   title = {MODIFIED KALMAN FILTER BASED METHOD FOR TRAINING STATE-RECURRENT MULTILAYER PERCEPTRONS},
}
@misc{,
   title = {PRAISE FOR THE COMPLETE DEVELOPER},
}
@misc{,
   author = {Ronald T Kneusel},
   title = {T H E F I N E S T I N G E E K E N T E RTA I N M E N T ™},
}
@article{Capretto2020,
   abstract = {The popularity of Bayesian statistical methods has increased dramatically in recent years across many research areas and industrial applications. This is the result of a variety of methodological advances with faster and cheaper hardware as well as the development of new software tools. Here we introduce an open source Python package named Bambi (BAyesian Model Building Interface) that is built on top of the PyMC probabilistic programming framework and the ArviZ package for exploratory analysis of Bayesian models. Bambi makes it easy to specify complex generalized linear hierarchical models using a formula notation similar to those found in R. We demonstrate Bambi's versatility and ease of use with a few examples spanning a range of common statistical models including multiple regression, logistic regression, and mixed-effects modeling with crossed group specific effects. Additionally we discuss how automatic priors are constructed. Finally, we conclude with a discussion of our plans for the future development of Bambi.},
   author = {Tomás Capretto and Camen Piho and Ravin Kumar and Jacob Westfall and Tal Yarkoni and Osvaldo A. Martin},
   month = {12},
   title = {Bambi: A simple interface for fitting Bayesian linear models in Python},
   url = {http://arxiv.org/abs/2012.10754},
   year = {2020},
}
@misc{,
   author = {Nick Morgan},
   title = {JavaScript Crash Course},
}
@misc{Zavala2023,
   author = {Victor M. Zavala},
   doi = {10.1021/acs.iecr.3c01565},
   issn = {15205045},
   issue = {23},
   journal = {Industrial and Engineering Chemistry Research},
   month = {6},
   pages = {8995-9005},
   publisher = {American Chemical Society},
   title = {Outlook: How i Learned to Love Machine Learning (A Personal Perspective on Machine Learning in Process Systems Engineering)},
   volume = {62},
   year = {2023},
}
@article{Huang2021,
   abstract = {This paper is focused on the optimization approach to the solution of inverse problems. We introduce a stochastic dynamical system in which the parameter-to-data map is embedded, with the goal of employing techniques from nonlinear Kalman filtering to estimate the parameter given the data. The extended Kalman filter (which we refer to as ExKI in the context of inverse problems) can be effective for some inverse problems approached this way, but is impractical when the forward map is not readily differentiable and is given as a black box, and also for high dimensional parameter spaces because of the need to propagate large covariance matrices. Application of ensemble Kalman filters, for example use of the ensemble Kalman inversion (EKI) algorithm, has emerged as a useful tool which overcomes both of these issues: it is derivative free and works with a low-rank covariance approximation formed from the ensemble. In this paper, we work with the ExKI, EKI, and a variant on EKI which we term unscented Kalman inversion (UKI). The paper contains two main contributions. Firstly, we identify a novel stochastic dynamical system in which the parameter-to-data map is embedded. We present theory in the linear case to show exponential convergence of the mean of the filtering distribution to the solution of a regularized least squares problem. This is in contrast to previous work in which the EKI has been employed where the dynamical system used leads to algebraic convergence to an unregularized problem. Secondly, we show that the application of the UKI to this novel stochastic dynamical system yields improved inversion results, in comparison with the application of EKI to the same novel stochastic dynamical system.},
   author = {Daniel Zhengyu Huang and Tapio Schneider and Andrew M. Stuart},
   month = {2},
   title = {Iterated Kalman Methodology For Inverse Problems},
   url = {http://arxiv.org/abs/2102.01580},
   year = {2021},
}
@article{Shao2021,
   abstract = {One hidden yet important issue for developing neural network potentials (NNPs) is the choice of training algorithm. In this article, we compare the performance of two popular training algorithms, the adaptive moment estimation algorithm (Adam) and the extended Kalman filter algorithm (EKF), using the Behler-Parrinello neural network and two publicly accessible datasets of liquid water [Morawietz et al., Proc. Natl. Acad. Sci. U. S. A. 113, 8368-8373, (2016) and Cheng et al., Proc. Natl. Acad. Sci. U. S. A. 116, 1110-1115, (2019)]. This is achieved by implementing EKF in TensorFlow. It is found that NNPs trained with EKF are more transferable and less sensitive to the value of the learning rate, as compared to Adam. In both cases, error metrics of the validation set do not always serve as a good indicator for the actual performance of NNPs. Instead, we show that their performance correlates well with a Fisher information based similarity measure.},
   author = {Yunqi Shao and Florian M. Dietrich and Carl Nettelblad and Chao Zhang},
   doi = {10.1063/5.0070931},
   issn = {10897690},
   issue = {20},
   journal = {Journal of Chemical Physics},
   month = {11},
   pmid = {34852491},
   publisher = {American Institute of Physics Inc.},
   title = {Training algorithm matters for the performance of neural network potential: A case study of Adam and the Kalman filter optimizers},
   volume = {155},
   year = {2021},
}
@article{Zhou2019,
   abstract = {The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects -- an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.},
   author = {Zongwei Zhou and Md Mahfuzur Rahman Siddiquee and Nima Tajbakhsh and Jianming Liang},
   month = {12},
   title = {UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation},
   url = {http://arxiv.org/abs/1912.05074},
   year = {2019},
}
@article{Gelman2020,
   abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
   author = {Andrew Gelman and Aki Vehtari and Daniel Simpson and Charles C. Margossian and Bob Carpenter and Yuling Yao and Lauren Kennedy and Jonah Gabry and Paul-Christian Bürkner and Martin Modrák},
   month = {11},
   title = {Bayesian Workflow},
   url = {http://arxiv.org/abs/2011.01808},
   year = {2020},
}
@article{Li2016,
   abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
   author = {Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
   month = {8},
   title = {Pruning Filters for Efficient ConvNets},
   url = {http://arxiv.org/abs/1608.08710},
   year = {2016},
}
@article{Champion2019,
   abstract = {The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam's razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom autoencoder to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional dynamical systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. It is the first method of its kind to place the discovery of coordinates and models on an equal footing.},
   author = {Kathleen Champion and Bethany Lusch and J. Nathan Kutz and Steven L. Brunton},
   doi = {10.1073/pnas.1906995116},
   month = {3},
   title = {Data-driven discovery of coordinates and governing equations},
   url = {http://arxiv.org/abs/1904.02107 http://dx.doi.org/10.1073/pnas.1906995116},
   year = {2019},
}
@article{Zhang2019,
   abstract = {Inverse problems are common and important in many applications in computational physics but are inherently ill-posed with many possible model parameters resulting in satisfactory results in the observation space. When solving the inverse problem with adjoint-based optimization, the problem can be regularized by adding additional constraints in the cost function. However, similar regularizations have not been used in ensemble-based methods, where the same optimization is done implicitly through the analysis step rather than through explicit minimization of the cost function. Ensemble-based methods, and in particular ensemble Kalman methods, have gained popularity in practice where physics models typically do not have readily available adjoint capabilities. While the model outputs can be improved by incorporating observations using these methods, the lack of regularization means the inference of the model parameters remains ill-posed. Here we propose a regularized ensemble Kalman method capable of enforcing regularization constraints. Specifically, we derive a modified analysis scheme that implicitly minimizes a cost function with generalized constraints. We demonstrate the method's ability to regularize the inverse problem with three cases of increasing complexity, starting with inferring scalar model parameters. As a final case, we utilize the proposed method to infer the closure field in the Reynolds-averaged Navier--Stokes equations; a problem of significant importance in fluid dynamics and many engineering applications.},
   author = {Xin-Lei Zhang and Carlos Michelén-Ströfer and Heng Xiao},
   doi = {10.1016/j.jcp.2020.109517},
   month = {10},
   title = {Regularized Ensemble Kalman Methods for Inverse Problems},
   url = {http://arxiv.org/abs/1910.01292 http://dx.doi.org/10.1016/j.jcp.2020.109517},
   year = {2019},
}
@article{Lykkegaard2022,
   abstract = {We develop a novel Markov chain Monte Carlo (MCMC) method that exploits a hierarchy of models of increasing complexity to efficiently generate samples from an unnormalized target distribution. Broadly, the method rewrites the Multilevel MCMC approach of Dodwell et al. (2015) in terms of the Delayed Acceptance (DA) MCMC of Christen & Fox (2005). In particular, DA is extended to use a hierarchy of models of arbitrary depth, and allow subchains of arbitrary length. We show that the algorithm satisfies detailed balance, hence is ergodic for the target distribution. Furthermore, multilevel variance reduction is derived that exploits the multiple levels and subchains, and an adaptive multilevel correction to coarse-level biases is developed. Three numerical examples of Bayesian inverse problems are presented that demonstrate the advantages of these novel methods. The software and examples are available in PyMC3.},
   author = {Mikkel B. Lykkegaard and Tim J. Dodwell and Colin Fox and Grigorios Mingas and Robert Scheichl},
   month = {2},
   title = {Multilevel Delayed Acceptance MCMC},
   url = {http://arxiv.org/abs/2202.03876},
   year = {2022},
}
@article{Bai2024,
   author = {Yun Bai and Simon Camal and Andrea Michiorri},
   doi = {10.1109/TPWRS.2024.3361074},
   issn = {0885-8950},
   journal = {IEEE Transactions on Power Systems},
   pages = {1-13},
   title = {News and Load: A Quantitative Exploration of Natural Language Processing Applications for Forecasting Day-ahead Electricity System Demand},
   url = {https://ieeexplore.ieee.org/document/10418518/},
   year = {2024},
}
@article{Ganin2014,
   abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
   author = {Yaroslav Ganin and Victor Lempitsky},
   month = {9},
   title = {Unsupervised Domain Adaptation by Backpropagation},
   url = {http://arxiv.org/abs/1409.7495},
   year = {2014},
}
@article{Rezende2014,
   abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
   author = {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
   month = {1},
   title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
   url = {http://arxiv.org/abs/1401.4082},
   year = {2014},
}
@article{Rezende2015,
   abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
   author = {Danilo Jimenez Rezende and Shakir Mohamed},
   month = {5},
   title = {Variational Inference with Normalizing Flows},
   url = {http://arxiv.org/abs/1505.05770},
   year = {2015},
}
@article{Liu2020,
   abstract = {Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a Gaussian process. On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.},
   author = {Jeremiah Zhe Liu and Zi Lin and Shreyas Padhy and Dustin Tran and Tania Bedrax-Weiss and Balaji Lakshminarayanan},
   month = {6},
   title = {Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness},
   url = {http://arxiv.org/abs/2006.10108},
   year = {2020},
}
@article{Kingma2013,
   abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
   author = {Diederik P Kingma and Max Welling},
   month = {12},
   title = {Auto-Encoding Variational Bayes},
   url = {http://arxiv.org/abs/1312.6114},
   year = {2013},
}
@misc{,
   abstract = {Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19. Source code is available at neural-additive-models.github.io.},
   author = {Rishabh Agarwal and Levi Melnick and Nicholas Frosst and Xuezhou Zhang and Ben Lengerich and Rich Caruana and Geoffrey E Hinton},
   title = {Neural Additive Models: Interpretable Machine Learning with Neural Nets},
}
@article{Tian2022,
   abstract = {Unsupervised Domain Adaptation (UDA) essentially trades a model's performance on a source domain for improving its performance on a target domain. To resolve the issue, Unsupervised Domain Expansion (UDE) has been proposed recently. UDE tries to adapt the model for the target domain as UDA does, and in the meantime maintains its source-domain performance. In both UDA and UDE settings, a model tailored to a given domain, let it be the source or the target domain, is assumed to well handle samples from the given domain. We question the assumption by reporting the existence of cross-domain visual ambiguity: Given the lack of a crystally clear boundary between the two domains, samples from one domain can be visually close to the other domain. Such sorts of samples are typically in minority in their host domain, so they tend to be overlooked by the domain-specific model, but can be better handled by a model from the other domain. We exploit this finding, and accordingly propose Co-Teaching (CT). The CT method is instantiated with knowledge distillation based CT (kdCT) plus mixup based CT (miCT). Specifically, kdCT transfers knowledge from a leading-teacher network and an assistant-teacher network to a student network, so the cross-domain ambiguity will be better handled by the student. Meanwhile, miCT further enhances the generalization ability of the student. Extensive experiments on two image classification datasets and two driving-scene segmentation datasets justify the viability of CT for UDA and UDE.},
   author = {Kaibin Tian and Qijie Wei and Xirong Li},
   month = {4},
   title = {Co-Teaching for Unsupervised Domain Adaptation and Expansion},
   url = {http://arxiv.org/abs/2204.01210},
   year = {2022},
}
@inproceedings{Haque2022,
   abstract = {Source code summarization involves creating brief descriptions of source code in natural language. These descriptions are a key component of software documentation such as JavaDocs. Automatic code summarization is a prized target of software engineering research, due to the high value summaries have to programmers and the simultaneously high cost of writing and maintaining documentation by hand. Current work is almost all based on machine models trained via big data input. Large datasets of examples of code and summaries of that code are used to train an e.g. encoder-decoder neural model. Then the output predictions of the model are evaluated against a set of reference summaries. The input is code not seen by the model, and the prediction is compared to a reference. The means by which a prediction is compared to a reference is essentially word overlap, calculated via a metric such as BLEU or ROUGE. The problem with using word overlap is that not all words in a sentence have the same importance, and many words have synonyms. The result is that calculated similarity may not match the perceived similarity by human readers. In this paper, we conduct an experiment to measure the degree to which various word overlap metrics correlate to human-rated similarity of predicted and reference summaries. We evaluate alternatives based on current work in semantic similarity metrics and propose recommendations for evaluation of source code summarization.},
   author = {Sakib Haque and Zachary Eberhart and Aakash Bansal and Collin McMillan},
   doi = {10.1145/nnnnnnn.nnnnnnn},
   isbn = {9781450392983},
   journal = {IEEE International Conference on Program Comprehension},
   keywords = {automatic documentation generation,evaluation metrics,source code summarization},
   pages = {36-47},
   publisher = {IEEE Computer Society},
   title = {Semantic Similarity Metrics for Evaluating Source Code Summarization},
   volume = {2022-March},
   year = {2022},
}
@article{Oord2017,
   abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
   author = {Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},
   month = {11},
   title = {Neural Discrete Representation Learning},
   url = {http://arxiv.org/abs/1711.00937},
   year = {2017},
}
@article{Karras2018,
   abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
   author = {Tero Karras and Samuli Laine and Timo Aila},
   month = {12},
   title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
   url = {http://arxiv.org/abs/1812.04948},
   year = {2018},
}
@article{Radford2015,
   abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
   author = {Alec Radford and Luke Metz and Soumith Chintala},
   month = {11},
   title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
   url = {http://arxiv.org/abs/1511.06434},
   year = {2015},
}
@article{Grill2020,
   abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
   author = {Jean-Bastien Grill and Florian Strub and Florent Altché and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and Rémi Munos and Michal Valko},
   month = {6},
   title = {Bootstrap your own latent: A new approach to self-supervised Learning},
   url = {http://arxiv.org/abs/2006.07733},
   year = {2020},
}
@article{Kim2018,
   abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon $\beta$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
   author = {Hyunjik Kim and Andriy Mnih},
   month = {2},
   title = {Disentangling by Factorising},
   url = {http://arxiv.org/abs/1802.05983},
   year = {2018},
}
@article{Eastwood2022,
   abstract = {Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging data drawn from multiple related training distributions or domains. To achieve this, DG is commonly formulated as an average- or worst-case problem over the set of possible domains. However, predictors that perform well on average lack robustness while predictors that perform well in the worst case tend to be overly-conservative. To address this, we propose a new probabilistic framework for DG where the goal is to learn predictors that perform well with high probability. Our key idea is that distribution shifts seen during training should inform us of probable shifts at test time, which we realize by explicitly relating training and test domains as draws from the same underlying meta-distribution. To achieve probable DG, we propose a new optimization problem called Quantile Risk Minimization (QRM). By minimizing the $\alpha$-quantile of predictor's risk distribution over domains, QRM seeks predictors that perform well with probability $\alpha$. To solve QRM in practice, we propose the Empirical QRM (EQRM) algorithm and provide: (i) a generalization bound for EQRM; and (ii) the conditions under which EQRM recovers the causal predictor as $\alpha \to 1$. In our experiments, we introduce a more holistic quantile-focused evaluation protocol for DG and demonstrate that EQRM outperforms state-of-the-art baselines on datasets from WILDS and DomainBed.},
   author = {Cian Eastwood and Alexander Robey and Shashank Singh and Julius von Kügelgen and Hamed Hassani and George J. Pappas and Bernhard Schölkopf},
   month = {7},
   title = {Probable Domain Generalization via Quantile Risk Minimization},
   url = {http://arxiv.org/abs/2207.09944},
   year = {2022},
}
@article{Chevalley2022,
   abstract = {Learning representations that capture the underlying data generating process is a key problem for data efficient and robust use of neural networks. One key property for robustness which the learned representation should capture and which recently received a lot of attention is described by the notion of invariance. In this work we provide a causal perspective and new algorithm for learning invariant representations. Empirically we show that this algorithm works well on a diverse set of tasks and in particular we observe state-of-the-art performance on domain generalization, where we are able to significantly boost the score of existing models.},
   author = {Mathieu Chevalley and Charlotte Bunne and Andreas Krause and Stefan Bauer},
   month = {6},
   title = {Invariant Causal Mechanisms through Distribution Matching},
   url = {http://arxiv.org/abs/2206.11646},
   year = {2022},
}
@article{Krishnan2015,
   abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the "Healing MNIST" dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
   author = {Rahul G. Krishnan and Uri Shalit and David Sontag},
   month = {11},
   title = {Deep Kalman Filters},
   url = {http://arxiv.org/abs/1511.05121},
   year = {2015},
}
@article{Zhou2021,
   abstract = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d.~assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.},
   author = {Kaiyang Zhou and Ziwei Liu and Yu Qiao and Tao Xiang and Chen Change Loy},
   doi = {10.1109/TPAMI.2022.3195549},
   month = {3},
   title = {Domain Generalization: A Survey},
   url = {http://arxiv.org/abs/2103.02503 http://dx.doi.org/10.1109/TPAMI.2022.3195549},
   year = {2021},
}
@misc{,
   abstract = {Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth-and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.},
   author = {Victor Sanh and Thomas Wolf and Alexander M Rush},
   title = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
}
@article{Ng2021,
   abstract = {Both Bayesian and varying coefficient models are very useful tools in practice as they can be used to model parameter heterogeneity in a generalizable way. Motivated by the need of enhancing Marketing Mix Modeling at Uber, we propose a Bayesian Time Varying Coefficient model, equipped with a hierarchical Bayesian structure. This model is different from other time varying coefficient models in the sense that the coefficients are weighted over a set of local latent variables following certain probabilistic distributions. Stochastic Variational Inference is used to approximate the posteriors of latent variables and dynamic coefficients. The proposed model also helps address many challenges faced by traditional MMM approaches. We used simulations as well as real world marketing datasets to demonstrate our model superior performance in terms of both accuracy and interpretability.},
   author = {Edwin Ng and Zhishi Wang and Athena Dai},
   month = {6},
   title = {Bayesian Time Varying Coefficient Model with Applications to Marketing Mix Modeling},
   url = {http://arxiv.org/abs/2106.03322},
   year = {2021},
}
@article{Koh2020,
   abstract = {Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.},
   author = {Pang Wei Koh and Shiori Sagawa and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton A. Earnshaw and Imran S. Haque and Sara Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
   month = {12},
   title = {WILDS: A Benchmark of in-the-Wild Distribution Shifts},
   url = {http://arxiv.org/abs/2012.07421},
   year = {2020},
}
@article{Li2022,
   abstract = {Human visual perception can easily generalize to out-of-distributed visual data, which is far beyond the capability of modern machine learning models. Domain generalization (DG) aims to close this gap, with existing DG methods mainly focusing on the loss function design. In this paper, we propose to explore an orthogonal direction, i.e., the design of the backbone architecture. It is motivated by an empirical finding that transformer-based models trained with empirical risk minimization (ERM) outperform CNN-based models employing state-of-the-art (SOTA) DG algorithms on multiple DG datasets. We develop a formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset. This analysis guides us to propose a novel DG model built upon vision transformers, namely Generalizable Mixture-of-Experts (GMoE). Extensive experiments on DomainBed demonstrate that GMoE trained with ERM outperforms SOTA DG baselines by a large margin. Moreover, GMoE is complementary to existing DG methods and its performance is substantially improved when trained with DG algorithms.},
   author = {Bo Li and Yifei Shen and Jingkang Yang and Yezhen Wang and Jiawei Ren and Tong Che and Jun Zhang and Ziwei Liu},
   month = {6},
   title = {Sparse Mixture-of-Experts are Domain Generalizable Learners},
   url = {http://arxiv.org/abs/2206.04046},
   year = {2022},
}
@article{Singraber2019,
   abstract = {Over the past years high-dimensional neural network potentials (HDNNPs), fitted to accurately reproduce ab initio potential energy surfaces, have become a powerful tool in chemistry, physics and materials science. Here, we focus on the training of the neural networks that lies at the heart of the HDNNP method. We present an efficient approach for optimizing the weight parameters of the neural network via multistream Kalman filtering, using potential energies and forces as reference data. In this procedure, the choice of the free parameters of the Kalman filter can have a significant impact on the fit quality. Carrying out a large parameter study, we determine optimal settings and demonstrate how to optimize training results of HDNNPs. Moreover, we illustrate our HDNNP training approach by revisiting previously presented fits for water and developing a new potential for copper sulfide. This material, accessible in computer simulations so far only via first-principles methods, forms a particularly complex solid structure at low temperatures and undergoes a phase transition to a superionic state upon heating. Analyzing MD simulations carried out with the Cu 2 S HDNNP, we confirm that the underlying ab initio reference method indeed reproduces this behavior.},
   author = {Andreas Singraber and Tobias Morawietz and Jörg Behler and Christoph Dellago},
   doi = {10.1021/acs.jctc.8b01092},
   issn = {15499626},
   issue = {5},
   journal = {Journal of Chemical Theory and Computation},
   month = {5},
   pages = {3075-3092},
   pmid = {30995035},
   publisher = {American Chemical Society},
   title = {Parallel Multistream Training of High-Dimensional Neural Network Potentials},
   volume = {15},
   year = {2019},
}
@article{Malinin2022,
   abstract = {Distributional shift, or the mismatch between training and deployment data, is a significant obstacle to the usage of machine learning in high-stakes industrial applications, such as autonomous driving and medicine. This creates a need to be able to assess how robustly ML models generalize as well as the quality of their uncertainty estimates. Standard ML baseline datasets do not allow these properties to be assessed, as the training, validation and test data are often identically distributed. Recently, a range of dedicated benchmarks have appeared, featuring both distributionally matched and shifted data. Among these benchmarks, the Shifts dataset stands out in terms of the diversity of tasks as well as the data modalities it features. While most of the benchmarks are heavily dominated by 2D image classification tasks, Shifts contains tabular weather forecasting, machine translation, and vehicle motion prediction tasks. This enables the robustness properties of models to be assessed on a diverse set of industrial-scale tasks and either universal or directly applicable task-specific conclusions to be reached. In this paper, we extend the Shifts Dataset with two datasets sourced from industrial, high-risk applications of high societal importance. Specifically, we consider the tasks of segmentation of white matter Multiple Sclerosis lesions in 3D magnetic resonance brain images and the estimation of power consumption in marine cargo vessels. Both tasks feature ubiquitous distributional shifts and a strict safety requirement due to the high cost of errors. These new datasets will allow researchers to further explore robust generalization and uncertainty estimation in new situations. In this work, we provide a description of the dataset and baseline results for both tasks.},
   author = {Andrey Malinin and Andreas Athanasopoulos and Muhamed Barakovic and Meritxell Bach Cuadra and Mark J. F. Gales and Cristina Granziera and Mara Graziani and Nikolay Kartashev and Konstantinos Kyriakopoulos and Po-Jui Lu and Nataliia Molchanova and Antonis Nikitakis and Vatsal Raina and Francesco La Rosa and Eli Sivena and Vasileios Tsarsitalidis and Efi Tsompopoulou and Elena Volf},
   month = {6},
   title = {Shifts 2.0: Extending The Dataset of Real Distributional Shifts},
   url = {http://arxiv.org/abs/2206.15407},
   year = {2022},
}
@article{Rusak2021,
   abstract = {We demonstrate that self-learning techniques like entropy minimization and pseudo-labeling are simple and effective at improving performance of a deployed computer vision model under systematic domain shifts. We conduct a wide range of large-scale experiments and show consistent improvements irrespective of the model architecture, the pre-training technique or the type of distribution shift. At the same time, self-learning is simple to use in practice because it does not require knowledge or access to the original training data or scheme, is robust to hyperparameter choices, is straight-forward to implement and requires only a few adaptation epochs. This makes self-learning techniques highly attractive for any practitioner who applies machine learning algorithms in the real world. We present state-of-the-art adaptation results on CIFAR10-C (8.5% error), ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error), theoretically study the dynamics of self-supervised adaptation methods and propose a new classification dataset (ImageNet-D) which is challenging even with adaptation.},
   author = {Evgenia Rusak and Steffen Schneider and George Pachitariu and Luisa Eck and Peter Gehler and Oliver Bringmann and Wieland Brendel and Matthias Bethge},
   month = {4},
   title = {If your data distribution shifts, use self-learning},
   url = {http://arxiv.org/abs/2104.12928},
   year = {2021},
}
@misc{,
   abstract = {Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant , thereby avoiding structural issues and significant performance degradation after pruning. To address this problem , we propose a general and fully automatic method, Dependency Graph (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances.},
   author = {Gongfan Fang and Xinyin Ma and Mingli Song and Michael Bi Mi and Xinchao Wang},
   title = {DepGraph: Towards Any Structural Pruning},
   url = {https://github.com/VainF/Torch-Pruning},
}
@article{Seo2022,
   abstract = {In this paper, SIA_Track is presented which is developed by a research team from SI Analytics. The proposed method was built from pre-existing detector and tracker under the tracking-by-detection paradigm. The tracker we used is an online tracker that merely links newly received detections with existing tracks. The core part of our method is training procedure of the object detector where synthetic and unlabeled real data were only used for training. To maximize the performance on real data, we first propose to use pseudo-labeling that generates imperfect labels for real data using a model trained with synthetic dataset. After that model soups scheme was applied to aggregate weights produced during iterative pseudo-labeling. Besides, cross-domain mixed sampling also helped to increase detection performance on real data. Our method, SIA_Track, takes the first place on MOTSynth2MOT17 track at BMTT 2022 challenge. The code is available on https://github.com/SIAnalytics/BMTT2022_SIA_track.},
   author = {Minseok Seo and Jeongwon Ryu and Kwangjin Yoon},
   month = {5},
   title = {Bag of Tricks for Domain Adaptive Multi-Object Tracking},
   url = {http://arxiv.org/abs/2205.15609},
   year = {2022},
}
@article{,
   abstract = {Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM.},
   author = {Alexandre Ramé and Nino Vieillard and Léonard Hussenot and Robert Dadashi and Geoffrey Cideron and Olivier Bachem and Johan Ferret},
   month = {1},
   title = {WARM: On the Benefits of Weight Averaged Reward Models},
   url = {http://arxiv.org/abs/2401.12187},
   year = {2024},
}
@article{Wortsman2022,
   abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results "model soups." When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
   author = {Mitchell Wortsman and Gabriel Ilharco and Samir Yitzhak Gadre and Rebecca Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Yair Carmon and Simon Kornblith and Ludwig Schmidt},
   month = {3},
   title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
   url = {http://arxiv.org/abs/2203.05482},
   year = {2022},
}
@article{Betancourt2017,
   abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
   author = {Michael Betancourt},
   month = {1},
   title = {A Conceptual Introduction to Hamiltonian Monte Carlo},
   url = {http://arxiv.org/abs/1701.02434},
   year = {2017},
}
@article{Zhan2022,
   abstract = {While deep learning (DL) is data-hungry and usually relies on extensive labeled data to deliver good performance, Active Learning (AL) reduces labeling costs by selecting a small proportion of samples from unlabeled data for labeling and training. Therefore, Deep Active Learning (DAL) has risen as a feasible solution for maximizing model performance under a limited labeling cost/budget in recent years. Although abundant methods of DAL have been developed and various literature reviews conducted, the performance evaluation of DAL methods under fair comparison settings is not yet available. Our work intends to fill this gap. In this work, We construct a DAL toolkit, DeepAL+, by re-implementing 19 highly-cited DAL methods. We survey and categorize DAL-related works and construct comparative experiments across frequently used datasets and DAL algorithms. Additionally, we explore some factors (e.g., batch size, number of epochs in the training process) that influence the efficacy of DAL, which provides better references for researchers to design their DAL experiments or carry out DAL-related applications.},
   author = {Xueying Zhan and Qingzhong Wang and Kuan-hao Huang and Haoyi Xiong and Dejing Dou and Antoni B. Chan},
   month = {3},
   title = {A Comparative Survey of Deep Active Learning},
   url = {http://arxiv.org/abs/2203.13450},
   year = {2022},
}
@article{Harris2020,
   abstract = {Array programming provides a powerful, compact, expressive syntax for accessing, manipulating, and operating on data in vectors, matrices, and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It plays an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, material science, engineering, finance, and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves and the first imaging of a black hole. Here we show how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring, and analyzing scientific data. NumPy is the foundation upon which the entire scientific Python universe is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Because of its central position in the ecosystem, NumPy increasingly plays the role of an interoperability layer between these new array computation libraries.},
   author = {Charles R. Harris and K. Jarrod Millman and Stéfan J. van der Walt and Ralf Gommers and Pauli Virtanen and David Cournapeau and Eric Wieser and Julian Taylor and Sebastian Berg and Nathaniel J. Smith and Robert Kern and Matti Picus and Stephan Hoyer and Marten H. van Kerkwijk and Matthew Brett and Allan Haldane and Jaime Fernández del Río and Mark Wiebe and Pearu Peterson and Pierre Gérard-Marchant and Kevin Sheppard and Tyler Reddy and Warren Weckesser and Hameer Abbasi and Christoph Gohlke and Travis E. Oliphant},
   doi = {10.1038/s41586-020-2649-2},
   month = {6},
   title = {Array Programming with NumPy},
   url = {http://arxiv.org/abs/2006.10256 http://dx.doi.org/10.1038/s41586-020-2649-2},
   year = {2020},
}
@misc{,
   abstract = {Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach that penalizes an entire residual module in a Transformer model toward an identity mapping. Our method identifies and discards unimportant non-linear mappings in the residual connections by applying a thresh-olding operator on the function norm. It is applicable to any structured module, including a single attention head, an entire attention block, or a feed-forward subnetwork. Furthermore , we introduce spectral normalization to stabilize the distribution of the post-activation values of the Transformer layers, further improving the pruning effectiveness of the proposed methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to demonstrate that SNIP achieves effective pruning results while maintaining comparable performance. Specifically, we improve the performance over the state-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.},
   author = {Zi Lin and Jeremiah Zhe Liu and Zi Yang and Google Research and Nan Hua and Dan Roth},
   title = {Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior},
   url = {https://github.},
}
@misc{,
   abstract = {Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning , according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architec-tures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned "impor-tant" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited "important" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on struc-tured pruning methods. We also compare with the "Lottery Ticket Hypothesis" (Frankle & Carbin, 2019), and find that with optimal learning rate, the "winning ticket" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.},
   author = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
   title = {RETHINKING THE VALUE OF NETWORK PRUNING},
}
@misc{Singhal1989,
   abstract = {A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. a1. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two-dimensional examples.},
   author = {Sharad Singhal and Lance Wu},
   title = {TRAINING MULTILAYER PERCEPTRONS WITH THE EXTENDED KALMAN ALGORITHM},
   year = {1989},
}
@article{Mayfrank2023,
   abstract = {(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the prevailing maximum prediction accuracy paradigm, and model-free neural network controllers trained using reinforcement learning. We show that our method matches the performance of the model-free neural network controllers while consistently outperforming models derived from system identification. Additionally, we show that the MPC policies can react to changes in the control setting without retraining.},
   author = {Daniel Mayfrank and Alexander Mitsos and Manuel Dahmen},
   month = {8},
   title = {End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear Model Predictive Control},
   url = {http://arxiv.org/abs/2308.01674},
   year = {2023},
}
@article{Maddox2019,
   abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
   author = {Wesley Maddox and Timur Garipov and Pavel Izmailov and Dmitry Vetrov and Andrew Gordon Wilson},
   month = {2},
   title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
   url = {http://arxiv.org/abs/1902.02476},
   year = {2019},
}
@article{Virtanen2019,
   abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
   author = {Pauli Virtanen and Ralf Gommers and Travis E. Oliphant and Matt Haberland and Tyler Reddy and David Cournapeau and Evgeni Burovski and Pearu Peterson and Warren Weckesser and Jonathan Bright and Stéfan J. van der Walt and Matthew Brett and Joshua Wilson and K. Jarrod Millman and Nikolay Mayorov and Andrew R. J. Nelson and Eric Jones and Robert Kern and Eric Larson and CJ Carey and İlhan Polat and Yu Feng and Eric W. Moore and Jake VanderPlas and Denis Laxalde and Josef Perktold and Robert Cimrman and Ian Henriksen and E. A. Quintero and Charles R Harris and Anne M. Archibald and Antônio H. Ribeiro and Fabian Pedregosa and Paul van Mulbregt and SciPy 1. 0 Contributors},
   doi = {10.1038/s41592-019-0686-2},
   month = {7},
   title = {SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python},
   url = {http://arxiv.org/abs/1907.10121 http://dx.doi.org/10.1038/s41592-019-0686-2},
   year = {2019},
}
@article{,
   abstract = {Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks. Empirically, it improves the state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, this work contributes to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to reliably update machine learning models. Our code is released: https://github.com/facebookresearch/ModelRatatouille.},
   author = {Alexandre Ramé and Kartik Ahuja and Jianyu Zhang and Matthieu Cord and Léon Bottou and David Lopez-Paz},
   month = {12},
   title = {Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization},
   url = {http://arxiv.org/abs/2212.10445},
   year = {2022},
}
@article{Balaguer2024,
   abstract = {There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.},
   author = {Angels Balaguer and Vinamra Benara and Renato Luiz de Freitas Cunha and Roberto de M. Estevão Filho and Todd Hendry and Daniel Holstein and Jennifer Marsman and Nick Mecklenburg and Sara Malvar and Leonardo O. Nunes and Rafael Padilha and Morris Sharp and Bruno Silva and Swati Sharma and Vijay Aski and Ranveer Chandra},
   month = {1},
   title = {RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture},
   url = {http://arxiv.org/abs/2401.08406},
   year = {2024},
}
@article{Angelopoulos2020,
   abstract = {Convolutional image classifiers can achieve high predictive accuracy, but quantifying their uncertainty remains an unresolved challenge, hindering their deployment in consequential settings. Existing uncertainty quantification techniques, such as Platt scaling, attempt to calibrate the network's probability estimates, but they do not have formal guarantees. We present an algorithm that modifies any classifier to output a predictive set containing the true label with a user-specified probability, such as 90%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. Our method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes after Platt scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other classifiers, our scheme outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling baseline.},
   author = {Anastasios Angelopoulos and Stephen Bates and Jitendra Malik and Michael I. Jordan},
   month = {9},
   title = {Uncertainty Sets for Image Classifiers using Conformal Prediction},
   url = {http://arxiv.org/abs/2009.14193},
   year = {2020},
}
@article{Wojke2017,
   abstract = {Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates.},
   author = {Nicolai Wojke and Alex Bewley and Dietrich Paulus},
   month = {3},
   title = {Simple Online and Realtime Tracking with a Deep Association Metric},
   url = {http://arxiv.org/abs/1703.07402},
   year = {2017},
}
@inproceedings{Yue2023,
   abstract = {Deep Neural Networks (DNNs) generalization is known to be closely related to the flatness of minima, leading to the development of Sharpness-Aware Minimization (SAM) for seeking flatter minima and better generalization. In this paper, we revisit the loss of SAM and propose a more general method, called WSAM, by incorporating sharpness as a regularization term. We prove its generalization bound through the combination of PAC and Bayes-PAC techniques, and evaluate its performance on various public datasets. The results demonstrate that WSAM achieves improved generalization, or is at least highly competitive, compared to the vanilla optimizer, SAM and its variants. The code is available at this link https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.},
   author = {Yun Yue and Jiadi Jiang and Zhiling Ye and Ning Gao and Yongchao Liu and Ke Zhang},
   doi = {10.1145/3580305.3599501},
   isbn = {9798400701030},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {optimization,regularization,sharpness-aware minimization,wsam},
   month = {8},
   pages = {3185-3194},
   publisher = {Association for Computing Machinery},
   title = {Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term},
   year = {2023},
}
@misc{,
   abstract = {The training of sparse neural networks is becoming an increasingly important tool for reducing the computational footprint of models at training and evaluation, as well enabling the effective scaling up of models. Whereas much work over the years has been dedicated to specialised pruning techniques, little attention has been paid to the inherent effect of gradient based training on model sparsity. In this work, we introduce Powerpropagation, a new weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, our method gives rise to weight updates exhibiting a "rich get richer" dynamic, leaving low-magnitude parameters largely unaffected by learning. Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely. Powerpropagation is general, intuitive, cheap and straightforward to implement and can readily be combined with various other techniques. To highlight its versatility , we explore it in two very different settings: Firstly, following a recent line of work, we investigate its effect on sparse training for resource-constrained settings. Here, we combine Powerpropagation with a traditional weight-pruning technique as well as recent state-of-the-art sparse-to-sparse algorithms, showing superior performance on the ImageNet benchmark. Secondly, we advocate the use of sparsity in overcoming catastrophic forgetting, where compressed representations allow accommodating a large number of tasks at fixed model capacity. In all cases our reparameterisation considerably increases the efficacy of the off-the-shelf methods.},
   author = {Jonathan Schwarz and Siddhant M Jayakumar and Razvan Pascanu and Deepmind Peter and E Latham and Yee Whye and Teh Deepmind},
   title = {Powerpropagation: A sparsity inducing weight reparameterisation},
}
@article{Gal2015,
   abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
   author = {Yarin Gal and Zoubin Ghahramani},
   month = {6},
   title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
   url = {http://arxiv.org/abs/1506.02142},
   year = {2015},
}
@misc{Liang2019,
   abstract = {We study the relationship between geometry and capacity measures for deep neu-ral networks from an invariance viewpoint. We introduce a new notion of capacity-the Fisher-Rao norm-that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.},
   author = {Tengyuan Liang and Tomaso Poggio and Alexander Rakhlin and James Stokes},
   title = {Fisher-Rao Metric, Geometry, and Complexity of Neural Networks},
   year = {2019},
}
@misc{,
   author = {Saad Hikmat Haji and Adnan Mohsin Abdulazeez},
   issue = {4},
   journal = {Journal Of Archaeology Of Egypt/Egyptology},
   keywords = {Deep Learning,Gradient Descent,Keyword: Machine Learning,Loss Function,Optimization},
   pages = {2715-2743},
   title = {COMPARISON OF OPTIMIZATION TECHNIQUES BASED ON GRADIENT DESCENT ALGORITHM: A REVIEW PJAEE, 18 (4) (2021) COMPARISON OF OPTIMIZATION TECHNIQUES BASED ON GRADIENT DESCENT ALGORITHM: A REVIEW Comparison Of Optimization Techniques Based On Gradient Descent Algorithm: A Review-Palarch's},
   volume = {18},
}
@article{Brossard2020,
   abstract = {The present paper introduces a novel methodology for Unscented Kalman Filtering (UKF) on manifolds that extends previous work by the authors on UKF on Lie groups. Beyond filtering performance, the main interests of the approach are its versatility, as the method applies to numerous state estimation problems, and its simplicity of implementation for practitioners not being necessarily familiar with manifolds and Lie groups. We have developed the method on two independent open-source Python and Matlab frameworks we call UKF-M, for quickly implementing and testing the approach. The online repositories contain tutorials, documentation, and various relevant robotics examples that the user can readily reproduce and then adapt, for fast prototyping and benchmarking. The code is available at https://github.com/CAOR-MINES-ParisTech/ukfm.},
   author = {Martin Brossard and Axel Barrau and Silvere Bonnabel},
   month = {2},
   title = {A Code for Unscented Kalman Filtering on Manifolds (UKF-M)},
   url = {http://arxiv.org/abs/2002.00878},
   year = {2020},
}
@article{,
   title = {1710.02824v2},
}
@article{,
   title = {liang19a-supp},
}
@article{Sun2020,
   abstract = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.},
   author = {Shiliang Sun and Zehui Cao and Han Zhu and Jing Zhao},
   doi = {10.1109/TCYB.2019.2950779},
   issn = {21682275},
   issue = {8},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Approximate Bayesian inference,deep neural network (DNN),machine learning,optimization method,reinforcement learning (RL)},
   month = {8},
   pages = {3668-3681},
   pmid = {31751262},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Survey of Optimization Methods from a Machine Learning Perspective},
   volume = {50},
   year = {2020},
}
@article{Pierson2017,
   abstract = {To assess racial disparities in police interactions with the public, we compiled and analyzed a dataset detailing over 60 million state patrol stops conducted in 20 U.S. states between 2011 and 2015. We find that black drivers are stopped more often than white drivers relative to their share of the driving-age population, but that Hispanic drivers are stopped less often than whites. Among stopped drivers -- and after controlling for age, gender, time, and location -- blacks and Hispanics are more likely to be ticketed, searched, and arrested than white drivers. These disparities may reflect differences in driving behavior, and are not necessarily the result of bias. In the case of search decisions, we explicitly test for discrimination by examining both the rate at which drivers are searched and the likelihood searches turn up contraband. We find evidence that the bar for searching black and Hispanic drivers is lower than for searching whites. Finally, we find that legalizing recreational marijuana in Washington and Colorado reduced the total number of searches and misdemeanors for all race groups, though a race gap still persists. We conclude by offering recommendations for improving data collection, analysis, and reporting by law enforcement agencies.},
   author = {Emma Pierson and Camelia Simoiu and Jan Overgoor and Sam Corbett-Davies and Vignesh Ramachandran and Cheryl Phillips and Sharad Goel},
   month = {6},
   title = {A large-scale analysis of racial disparities in police stops across the United States},
   url = {http://arxiv.org/abs/1706.05678},
   year = {2017},
}
@article{Tan2019,
   abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
   author = {Mingxing Tan and Quoc V. Le},
   month = {5},
   title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
   url = {http://arxiv.org/abs/1905.11946},
   year = {2019},
}
@article{Chavan2023,
   abstract = {We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.},
   author = {Arnav Chavan and Zhuang Liu and Deepak Gupta and Eric Xing and Zhiqiang Shen},
   month = {6},
   title = {One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning},
   url = {http://arxiv.org/abs/2306.07967},
   year = {2023},
}
@article{Shi2021,
   abstract = {Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an inter-domain gradient matching objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive -- requires computation of second-order derivatives -- we derive a simpler first-order algorithm named Fish that approximates its optimization. We demonstrate the efficacy of Fish on 6 datasets from the Wilds benchmark, which captures distribution shift across a diverse range of modalities. Our method produces competitive results on these datasets and surpasses all baselines on 4 of them. We perform experiments on both the Wilds benchmark, which captures distribution shift in the real world, as well as datasets in DomainBed benchmark that focuses more on synthetic-to-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks.},
   author = {Yuge Shi and Jeffrey Seely and Philip H. S. Torr and N. Siddharth and Awni Hannun and Nicolas Usunier and Gabriel Synnaeve},
   month = {4},
   title = {Gradient Matching for Domain Generalization},
   url = {http://arxiv.org/abs/2104.09937},
   year = {2021},
}
@article{Yu2019,
   abstract = {Deep unsupervised domain adaptation (UDA) has recently received increasing attention from researchers. However, existing methods are computationally intensive due to the computation cost of Convolutional Neural Networks (CNN) adopted by most work. To date, there is no effective network compression method for accelerating these models. In this paper, we propose a unified Transfer Channel Pruning (TCP) approach for accelerating UDA models. TCP is capable of compressing the deep UDA model by pruning less important channels while simultaneously learning transferable features by reducing the cross-domain distribution divergence. Therefore, it reduces the impact of negative transfer and maintains competitive performance on the target task. To the best of our knowledge, TCP is the first approach that aims at accelerating deep UDA models. TCP is validated on two benchmark datasets-Office-31 and ImageCLEF-DA with two common backbone networks-VGG16 and ResNet50. Experimental results demonstrate that TCP achieves comparable or better classification accuracy than other comparison methods while significantly reducing the computational cost. To be more specific, in VGG16, we get even higher accuracy after pruning 26% floating point operations (FLOPs); in ResNet50, we also get higher accuracy on half of the tasks after pruning 12% FLOPs. We hope that TCP will open a new door for future research on accelerating transfer learning models.},
   author = {Chaohui Yu and Jindong Wang and Yiqiang Chen and Zijing Wu},
   month = {3},
   title = {Accelerating Deep Unsupervised Domain Adaptation with Transfer Channel Pruning},
   url = {http://arxiv.org/abs/1904.02654},
   year = {2019},
}
@article{Nagarajan2019,
   abstract = {Why does training deep neural networks using stochastic gradient descent (SGD) result in a generalization error that does not worsen with the number of parameters in the network? To answer this question, we advocate a notion of effective model capacity that is dependent on \{\em a given random initialization of the network\} and not just the training algorithm and the data distribution. We provide empirical evidences that demonstrate that the model capacity of SGD-trained deep networks is in fact restricted through implicit regularization of \{\em the $\ell_2$ distance from the initialization\}. We also provide theoretical arguments that further highlight the need for initialization-dependent notions of model capacity. We leave as open questions how and why distance from initialization is regularized, and whether it is sufficient to explain generalization.},
   author = {Vaishnavh Nagarajan and J. Zico Kolter},
   month = {1},
   title = {Generalization in Deep Networks: The Role of Distance from Initialization},
   url = {http://arxiv.org/abs/1901.01672},
   year = {2019},
}
@article{Ruder2016,
   abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
   author = {Sebastian Ruder},
   month = {9},
   title = {An overview of gradient descent optimization algorithms},
   url = {http://arxiv.org/abs/1609.04747},
   year = {2016},
}
@article{Foret2020,
   abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by prior work connecting the geometry of the loss landscape and generalization, we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels. We open source our code at \url\{https://github.com/google-research/sam\}.},
   author = {Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
   month = {10},
   title = {Sharpness-Aware Minimization for Efficiently Improving Generalization},
   url = {http://arxiv.org/abs/2010.01412},
   year = {2020},
}
@article{Watanabe1990,
   abstract = {Based on various approaches, several different learning algorithms have been given in the literature for neural networks. Almost all algorithms have constant learning rates or constant accelerative parameters, though they have been shown to be effective for some practical applications. The learning procedure of neural networks can be regarded as a problem of estimating (or identifying) constant parameters (i.e. connection weights of network) with a nonlinear or linear observation equation. Making use of the Kalman filtering, we derive a new back-propagation algorithm whose learning rate is computed by a time-varying Riccati difference equation. Perceptron-like and correlational learning algorithms are also obtained as special cases. Furthermore, a self-organising algorithm of feature maps is constructed within a similar framework.},
   author = {Keigo Watanabe and Spyros Tzafestas},
   journal = {Journal of lntelligent and Robotic Systems},
   keywords = {Kalman filters,Neural nets,learning systems,nonlinear filtering,parameter estimation,pattern recognition},
   pages = {305-319},
   publisher = {Kluwer Academic Publishers},
   title = {Learning Algorithms for Neural Networks with the Kalman Filters},
   volume = {3},
   year = {1990},
}
@article{Zumel2016,
   abstract = {We look at common problems found in data that is used for predictive modeling tasks, and describe how to address them with the vtreat R package. vtreat prepares real-world data for predictive modeling in a reproducible and statistically sound manner. We describe the theory of preparing variables so that data has fewer exceptional cases, making it easier to safely use models in production. Common problems dealt with include: infinite values, invalid values, NA, too many categorical levels, rare categorical levels, and new categorical levels (levels seen during application, but not during training). Of special interest are techniques needed to avoid needlessly introducing undesirable nested modeling bias (which is a risk when using a data-preprocessor).},
   author = {Nina Zumel and John Mount},
   month = {11},
   title = {vtreat: a data.frame Processor for Predictive Modeling},
   url = {http://arxiv.org/abs/1611.09477},
   year = {2016},
}
@article{Huang2023,
   abstract = {Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a simple framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a new task, LoraHub can fluidly combine multiple LoRA modules, eliminating the need for human expertise and assumptions. Notably, the composition requires neither additional model parameters nor gradients. Empirical results on the Big-Bench Hard benchmark suggest that LoraHub, while not surpassing the performance of in-context learning, offers a notable performance-efficiency trade-off in few-shot scenarios by employing a significantly reduced number of tokens per example during inference. Notably, LoraHub establishes a better upper bound compared to in-context learning when paired with different demonstration examples, demonstrating its potential for future development. Our vision is to establish a platform for LoRA modules, empowering users to share their trained LoRA modules. This collaborative approach facilitates the seamless application of LoRA modules to novel tasks, contributing to an adaptive ecosystem. Our code is available at https://github.com/sail-sg/lorahub, and all the pre-trained LoRA modules are released at https://huggingface.co/lorahub.},
   author = {Chengsong Huang and Qian Liu and Bill Yuchen Lin and Tianyu Pang and Chao Du and Min Lin},
   month = {7},
   title = {LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition},
   url = {http://arxiv.org/abs/2307.13269},
   year = {2023},
}
@misc{,
   abstract = {Three sampling methods are compared for efficiency on a number of test problems of various complexity for which analytic quadratures are available. The methods compared are Monte Carlo with pseudo-random numbers, Latin Hypercube Sampling, and Quasi Monte Carlo with sampling based on Sobol' sequences. Generally results show superior performance of the Quasi Monte Carlo approach based on Sobol' sequences in line with theoretical predictions. Latin Hypercube Sampling can be more efficient than both Monte Carlo method and Quasi Monte Carlo method but the latter inequality holds for a reduced set of function typology and at small number of sampled points. In conclusion Quasi Monte Carlo method would appear the safest bet when integrating functions of unknown typology.},
   author = {Sergei Kucherenko and Daniel Albrecht and Andrea Saltelli},
   keywords = {High Dimensional Integration,Latin Hypercube Sampling,Monte Carlo,Quasi Monte Carlo,Sobol' sequences},
   title = {Exploring multi-dimensional spaces: a Comparison of Latin Hypercube and Quasi Monte Carlo Sampling Techniques},
}
@misc{,
   abstract = {Adversarial training is widely used to make classifiers robust to a specific threat or adversary, such as ℓ p-norm bounded perturbations of a given p-norm. However, existing methods for training classifiers robust to multiple threats require knowledge of all attacks during training and remain vulnerable to unseen distribution shifts. In this work, we describe how to obtain adversarially-robust model soups (i.e., linear combinations of parameters) that smoothly trade-off robustness to different ℓ p-norm bounded adversaries. We demonstrate that such soups allow us to control the type and level of robustness, and can achieve robustness to all threats without jointly training on all of them. In some cases, the resulting model soups are more robust to a given ℓ p-norm adversary than the constituent model specialized against that same adversary. Finally, we show that adversarially-robust model soups can be a viable tool to adapt to distribution shifts from a few examples.},
   author = {Francesco Croce and Sylvestre-Alvise Rebuffi and Deepmind Evan and Shelhamer Deepmind and Sven Gowal Deepmind},
   title = {Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts},
}
@article{Wang2021,
   abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.},
   author = {Jindong Wang and Cuiling Lan and Chang Liu and Yidong Ouyang and Tao Qin and Wang Lu and Yiqiang Chen and Wenjun Zeng and Philip S. Yu},
   month = {3},
   title = {Generalizing to Unseen Domains: A Survey on Domain Generalization},
   url = {http://arxiv.org/abs/2103.03097},
   year = {2021},
}
@article{Kingma2014,
   abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
   author = {Diederik P. Kingma and Jimmy Ba},
   month = {12},
   title = {Adam: A Method for Stochastic Optimization},
   url = {http://arxiv.org/abs/1412.6980},
   year = {2014},
}
@article{Zhang2020,
   abstract = {It is well known that featuremap attention and multi-path representation are important for visual recognition. In this paper, we present a modularized architecture, which applies the channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. Our design results in a simple and unified computation block, which can be parameterized using only a few variables. Our model, named ResNeSt, outperforms EfficientNet in accuracy and latency trade-off on image classification. In addition, ResNeSt has achieved superior transfer learning results on several public benchmarks serving as the backbone, and has been adopted by the winning entries of COCO-LVIS challenge. The source code for complete system and pretrained models are publicly available.},
   author = {Hang Zhang and Chongruo Wu and Zhongyue Zhang and Yi Zhu and Haibin Lin and Zhi Zhang and Yue Sun and Tong He and Jonas Mueller and R. Manmatha and Mu Li and Alexander Smola},
   month = {4},
   title = {ResNeSt: Split-Attention Networks},
   url = {http://arxiv.org/abs/2004.08955},
   year = {2020},
}
@article{Spinellis2023,
   abstract = {Considerable scientific work involves locating, analyzing, systematizing, and synthesizing other publications. Its results end up in a paper's "background" section or in standalone articles, which include meta-analyses and systematic literature reviews. The required research is aided through the use of online scientific publication databases and search engines, such as Web of Science, Scopus, and Google Scholar. However, use of online databases suffers from a lack of repeatability and transparency, as well as from technical restrictions. Thankfully, open data, powerful personal computers, and open source software now make it possible to run sophisticated publication studies on the desktop in a self-contained environment that peers can readily reproduce. Here we report a Python software package and an associated command-line tool that can populate embedded relational databases with slices from the complete set of Crossref publication metadata, ORCID author records, and other open data sets, for in-depth processing through performant queries. We demonstrate the software's utility by analyzing the underlying dataset's contents, by visualizing the evolution of publications in diverse scientific fields and relationships among them, by outlining scientometric facts associated with COVID-19 research, and by replicating commonly-used bibliometric measures of productivity, impact, and disruption.},
   author = {Diomidis Spinellis},
   doi = {10.1371/journal.pone.0294946},
   month = {1},
   title = {Open Reproducible Publication Research},
   url = {http://arxiv.org/abs/2301.13312 http://dx.doi.org/10.1371/journal.pone.0294946},
   year = {2023},
}
@article{Pascanu2012,
   abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
   author = {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
   month = {11},
   title = {On the difficulty of training Recurrent Neural Networks},
   url = {http://arxiv.org/abs/1211.5063},
   year = {2012},
}
@article{Wang2019,
   abstract = {Accurate predictions of customers' future lifetime value (LTV) given their attributes and past purchase behavior enables a more customer-centric marketing strategy. Marketers can segment customers into various buckets based on the predicted LTV and, in turn, customize marketing messages or advertising copies to serve customers in different segments better. Furthermore, LTV predictions can directly inform marketing budget allocations and improve real-time targeting and bidding of ad impressions. One challenge of LTV modeling is that some customers never come back, and the distribution of LTV can be heavy-tailed. The commonly used mean squared error (MSE) loss does not accommodate the significant fraction of zero value LTV from one-time purchasers and can be sensitive to extremely large LTV's from top spenders. In this article, we model the distribution of LTV given associated features as a mixture of zero point mass and lognormal distribution, which we refer to as the zero-inflated lognormal (ZILN) distribution. This modeling approach allows us to capture the churn probability and account for the heavy-tailedness nature of LTV at the same time. It also yields straightforward uncertainty quantification of the point prediction. The ZILN loss can be used in both linear models and deep neural networks (DNN). For model evaluation, we recommend the normalized Gini coefficient to quantify model discrimination and decile charts to assess model calibration. Empirically, we demonstrate the predictive performance of our proposed model on two real-world public datasets.},
   author = {Xiaojing Wang and Tianqi Liu and Jingang Miao},
   month = {12},
   title = {A Deep Probabilistic Model for Customer Lifetime Value Prediction},
   url = {http://arxiv.org/abs/1912.07753},
   year = {2019},
}
@article{Louizos2017,
   abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
   author = {Christos Louizos and Uri Shalit and Joris Mooij and David Sontag and Richard Zemel and Max Welling},
   month = {5},
   title = {Causal Effect Inference with Deep Latent-Variable Models},
   url = {http://arxiv.org/abs/1705.08821},
   year = {2017},
}
@article{Ollivier2018,
   abstract = {We cast Amari’s natural gradient in statistical learning as a specific case of Kalman filtering. Namely, applying an extended Kalman filter to estimate a fixed unknown parameter of a probabilistic model from a series of observations, is rigorously equivalent to estimating this parameter via an online stochastic natural gradient descent on the log-likelihood of the observations. In the i.i.d. case, this relation is a consequence of the “information filter” phrasing of the extended Kalman filter. In the recurrent (state space, non-i.i.d.) case, we prove that the joint Kalman filter over states and parameters is a natural gradient on top of real-time recurrent learning (RTRL), a classical algorithm to train recurrent models. This exact algebraic correspondence provides relevant interpretations for natural gradient hyperparameters such as learning rates or initialization and regularization of the Fisher information matrix.},
   author = {Yann Ollivier},
   doi = {10.1214/18-EJS1468},
   issn = {19357524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   keywords = {Kalman filter,Natural gradient,Statistical learning,Stochastic gradient descent},
   pages = {2930-2961},
   publisher = {Institute of Mathematical Statistics},
   title = {Online natural gradient as a Kalman filter},
   volume = {12},
   year = {2018},
}
@misc{,
   abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making nor-malization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Nor-malization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
   author = {Sergey Ioffe and Christian Szegedy},
   title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
}
@misc{,
   author = {Yunqi Shao and Florian M Dietrich and Carl Nettelblad and Chao Zhang},
   title = {Supplementary Material Training Algorithm Matters for the Performance of Neural Network Potential: A Case Study of Adam and the Kalman Filter Optimizers},
}
@misc{,
   abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example-deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
   author = {Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
   title = {LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS},
   url = {https://github.com/microsoft/LoRA.},
}
@article{Izmailov2018,
   abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
   author = {Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
   month = {3},
   title = {Averaging Weights Leads to Wider Optima and Better Generalization},
   url = {http://arxiv.org/abs/1803.05407},
   year = {2018},
}
@article{Kingma2015,
   abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
   author = {Diederik P. Kingma and Tim Salimans and Max Welling},
   month = {6},
   title = {Variational Dropout and the Local Reparameterization Trick},
   url = {http://arxiv.org/abs/1506.02557},
   year = {2015},
}
@article{Sun2023,
   abstract = {Recently, the instruction-tuning of large language models is a crucial area of research in the field of natural language processing. Due to resource and cost limitations, several researchers have employed parameter-efficient tuning techniques, such as LoRA, for instruction tuning, and have obtained encouraging results In comparison to full-parameter fine-tuning, LoRA-based tuning demonstrates salient benefits in terms of training costs. In this study, we undertook experimental comparisons between full-parameter fine-tuning and LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental results show that the selection of the foundational model, training dataset scale, learnable parameter quantity, and model training cost are all important factors. We hope that the experimental conclusions of this paper can provide inspiration for training large language models, especially in the field of Chinese, and help researchers find a better trade-off strategy between training cost and model performance. To facilitate the reproduction of the paper's results, the dataset, model and code will be released.},
   author = {Xianghui Sun and Yunjie Ji and Baochang Ma and Xiangang Li},
   month = {4},
   title = {A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model},
   url = {http://arxiv.org/abs/2304.08109},
   year = {2023},
}
@misc{Simon2002,
   abstract = {Radial basis function (RBF) neural networks provide attractive possibilities for solving signal processing and pattern classiÿcation problems. Several algorithms have been proposed for choosing the RBF prototypes and training the network. The selection of the RBF prototypes and the network weights can be viewed as a system identiÿcation problem. As such, this paper proposes the use of the extended Kalman ÿlter for the learning procedure. After the user chooses how many prototypes to include in the network, the Kalman ÿlter simultaneously solves for the prototype vectors and the weight matrix. A decoupled extended Kalman ÿlter is then proposed in order to decrease the computational eeort of the training algorithm. Simulation results are presented on reformulated radial basis neural networks as applied to the Iris classiÿcation problem. It is shown that the use of the Kalman ÿlter results in better learning than conventional RBF networks and faster learning than gradient descent.},
   author = {Dan Simon},
   journal = {Neurocomputing},
   keywords = {Gradient descent,Kalman ÿlter,Optimization,Radial basis function (RBF),Training},
   pages = {455-475},
   title = {Training radial basis neural networks with the extended Kalman ÿlter},
   volume = {48},
   url = {www.elsevier.com/locate/neucom},
   year = {2002},
}
